{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOv7AT4ZcpwP"
      },
      "source": [
        "# CS224N Assignment 1: Exploring Word Vectors (25 Points)\n",
        "### <font color='blue'> Due 4:30pm, Tue Jan 19 </font>\n",
        "\n",
        "Welcome to CS224N!\n",
        "\n",
        "Before you start, make sure you read the README.txt in the same directory as this notebook for important setup information. A lot of code is provided in this notebook, and we highly encourage you to read and understand it as part of the learning :)\n",
        "\n",
        "If you aren't super familiar with Python, Numpy, or Matplotlib, we recommend you check out the review session on Friday. The session will be recorded and the material will be made available on our [website](http://web.stanford.edu/class/cs224n/index.html#schedule). The CS231N Python/Numpy [tutorial](https://cs231n.github.io/python-numpy-tutorial/) is also a great resource.\n",
        "\n",
        "\n",
        "**Assignment Notes:** Please make sure to save the notebook as you go along. Submission Instructions are located at the bottom of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install gensim numpy==1.26.4 scipy\n",
        "!pip install numpy==1.23.5 --force-reinstall\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "INjXe5m4dMcq",
        "outputId": "9fa212e6-b5b9-4477-cf4a-8cfa74aa3b79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "4a198304dbe541bf865bcd51e8b5bd1b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "at first need to install gensim\n"
      ],
      "metadata": {
        "id": "1aKRVQJBdRUm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXycxxVzcpwQ",
        "outputId": "96da92dc-3598-46d4-c7ce-66c87aeb958d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# All Import Statements Defined Here\n",
        "# Note: Do not add to this list.\n",
        "# ----------------\n",
        "\n",
        "import sys\n",
        "assert sys.version_info[0]==3\n",
        "assert sys.version_info[1] >= 5\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.test.utils import datapath\n",
        "import pprint\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = [10, 5]\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "from nltk.corpus import reuters\n",
        "import numpy as np\n",
        "import random\n",
        "import scipy as sp\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "START_TOKEN = '<START>'\n",
        "END_TOKEN = '<END>'\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "# ----------------"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet sets up the programming environment for the assignment. It starts by asserting that the code is running in Python 3 (with version 3.5 or higher), ensuring compatibility with language features used later. The snippet then imports several libraries essential for NLP and data processing tasks: `gensim` is used for word vector models, `pprint` for neat printing, `matplotlib.pyplot` for plotting (with a default figure size set for consistency), and `nltk` for accessing and downloading the Reuters dataset. Additionally, `numpy` and `random` are used for numerical computations and reproducibility, while `scipy` and the decomposition tools from `sklearn` (TruncatedSVD and PCA) provide methods for statistical analysis and dimensionality reduction.\n",
        "\n",
        "The second paragraph defines special tokens, `START_TOKEN` and `END_TOKEN`, which are placeholders used to mark the boundaries of texts in processing tasks. Reproducibility is further ensured by setting fixed random seeds for both `numpy` and the Python `random` module. This guarantees that any random operations produce the same results on different runs, which is especially important for debugging and comparing results in experiments that involve random initialization."
      ],
      "metadata": {
        "id": "ANXlHb5WdCY5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzTEMsHEcpwQ"
      },
      "source": [
        "## Word Vectors\n",
        "\n",
        "Word Vectors are often used as a fundamental component for downstream NLP tasks, e.g. question answering, text generation, translation, etc., so it is important to build some intuitions as to their strengths and weaknesses. Here, you will explore two types of word vectors: those derived from *co-occurrence matrices*, and those derived via *GloVe*.\n",
        "\n",
        "**Note on Terminology:** The terms \"word vectors\" and \"word embeddings\" are often used interchangeably. The term \"embedding\" refers to the fact that we are encoding aspects of a word's meaning in a lower dimensional space. As [Wikipedia](https://en.wikipedia.org/wiki/Word_embedding) states, \"*conceptually it involves a mathematical embedding from a space with one dimension per word to a continuous vector space with a much lower dimension*\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te5OezrscpwQ"
      },
      "source": [
        "## Part 1: Count-Based Word Vectors (10 points)\n",
        "\n",
        "Most word vector models start from the following idea:\n",
        "\n",
        "*You shall know a word by the company it keeps ([Firth, J. R. 1957:11](https://en.wikipedia.org/wiki/John_Rupert_Firth))*\n",
        "\n",
        "Many word vector implementations are driven by the idea that similar words, i.e., (near) synonyms, will be used in similar contexts. As a result, similar words will often be spoken or written along with a shared subset of words, i.e., contexts. By examining these contexts, we can try to develop embeddings for our words. With this intuition in mind, many \"old school\" approaches to constructing word vectors relied on word counts. Here we elaborate upon one of those strategies, *co-occurrence matrices* (for more information, see [here](http://web.stanford.edu/class/cs124/lec/vectorsemantics.video.pdf) or [here](https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9vh7wq-cpwR"
      },
      "source": [
        "### Co-Occurrence\n",
        "\n",
        "A co-occurrence matrix counts how often things co-occur in some environment. Given some word $w_i$ occurring in the document, we consider the *context window* surrounding $w_i$. Supposing our fixed window size is $n$, then this is the $n$ preceding and $n$ subsequent words in that document, i.e. words $w_{i-n} \\dots w_{i-1}$ and $w_{i+1} \\dots w_{i+n}$. We build a *co-occurrence matrix* $M$, which is a symmetric word-by-word matrix in which $M_{ij}$ is the number of times $w_j$ appears inside $w_i$'s window among all documents.\n",
        "\n",
        "**Example: Co-Occurrence with Fixed Window of n=1**:\n",
        "\n",
        "Document 1: \"all that glitters is not gold\"\n",
        "\n",
        "Document 2: \"all is well that ends well\"\n",
        "\n",
        "\n",
        "|     *    | `<START>` | all | that | glitters | is   | not  | gold  | well | ends | `<END>` |\n",
        "|----------|-------|-----|------|----------|------|------|-------|------|------|-----|\n",
        "| `<START>`    | 0     | 2   | 0    | 0        | 0    | 0    | 0     | 0    | 0    | 0   |\n",
        "| all      | 2     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
        "| that     | 0     | 1   | 0    | 1        | 0    | 0    | 0     | 1    | 1    | 0   |\n",
        "| glitters | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 0    | 0   |\n",
        "| is       | 0     | 1   | 0    | 1        | 0    | 1    | 0     | 1    | 0    | 0   |\n",
        "| not      | 0     | 0   | 0    | 0        | 1    | 0    | 1     | 0    | 0    | 0   |\n",
        "| gold     | 0     | 0   | 0    | 0        | 0    | 1    | 0     | 0    | 0    | 1   |\n",
        "| well     | 0     | 0   | 1    | 0        | 1    | 0    | 0     | 0    | 1    | 1   |\n",
        "| ends     | 0     | 0   | 1    | 0        | 0    | 0    | 0     | 1    | 0    | 0   |\n",
        "| `<END>`      | 0     | 0   | 0    | 0        | 0    | 0    | 1     | 1    | 0    | 0   |\n",
        "\n",
        "**Note:** In NLP, we often add `<START>` and `<END>` tokens to represent the beginning and end of sentences, paragraphs or documents. In thise case we imagine `<START>` and `<END>` tokens encapsulating each document, e.g., \"`<START>` All that glitters is not gold `<END>`\", and include these tokens in our co-occurrence counts.\n",
        "\n",
        "The rows (or columns) of this matrix provide one type of word vectors (those based on word-word co-occurrence), but the vectors will be large in general (linear in the number of distinct words in a corpus). Thus, our next step is to run *dimensionality reduction*. In particular, we will run *SVD (Singular Value Decomposition)*, which is a kind of generalized *PCA (Principal Components Analysis)* to select the top $k$ principal components. Here's a visualization of dimensionality reduction with SVD. In this picture our co-occurrence matrix is $A$ with $n$ rows corresponding to $n$ words. We obtain a full matrix decomposition, with the singular values ordered in the diagonal $S$ matrix, and our new, shorter length-$k$ word vectors in $U_k$.\n",
        "\n",
        "![Picture of an SVD](./imgs/svd.png \"SVD\")\n",
        "\n",
        "This reduced-dimensionality co-occurrence representation preserves semantic relationships between words, e.g. *doctor* and *hospital* will be closer than *doctor* and *dog*.\n",
        "\n",
        "**Notes:** If you can barely remember what an eigenvalue is, here's [a slow, friendly introduction to SVD](https://davetang.org/file/Singular_Value_Decomposition_Tutorial.pdf). If you want to learn more thoroughly about PCA or SVD, feel free to check out lectures [7](https://web.stanford.edu/class/cs168/l/l7.pdf), [8](http://theory.stanford.edu/~tim/s15/l/l8.pdf), and [9](https://web.stanford.edu/class/cs168/l/l9.pdf) of CS168. These course notes provide a great high-level treatment of these general purpose algorithms. Though, for the purpose of this class, you only need to know how to extract the k-dimensional embeddings by utilizing pre-programmed implementations of these algorithms from the numpy, scipy, or sklearn python packages. In practice, it is challenging to apply full SVD to large corpora because of the memory needed to perform PCA or SVD. However, if you only want the top $k$ vector components for relatively small $k$ — known as [Truncated SVD](https://en.wikipedia.org/wiki/Singular_value_decomposition#Truncated_SVD) — then there are reasonably scalable techniques to compute those iteratively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XldoxzjRcpwR"
      },
      "source": [
        "### Plotting Co-Occurrence Word Embeddings\n",
        "\n",
        "Here, we will be using the Reuters (business and financial news) corpus. If you haven't run the import cell at the top of this page, please run it now (click it and press SHIFT-RETURN). The corpus consists of 10,788 news documents totaling 1.3 million words. These documents span 90 categories and are split into train and test. For more details, please see https://www.nltk.org/book/ch02.html. We provide a `read_corpus` function below that pulls out only articles from the \"crude\" (i.e. news articles about oil, gas, etc.) category. The function also adds `<START>` and `<END>` tokens to each of the documents, and lowercases words. You do **not** have to perform any other kind of pre-processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KvTa_iEwcpwR"
      },
      "outputs": [],
      "source": [
        "def read_corpus(category=\"crude\"):\n",
        "    \"\"\" Read files from the specified Reuter's category.\n",
        "        Params:\n",
        "            category (string): category name\n",
        "        Return:\n",
        "            list of lists, with words from each of the processed files\n",
        "    \"\"\"\n",
        "    files = reuters.fileids(category)\n",
        "    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5tnW6cVcpwR"
      },
      "source": [
        "Let's have a look what these documents are like…."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2GRuqtocpwR",
        "outputId": "5af44456-cd1a-4ce6-e4de-19df38bb3d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['<START>', 'japan', 'to', 'revise', 'long', '-', 'term', 'energy', 'demand', 'downwards', 'the',\n",
            "  'ministry', 'of', 'international', 'trade', 'and', 'industry', '(', 'miti', ')', 'will', 'revise',\n",
            "  'its', 'long', '-', 'term', 'energy', 'supply', '/', 'demand', 'outlook', 'by', 'august', 'to',\n",
            "  'meet', 'a', 'forecast', 'downtrend', 'in', 'japanese', 'energy', 'demand', ',', 'ministry',\n",
            "  'officials', 'said', '.', 'miti', 'is', 'expected', 'to', 'lower', 'the', 'projection', 'for',\n",
            "  'primary', 'energy', 'supplies', 'in', 'the', 'year', '2000', 'to', '550', 'mln', 'kilolitres',\n",
            "  '(', 'kl', ')', 'from', '600', 'mln', ',', 'they', 'said', '.', 'the', 'decision', 'follows',\n",
            "  'the', 'emergence', 'of', 'structural', 'changes', 'in', 'japanese', 'industry', 'following',\n",
            "  'the', 'rise', 'in', 'the', 'value', 'of', 'the', 'yen', 'and', 'a', 'decline', 'in', 'domestic',\n",
            "  'electric', 'power', 'demand', '.', 'miti', 'is', 'planning', 'to', 'work', 'out', 'a', 'revised',\n",
            "  'energy', 'supply', '/', 'demand', 'outlook', 'through', 'deliberations', 'of', 'committee',\n",
            "  'meetings', 'of', 'the', 'agency', 'of', 'natural', 'resources', 'and', 'energy', ',', 'the',\n",
            "  'officials', 'said', '.', 'they', 'said', 'miti', 'will', 'also', 'review', 'the', 'breakdown',\n",
            "  'of', 'energy', 'supply', 'sources', ',', 'including', 'oil', ',', 'nuclear', ',', 'coal', 'and',\n",
            "  'natural', 'gas', '.', 'nuclear', 'energy', 'provided', 'the', 'bulk', 'of', 'japan', \"'\", 's',\n",
            "  'electric', 'power', 'in', 'the', 'fiscal', 'year', 'ended', 'march', '31', ',', 'supplying',\n",
            "  'an', 'estimated', '27', 'pct', 'on', 'a', 'kilowatt', '/', 'hour', 'basis', ',', 'followed',\n",
            "  'by', 'oil', '(', '23', 'pct', ')', 'and', 'liquefied', 'natural', 'gas', '(', '21', 'pct', '),',\n",
            "  'they', 'noted', '.', '<END>'],\n",
            " ['<START>', 'energy', '/', 'u', '.', 's', '.', 'petrochemical', 'industry', 'cheap', 'oil',\n",
            "  'feedstocks', ',', 'the', 'weakened', 'u', '.', 's', '.', 'dollar', 'and', 'a', 'plant',\n",
            "  'utilization', 'rate', 'approaching', '90', 'pct', 'will', 'propel', 'the', 'streamlined', 'u',\n",
            "  '.', 's', '.', 'petrochemical', 'industry', 'to', 'record', 'profits', 'this', 'year', ',',\n",
            "  'with', 'growth', 'expected', 'through', 'at', 'least', '1990', ',', 'major', 'company',\n",
            "  'executives', 'predicted', '.', 'this', 'bullish', 'outlook', 'for', 'chemical', 'manufacturing',\n",
            "  'and', 'an', 'industrywide', 'move', 'to', 'shed', 'unrelated', 'businesses', 'has', 'prompted',\n",
            "  'gaf', 'corp', '&', 'lt', ';', 'gaf', '>,', 'privately', '-', 'held', 'cain', 'chemical', 'inc',\n",
            "  ',', 'and', 'other', 'firms', 'to', 'aggressively', 'seek', 'acquisitions', 'of', 'petrochemical',\n",
            "  'plants', '.', 'oil', 'companies', 'such', 'as', 'ashland', 'oil', 'inc', '&', 'lt', ';', 'ash',\n",
            "  '>,', 'the', 'kentucky', '-', 'based', 'oil', 'refiner', 'and', 'marketer', ',', 'are', 'also',\n",
            "  'shopping', 'for', 'money', '-', 'making', 'petrochemical', 'businesses', 'to', 'buy', '.', '\"',\n",
            "  'i', 'see', 'us', 'poised', 'at', 'the', 'threshold', 'of', 'a', 'golden', 'period', ',\"', 'said',\n",
            "  'paul', 'oreffice', ',', 'chairman', 'of', 'giant', 'dow', 'chemical', 'co', '&', 'lt', ';',\n",
            "  'dow', '>,', 'adding', ',', '\"', 'there', \"'\", 's', 'no', 'major', 'plant', 'capacity', 'being',\n",
            "  'added', 'around', 'the', 'world', 'now', '.', 'the', 'whole', 'game', 'is', 'bringing', 'out',\n",
            "  'new', 'products', 'and', 'improving', 'the', 'old', 'ones', '.\"', 'analysts', 'say', 'the',\n",
            "  'chemical', 'industry', \"'\", 's', 'biggest', 'customers', ',', 'automobile', 'manufacturers',\n",
            "  'and', 'home', 'builders', 'that', 'use', 'a', 'lot', 'of', 'paints', 'and', 'plastics', ',',\n",
            "  'are', 'expected', 'to', 'buy', 'quantities', 'this', 'year', '.', 'u', '.', 's', '.',\n",
            "  'petrochemical', 'plants', 'are', 'currently', 'operating', 'at', 'about', '90', 'pct',\n",
            "  'capacity', ',', 'reflecting', 'tighter', 'supply', 'that', 'could', 'hike', 'product', 'prices',\n",
            "  'by', '30', 'to', '40', 'pct', 'this', 'year', ',', 'said', 'john', 'dosher', ',', 'managing',\n",
            "  'director', 'of', 'pace', 'consultants', 'inc', 'of', 'houston', '.', 'demand', 'for', 'some',\n",
            "  'products', 'such', 'as', 'styrene', 'could', 'push', 'profit', 'margins', 'up', 'by', 'as',\n",
            "  'much', 'as', '300', 'pct', ',', 'he', 'said', '.', 'oreffice', ',', 'speaking', 'at', 'a',\n",
            "  'meeting', 'of', 'chemical', 'engineers', 'in', 'houston', ',', 'said', 'dow', 'would', 'easily',\n",
            "  'top', 'the', '741', 'mln', 'dlrs', 'it', 'earned', 'last', 'year', 'and', 'predicted', 'it',\n",
            "  'would', 'have', 'the', 'best', 'year', 'in', 'its', 'history', '.', 'in', '1985', ',', 'when',\n",
            "  'oil', 'prices', 'were', 'still', 'above', '25', 'dlrs', 'a', 'barrel', 'and', 'chemical',\n",
            "  'exports', 'were', 'adversely', 'affected', 'by', 'the', 'strong', 'u', '.', 's', '.', 'dollar',\n",
            "  ',', 'dow', 'had', 'profits', 'of', '58', 'mln', 'dlrs', '.', '\"', 'i', 'believe', 'the',\n",
            "  'entire', 'chemical', 'industry', 'is', 'headed', 'for', 'a', 'record', 'year', 'or', 'close',\n",
            "  'to', 'it', ',\"', 'oreffice', 'said', '.', 'gaf', 'chairman', 'samuel', 'heyman', 'estimated',\n",
            "  'that', 'the', 'u', '.', 's', '.', 'chemical', 'industry', 'would', 'report', 'a', '20', 'pct',\n",
            "  'gain', 'in', 'profits', 'during', '1987', '.', 'last', 'year', ',', 'the', 'domestic',\n",
            "  'industry', 'earned', 'a', 'total', 'of', '13', 'billion', 'dlrs', ',', 'a', '54', 'pct', 'leap',\n",
            "  'from', '1985', '.', 'the', 'turn', 'in', 'the', 'fortunes', 'of', 'the', 'once', '-', 'sickly',\n",
            "  'chemical', 'industry', 'has', 'been', 'brought', 'about', 'by', 'a', 'combination', 'of', 'luck',\n",
            "  'and', 'planning', ',', 'said', 'pace', \"'\", 's', 'john', 'dosher', '.', 'dosher', 'said', 'last',\n",
            "  'year', \"'\", 's', 'fall', 'in', 'oil', 'prices', 'made', 'feedstocks', 'dramatically', 'cheaper',\n",
            "  'and', 'at', 'the', 'same', 'time', 'the', 'american', 'dollar', 'was', 'weakening', 'against',\n",
            "  'foreign', 'currencies', '.', 'that', 'helped', 'boost', 'u', '.', 's', '.', 'chemical',\n",
            "  'exports', '.', 'also', 'helping', 'to', 'bring', 'supply', 'and', 'demand', 'into', 'balance',\n",
            "  'has', 'been', 'the', 'gradual', 'market', 'absorption', 'of', 'the', 'extra', 'chemical',\n",
            "  'manufacturing', 'capacity', 'created', 'by', 'middle', 'eastern', 'oil', 'producers', 'in',\n",
            "  'the', 'early', '1980s', '.', 'finally', ',', 'virtually', 'all', 'major', 'u', '.', 's', '.',\n",
            "  'chemical', 'manufacturers', 'have', 'embarked', 'on', 'an', 'extensive', 'corporate',\n",
            "  'restructuring', 'program', 'to', 'mothball', 'inefficient', 'plants', ',', 'trim', 'the',\n",
            "  'payroll', 'and', 'eliminate', 'unrelated', 'businesses', '.', 'the', 'restructuring', 'touched',\n",
            "  'off', 'a', 'flurry', 'of', 'friendly', 'and', 'hostile', 'takeover', 'attempts', '.', 'gaf', ',',\n",
            "  'which', 'made', 'an', 'unsuccessful', 'attempt', 'in', '1985', 'to', 'acquire', 'union',\n",
            "  'carbide', 'corp', '&', 'lt', ';', 'uk', '>,', 'recently', 'offered', 'three', 'billion', 'dlrs',\n",
            "  'for', 'borg', 'warner', 'corp', '&', 'lt', ';', 'bor', '>,', 'a', 'chicago', 'manufacturer',\n",
            "  'of', 'plastics', 'and', 'chemicals', '.', 'another', 'industry', 'powerhouse', ',', 'w', '.',\n",
            "  'r', '.', 'grace', '&', 'lt', ';', 'gra', '>', 'has', 'divested', 'its', 'retailing', ',',\n",
            "  'restaurant', 'and', 'fertilizer', 'businesses', 'to', 'raise', 'cash', 'for', 'chemical',\n",
            "  'acquisitions', '.', 'but', 'some', 'experts', 'worry', 'that', 'the', 'chemical', 'industry',\n",
            "  'may', 'be', 'headed', 'for', 'trouble', 'if', 'companies', 'continue', 'turning', 'their',\n",
            "  'back', 'on', 'the', 'manufacturing', 'of', 'staple', 'petrochemical', 'commodities', ',', 'such',\n",
            "  'as', 'ethylene', ',', 'in', 'favor', 'of', 'more', 'profitable', 'specialty', 'chemicals',\n",
            "  'that', 'are', 'custom', '-', 'designed', 'for', 'a', 'small', 'group', 'of', 'buyers', '.', '\"',\n",
            "  'companies', 'like', 'dupont', '&', 'lt', ';', 'dd', '>', 'and', 'monsanto', 'co', '&', 'lt', ';',\n",
            "  'mtc', '>', 'spent', 'the', 'past', 'two', 'or', 'three', 'years', 'trying', 'to', 'get', 'out',\n",
            "  'of', 'the', 'commodity', 'chemical', 'business', 'in', 'reaction', 'to', 'how', 'badly', 'the',\n",
            "  'market', 'had', 'deteriorated', ',\"', 'dosher', 'said', '.', '\"', 'but', 'i', 'think', 'they',\n",
            "  'will', 'eventually', 'kill', 'the', 'margins', 'on', 'the', 'profitable', 'chemicals', 'in',\n",
            "  'the', 'niche', 'market', '.\"', 'some', 'top', 'chemical', 'executives', 'share', 'the',\n",
            "  'concern', '.', '\"', 'the', 'challenge', 'for', 'our', 'industry', 'is', 'to', 'keep', 'from',\n",
            "  'getting', 'carried', 'away', 'and', 'repeating', 'past', 'mistakes', ',\"', 'gaf', \"'\", 's',\n",
            "  'heyman', 'cautioned', '.', '\"', 'the', 'shift', 'from', 'commodity', 'chemicals', 'may', 'be',\n",
            "  'ill', '-', 'advised', '.', 'specialty', 'businesses', 'do', 'not', 'stay', 'special', 'long',\n",
            "  '.\"', 'houston', '-', 'based', 'cain', 'chemical', ',', 'created', 'this', 'month', 'by', 'the',\n",
            "  'sterling', 'investment', 'banking', 'group', ',', 'believes', 'it', 'can', 'generate', '700',\n",
            "  'mln', 'dlrs', 'in', 'annual', 'sales', 'by', 'bucking', 'the', 'industry', 'trend', '.',\n",
            "  'chairman', 'gordon', 'cain', ',', 'who', 'previously', 'led', 'a', 'leveraged', 'buyout', 'of',\n",
            "  'dupont', \"'\", 's', 'conoco', 'inc', \"'\", 's', 'chemical', 'business', ',', 'has', 'spent', '1',\n",
            "  '.', '1', 'billion', 'dlrs', 'since', 'january', 'to', 'buy', 'seven', 'petrochemical', 'plants',\n",
            "  'along', 'the', 'texas', 'gulf', 'coast', '.', 'the', 'plants', 'produce', 'only', 'basic',\n",
            "  'commodity', 'petrochemicals', 'that', 'are', 'the', 'building', 'blocks', 'of', 'specialty',\n",
            "  'products', '.', '\"', 'this', 'kind', 'of', 'commodity', 'chemical', 'business', 'will', 'never',\n",
            "  'be', 'a', 'glamorous', ',', 'high', '-', 'margin', 'business', ',\"', 'cain', 'said', ',',\n",
            "  'adding', 'that', 'demand', 'is', 'expected', 'to', 'grow', 'by', 'about', 'three', 'pct',\n",
            "  'annually', '.', 'garo', 'armen', ',', 'an', 'analyst', 'with', 'dean', 'witter', 'reynolds', ',',\n",
            "  'said', 'chemical', 'makers', 'have', 'also', 'benefitted', 'by', 'increasing', 'demand', 'for',\n",
            "  'plastics', 'as', 'prices', 'become', 'more', 'competitive', 'with', 'aluminum', ',', 'wood',\n",
            "  'and', 'steel', 'products', '.', 'armen', 'estimated', 'the', 'upturn', 'in', 'the', 'chemical',\n",
            "  'business', 'could', 'last', 'as', 'long', 'as', 'four', 'or', 'five', 'years', ',', 'provided',\n",
            "  'the', 'u', '.', 's', '.', 'economy', 'continues', 'its', 'modest', 'rate', 'of', 'growth', '.',\n",
            "  '<END>'],\n",
            " ['<START>', 'turkey', 'calls', 'for', 'dialogue', 'to', 'solve', 'dispute', 'turkey', 'said',\n",
            "  'today', 'its', 'disputes', 'with', 'greece', ',', 'including', 'rights', 'on', 'the',\n",
            "  'continental', 'shelf', 'in', 'the', 'aegean', 'sea', ',', 'should', 'be', 'solved', 'through',\n",
            "  'negotiations', '.', 'a', 'foreign', 'ministry', 'statement', 'said', 'the', 'latest', 'crisis',\n",
            "  'between', 'the', 'two', 'nato', 'members', 'stemmed', 'from', 'the', 'continental', 'shelf',\n",
            "  'dispute', 'and', 'an', 'agreement', 'on', 'this', 'issue', 'would', 'effect', 'the', 'security',\n",
            "  ',', 'economy', 'and', 'other', 'rights', 'of', 'both', 'countries', '.', '\"', 'as', 'the',\n",
            "  'issue', 'is', 'basicly', 'political', ',', 'a', 'solution', 'can', 'only', 'be', 'found', 'by',\n",
            "  'bilateral', 'negotiations', ',\"', 'the', 'statement', 'said', '.', 'greece', 'has', 'repeatedly',\n",
            "  'said', 'the', 'issue', 'was', 'legal', 'and', 'could', 'be', 'solved', 'at', 'the',\n",
            "  'international', 'court', 'of', 'justice', '.', 'the', 'two', 'countries', 'approached', 'armed',\n",
            "  'confrontation', 'last', 'month', 'after', 'greece', 'announced', 'it', 'planned', 'oil',\n",
            "  'exploration', 'work', 'in', 'the', 'aegean', 'and', 'turkey', 'said', 'it', 'would', 'also',\n",
            "  'search', 'for', 'oil', '.', 'a', 'face', '-', 'off', 'was', 'averted', 'when', 'turkey',\n",
            "  'confined', 'its', 'research', 'to', 'territorrial', 'waters', '.', '\"', 'the', 'latest',\n",
            "  'crises', 'created', 'an', 'historic', 'opportunity', 'to', 'solve', 'the', 'disputes', 'between',\n",
            "  'the', 'two', 'countries', ',\"', 'the', 'foreign', 'ministry', 'statement', 'said', '.', 'turkey',\n",
            "  \"'\", 's', 'ambassador', 'in', 'athens', ',', 'nazmi', 'akiman', ',', 'was', 'due', 'to', 'meet',\n",
            "  'prime', 'minister', 'andreas', 'papandreou', 'today', 'for', 'the', 'greek', 'reply', 'to', 'a',\n",
            "  'message', 'sent', 'last', 'week', 'by', 'turkish', 'prime', 'minister', 'turgut', 'ozal', '.',\n",
            "  'the', 'contents', 'of', 'the', 'message', 'were', 'not', 'disclosed', '.', '<END>']]\n"
          ]
        }
      ],
      "source": [
        "reuters_corpus = read_corpus()\n",
        "pprint.pprint(reuters_corpus[:3], compact=True, width=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqs8wNbIcpwR"
      },
      "source": [
        "### Question 1.1: Implement `distinct_words` [code] (2 points)\n",
        "\n",
        "Write a method to work out the distinct words (word types) that occur in the corpus. You can do this with `for` loops, but it's more efficient to do it with Python list comprehensions. In particular, [this](https://coderwall.com/p/rcmaea/flatten-a-list-of-lists-in-one-line-in-python) may be useful to flatten a list of lists. If you're not familiar with Python list comprehensions in general, here's [more information](https://python-3-patterns-idioms-test.readthedocs.io/en/latest/Comprehensions.html).\n",
        "\n",
        "Your returned `corpus_words` should be sorted. You can use python's `sorted` function for this.\n",
        "\n",
        "You may find it useful to use [Python sets](https://www.w3schools.com/python/python_sets.asp) to remove duplicate words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AI_OzfBEcpwS"
      },
      "outputs": [],
      "source": [
        "def distinct_words(corpus):\n",
        "    \"\"\" Determine a list of distinct words for the corpus.\n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "        Return:\n",
        "            corpus_words (list of strings): sorted list of distinct words across the corpus\n",
        "            num_corpus_words (integer): number of distinct words across the corpus\n",
        "    \"\"\"\n",
        "    corpus_words = []\n",
        "    num_corpus_words = -1\n",
        "\n",
        "    corpus_words = {word for doc in corpus for word in doc}\n",
        "    corpus_words = sorted(list(corpus_words))\n",
        "    num_corpus_words = len(corpus_words)\n",
        "\n",
        "    return corpus_words, num_corpus_words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This sanity check validates the correctness of a function called `distinct_words`, which is expected to extract and return a sorted list of unique words from a given corpus. It defines a toy corpus consisting of two sentences, each wrapped with special `START_TOKEN` and `END_TOKEN`, and splits them into word lists. The function is then called on this corpus to obtain the output `test_corpus_words` and the count `num_corpus_words`. These results are compared against the expected sorted list of distinct words (`ans_test_corpus_words`) and its length (`ans_num_corpus_words`) using `assert` statements. If both assertions pass—i.e., the correct number and content of distinct words are returned—a success message is printed, confirming that the function behaves correctly for this test case."
      ],
      "metadata": {
        "id": "Ww3KlNRvmAtW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxKoKeCfcpwS",
        "outputId": "4a33ba00-ce16-4c50-f379-c6cb787b1bc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "test_corpus_words, num_corpus_words = distinct_words(test_corpus)\n",
        "\n",
        "# Correct answers\n",
        "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
        "ans_num_corpus_words = len(ans_test_corpus_words)\n",
        "\n",
        "# Test correct number of words\n",
        "assert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\n",
        "\n",
        "# Test correct words\n",
        "assert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\n",
        "\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `distinct_words` takes a corpus (a list of documents, where each document is a list of words) and returns a sorted list of unique words across all documents, along with the total number of distinct words. It uses a set comprehension to extract all unique words from every document in the corpus, then converts the set to a sorted list to maintain consistent ordering. The number of distinct words is calculated using `len()` on the resulting list. Finally, it returns both the sorted list of distinct words (`corpus_words`) and the count (`num_corpus_words`)."
      ],
      "metadata": {
        "id": "qmdSg5OdmI1k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cy3Sb4OcpwS"
      },
      "source": [
        "### Question 1.2: Implement `compute_co_occurrence_matrix` [code] (3 points)\n",
        "\n",
        "Write a method that constructs a co-occurrence matrix for a certain window-size $n$ (with a default of 4), considering words $n$ before and $n$ after the word in the center of the window. Here, we start to use `numpy (np)` to represent vectors, matrices, and tensors. If you're not familiar with NumPy, there's a NumPy tutorial in the second half of this cs231n [Python NumPy tutorial](http://cs231n.github.io/python-numpy-tutorial/).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fmCUAixNcpwS"
      },
      "outputs": [],
      "source": [
        "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
        "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
        "\n",
        "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
        "              number of co-occurring words.\n",
        "\n",
        "              For example, if we take the document \"<START> All that glitters is not gold <END>\" with window size of 4,\n",
        "              \"All\" will co-occur with \"<START>\", \"that\", \"glitters\", \"is\", and \"not\".\n",
        "\n",
        "        Params:\n",
        "            corpus (list of list of strings): corpus of documents\n",
        "            window_size (int): size of context window\n",
        "        Return:\n",
        "            M (a symmetric numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)):\n",
        "                Co-occurence matrix of word counts.\n",
        "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
        "            word2ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
        "    \"\"\"\n",
        "    words, num_words = distinct_words(corpus)\n",
        "    M = None\n",
        "    word2ind = {}\n",
        "\n",
        "    word2ind = {word: i for i, word in enumerate(words)}\n",
        "\n",
        "    M = np.zeros((num_words, num_words))\n",
        "    for body in corpus:\n",
        "        for curr_idx, word in enumerate(body):\n",
        "            for window_idx in range(-window_size, window_size + 1):\n",
        "                neighbor_idx = curr_idx + window_idx\n",
        "                if (neighbor_idx < 0) or (neighbor_idx >= len(body)) or (curr_idx == neighbor_idx):\n",
        "                    continue\n",
        "                co_occur_word = body[neighbor_idx]\n",
        "                (word_idx, co_occur_idx) = (word2ind[word], word2ind[co_occur_word])\n",
        "                M[word_idx, co_occur_idx] += 1\n",
        "\n",
        "    return M, word2ind"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The `compute_co_occurrence_matrix` function takes a corpus of documents (where each document is a list of words) and a specified context window size, and returns a symmetric co-occurrence matrix `M` along with a dictionary `word2ind` mapping each unique word to its index in the matrix. It first retrieves the sorted list of distinct words and their count using the `distinct_words()` function, then initializes an empty square matrix of size equal to the number of unique words. For each word in the corpus, the function examines the surrounding words within the given window size (excluding the word itself) and increments the corresponding entries in the matrix to reflect how often each pair of words co-occur. The matrix is built such that each element \\( M_{i,j} \\) counts how many times word \\( j \\) appears in the context window of word \\( i \\), based on the word indices from `word2ind`.\n"
      ],
      "metadata": {
        "id": "5of2PbiCmUaD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEU7bBm2cpwS",
        "outputId": "29986101-fdba-4d51-c76d-c49a728b47be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and get student's co-occurrence matrix\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "M_test, word2ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "\n",
        "# Correct M and word2ind\n",
        "M_test_ans = np.array(\n",
        "    [[0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,],\n",
        "     [0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,],\n",
        "     [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,],\n",
        "     [1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,],\n",
        "     [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,],\n",
        "     [0., 0., 1., 0., 1., 1., 0., 0., 0., 1.,],\n",
        "     [1., 0., 0., 1., 1., 0., 0., 0., 1., 0.,]]\n",
        ")\n",
        "ans_test_corpus_words = sorted([START_TOKEN, \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", END_TOKEN])\n",
        "word2ind_ans = dict(zip(ans_test_corpus_words, range(len(ans_test_corpus_words))))\n",
        "\n",
        "# Test correct word2ind\n",
        "assert (word2ind_ans == word2ind_test), \"Your word2ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2ind_ans, word2ind_test)\n",
        "\n",
        "# Test correct M shape\n",
        "assert (M_test.shape == M_test_ans.shape), \"M matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(M_test.shape, M_test_ans.shape)\n",
        "\n",
        "for w1 in word2ind_ans.keys():\n",
        "    idx1 = word2ind_ans[w1]\n",
        "    for w2 in word2ind_ans.keys():\n",
        "        idx2 = word2ind_ans[w2]\n",
        "        student = M_test[idx1, idx2]\n",
        "        correct = M_test_ans[idx1, idx2]\n",
        "        if student != correct:\n",
        "            print(\"Correct M:\")\n",
        "            print(M_test_ans)\n",
        "            print(\"Your M: \")\n",
        "            print(M_test)\n",
        "            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix M. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student, correct))\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wlUpa9RcpwS"
      },
      "source": [
        "### Question 1.3: Implement `reduce_to_k_dim` [code] (1 point)\n",
        "\n",
        "Construct a method that performs dimensionality reduction on the matrix to produce k-dimensional embeddings. Use SVD to take the top k components and produce a new matrix of k-dimensional embeddings.\n",
        "\n",
        "**Note:** All of numpy, scipy, and scikit-learn (`sklearn`) provide *some* implementation of SVD, but only scipy and sklearn provide an implementation of Truncated SVD, and only sklearn provides an efficient randomized algorithm for calculating large-scale Truncated SVD. So please use [sklearn.decomposition.TruncatedSVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "b5q2vMTtcpwS"
      },
      "outputs": [],
      "source": [
        "def reduce_to_k_dim(M, k=2):\n",
        "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
        "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
        "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
        "\n",
        "        Params:\n",
        "            M (numpy matrix of shape (number of unique words in the corpus , number of unique words in the corpus)): co-occurence matrix of word counts\n",
        "            k (int): embedding size of each word after dimension reduction\n",
        "        Return:\n",
        "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
        "                    In terms of the SVD from math class, this actually returns U * S\n",
        "    \"\"\"\n",
        "    n_iters = 10\n",
        "    M_reduced = None\n",
        "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
        "\n",
        "    svd = TruncatedSVD(n_components = k, n_iter = n_iters)\n",
        "    M_reduced = svd.fit_transform(M)\n",
        "    print(\"Done.\")\n",
        "    return M_reduced"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `reduce_to_k_dim` function reduces the dimensionality of a square co-occurrence matrix `M` (of shape $(n, n)$) to a lower-dimensional matrix `M_reduced` (of shape $(n, k)$) using **Truncated Singular Value Decomposition (TruncatedSVD)** from Scikit-learn. It initializes the SVD with `k` components (the desired embedding size) and sets the number of iterations to 10 for improved convergence. The function then fits the SVD model to `M` and transforms it to obtain the reduced representation, effectively computing $ U \\times S $ from the classical SVD decomposition. This reduced matrix captures the most important latent semantic features of the original high-dimensional data."
      ],
      "metadata": {
        "id": "2uZeQn1rmgrc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8Ot7yHecpwS",
        "outputId": "104164ff-17ae-401d-f820-db146e30208b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Truncated SVD over 10 words...\n",
            "Done.\n",
            "--------------------------------------------------------------------------------\n",
            "Passed All Tests!\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness\n",
        "# In fact we only check that your M_reduced has the right dimensions.\n",
        "# ---------------------\n",
        "\n",
        "# Define toy corpus and run student code\n",
        "test_corpus = [\"{} All that glitters isn't gold {}\".format(START_TOKEN, END_TOKEN).split(\" \"), \"{} All's well that ends well {}\".format(START_TOKEN, END_TOKEN).split(\" \")]\n",
        "M_test, word2ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n",
        "M_test_reduced = reduce_to_k_dim(M_test, k=2)\n",
        "\n",
        "# Test proper dimensions\n",
        "assert (M_test_reduced.shape[0] == 10), \"M_reduced has {} rows; should have {}\".format(M_test_reduced.shape[0], 10)\n",
        "assert (M_test_reduced.shape[1] == 2), \"M_reduced has {} columns; should have {}\".format(M_test_reduced.shape[1], 2)\n",
        "\n",
        "# Print Success\n",
        "print (\"-\" * 80)\n",
        "print(\"Passed All Tests!\")\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2iZPM97cpwT"
      },
      "source": [
        "### Question 1.4: Implement `plot_embeddings` [code] (1 point)\n",
        "\n",
        "Here you will write a function to plot a set of 2D vectors in 2D space. For graphs, we will use Matplotlib (`plt`).\n",
        "\n",
        "For this example, you may find it useful to adapt [this code](http://web.archive.org/web/20190924160434/https://www.pythonmembers.club/2018/05/08/matplotlib-scatter-plot-annotate-set-text-at-label-each-point/). In the future, a good way to make a plot is to look at [the Matplotlib gallery](https://matplotlib.org/gallery/index.html), find a plot that looks somewhat like what you want, and adapt the code they give."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ahLJviDxcpwT"
      },
      "outputs": [],
      "source": [
        "def plot_embeddings(M_reduced, word2ind, words):\n",
        "    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n",
        "        NOTE: do not plot all the words listed in M_reduced / word2ind.\n",
        "        Include a label next to each point.\n",
        "\n",
        "        Params:\n",
        "            M_reduced (numpy matrix of shape (number of unique words in the corpus , 2)): matrix of 2-dimensioal word embeddings\n",
        "            word2ind (dict): dictionary that maps word to indices for matrix M\n",
        "            words (list of strings): words whose embeddings we want to visualize\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    word_idxs = [word2ind[word] for word in words]\n",
        "    word_vectors = M_reduced[word_idxs]\n",
        "    x_coords = [vec[0] for vec in word_vectors]\n",
        "    y_coords = [vec[1] for vec in word_vectors]\n",
        "    for i, word in enumerate(words):\n",
        "        x = x_coords[i]\n",
        "        y = y_coords[i]\n",
        "        plt.scatter(x, y, marker='x', color='red')\n",
        "        plt.text(x, y, word, fontsize=9)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `plot_embeddings` function visualizes word embeddings in a 2D space by plotting the reduced word vectors corresponding to a specified list of words. It takes three inputs: `M_reduced`, a 2D NumPy array of word embeddings (typically obtained from SVD); `word2ind`, a dictionary mapping each word to its row index in `M_reduced`; and `words`, the list of specific words to plot. The function first retrieves the indices of the desired words and extracts their corresponding 2D coordinates. It then plots each word as a red \"x\" on a scatter plot and labels each point with the word itself using `plt.text`. This visualization helps in intuitively examining semantic relationships, such as clustering or proximity, between the selected words."
      ],
      "metadata": {
        "id": "KeH2Gq-PmsuZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "qC2vpgnCcpwT",
        "outputId": "f85313a8-20a8-4739-aa2c-fd8881d62b31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Outputted Plot:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# ---------------------\n",
        "# Run this sanity check\n",
        "# Note that this is not an exhaustive check for correctness.\n",
        "# The plot produced should look like the \"test solution plot\" depicted below.\n",
        "# ---------------------\n",
        "\n",
        "print (\"-\" * 80)\n",
        "print (\"Outputted Plot:\")\n",
        "\n",
        "M_reduced_plot_test = np.array([[1, 1], [-1, -1], [1, -1], [-1, 1], [0, 0]])\n",
        "word2ind_plot_test = {'test1': 0, 'test2': 1, 'test3': 2, 'test4': 3, 'test5': 4}\n",
        "words = ['test1', 'test2', 'test3', 'test4', 'test5']\n",
        "plot_embeddings(M_reduced_plot_test, word2ind_plot_test, words)\n",
        "\n",
        "print (\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rtuWz5acpwT"
      },
      "source": [
        "<font color=red>**Test Plot Solution**</font>\n",
        "<br>\n",
        "<img src=\"./imgs/test_plot.png\" width=40% style=\"float: left;\"> </img>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOTaonQKcpwT"
      },
      "source": [
        "### Question 1.5: Co-Occurrence Plot Analysis [written] (3 points)\n",
        "\n",
        "Now we will put together all the parts you have written! We will compute the co-occurrence matrix with fixed window of 4 (the default window size), over the Reuters \"crude\" (oil) corpus. Then we will use TruncatedSVD to compute 2-dimensional embeddings of each word. TruncatedSVD returns U\\*S, so we need to normalize the returned vectors, so that all the vectors will appear around the unit circle (therefore closeness is directional closeness). **Note**: The line of code below that does the normalizing uses the NumPy concept of *broadcasting*. If you don't know about broadcasting, check out\n",
        "[Computation on Arrays: Broadcasting by Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html).\n",
        "\n",
        "Run the below cell to produce the plot. It'll probably take a few seconds to run. What clusters together in 2-dimensional embedding space? What doesn't cluster together that you might think should have?  **Note:** \"bpd\" stands for \"barrels per day\" and is a commonly used abbreviation in crude oil topic articles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "edJeLbYNcpwT",
        "outputId": "39af0bbf-316c-4365-b0c1-258b0db59b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Truncated SVD over 8185 words...\n",
            "Done.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAGsCAYAAAAxCF0DAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAATd1JREFUeJzt3XtcVVXC//EvFwEROIjK4XYUUctSE7UkNRWNZ8Cxi2VPat71qRknLaM0zbzMWKNZ+ljpVGMXjLSsp/I3NUZjKFRKWZo2mZmGyM0DXuIAaqiwf38wHj0B5mUfEf28X6/96py11157rTNb6ztr77U9DMMwBAAAAAC4IJ713QEAAAAAuBwQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwgXd9d6A+VFVVqbCwUIGBgfLw8Kjv7gAAAACoJ4ZhqKysTBEREfL0vLC5pysyXBUWFspms9V3NwAAAABcIvLy8hQVFXVBbVyR4SowMFBS9Q8YFBRUz70BAAAAUF9KS0tls9mcGeFCXJHh6uStgEFBQYQrAAAAAKY8LsSCFgAAAABgAsIVAAAAAJjgooSrpUuXKjo6Wn5+foqLi9OmTZvqrPvee+/p+uuvV3BwsJo0aaLY2Filpqa61DEMQ7NmzVJ4eLgaN26shIQE7dq1y93DAAAAAIA6uT1crVq1SsnJyZo9e7a2bNmizp07KzExUcXFxbXWDwkJ0YwZM5SVlaVvv/1WY8eO1dixY/Xxxx876yxYsEDPPfecXnzxRX355Zdq0qSJEhMT9csvv7h7OAAAAABQKw/DMAx3niAuLk433HCDlixZIqn6HVM2m02TJk3StGnTzqqNrl27auDAgZo7d64Mw1BERIQefvhhPfLII5Ikh8Mhq9WqlJQUDR069DfbKy0tlcVikcPhYEELAAAA4ApmZjZw68zVsWPHtHnzZiUkJJw6oaenEhISlJWV9ZvHG4ah9PR07dy5U3369JEk7dmzR3a73aVNi8WiuLi4OtusqKhQaWmpywYAAAAAZnJruDpw4IAqKytltVpdyq1Wq+x2e53HORwOBQQEyMfHRwMHDtTzzz+v//qv/5Ik53Hn0ua8efNksVic26X4AuHo6GitXr3alLYmT56sMWPGmNIWAAAAgLNzSa4WGBgYqK1bt+qrr77Sk08+qeTkZGVkZJx3e9OnT5fD4XBueXl55nUWAAAAAOTmlwg3b95cXl5eKioqcikvKipSWFhYncd5enqqbdu2kqTY2Fjt2LFD8+bNU3x8vPO4oqIihYeHu7QZGxtba3u+vr7y9fW9wNEAAAAAQN3cOnPl4+Ojbt26KT093VlWVVWl9PR09ejR46zbqaqqUkVFhSSpdevWCgsLc2mztLRUX3755Tm1eSnavn27unbtqqCgICUmJqqwsFBS9duin332WV199dUKDg7WkCFD5HA4nMd9+umn6tSpkwICAnTnnXeqrKysvoYAAAAAXLHcfltgcnKyli1bpuXLl2vHjh2aMGGCDh8+rLFjx0qSRo0apenTpzvrz5s3T2vXrlV2drZ27NihhQsXKjU1VSNGjJBUHTQmT56sJ554Qv/4xz/073//W6NGjVJERIQGDRrk7uGYw+GQ8vNrFL/88stauWiR7Dt3KiwszDlmSUpNTdX69euVk5Ojn3/+WZMnT5Yk/fzzz7rttts0ceJElZSUaOzYsXrjjTcu1kgAAAAA/IdbbwuUpCFDhmj//v2aNWuW7Ha7YmNjlZaW5lyQIjc3V56epzLe4cOH9ac//Un5+flq3Lix2rdvrzfeeENDhgxx1pk6daoOHz6s++67TyUlJbrpppuUlpYmPz8/dw/nwjkcUlKSVFwsZWRIpy2uMWHYMLUfP14KDdWC119X2FVXKf8/IWzq1KmKiIiQJM2dO1d9+vTRK6+8og8//FARERH6wx/+IEm69dZb1b9//4s+LAAAAOBK5/b3XF2K6vU9V/n5Ut++Una2FBPjDFjRNpueOnZMQ4qLq8szM+XXtq0yMzN144036osvvlBcXJyk6hUTw8PDZbfblZKSooyMDH300UfOU/zxj3/UL7/8opSUlIs7NgAAAKCBaTDvuUItoqKqA1VMTHXAio+XNm6U7HbtPRmsMjJU7OOjiooKRUZGSpL27t3rbCI3N1c+Pj5q0aKFIiIiXPad3A8AAADg4iJc1QebzTVg9eolnTihl7y9tfOVV3S0eXM9+uij6tOnj6KioiRJTz/9tAoLC1VSUqJZs2Zp6NCh8vT01MCBA1VQUKBly5bpxIkT+uc//6l169bV7/gAAACAKxDhqr7YbFJqqkvRuHHjNCw5WVarVQUFBVqxYoVz34gRI9SvXz+1atVKgYGBevbZZyVJISEh+n//7//p2WefVXBwsF5++WUNHz78og4FAAAAAM9cXfxnrk7Ky6u+JTA7+1TZac9gnc7Dw0PffPNNne/xAgAAAHB+eOaqoTs9WMXESBs2uD6DlZdX3z0EAAAAcI4IVxdbfr5rsMrIkHr2rLnIRS3vwQIAAABw6XL7e67wK4GBUmho9efTbwE8uchFfHz1/sBA5yFX4J2bAAAAQINDuLrYLBYpLU0qK6telv10NpuUmVkdrCyW+ukfAAAAgPNCuKoPFkvd4enXgQsAAABAg8AzVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJLkq4Wrp0qaKjo+Xn56e4uDht2rSpzrrLli1T79691bRpUzVt2lQJCQk16o8ZM0YeHh4uW1JSkruHAQAAAAB1cnu4WrVqlZKTkzV79mxt2bJFnTt3VmJiooqLi2utn5GRoWHDhmn9+vXKysqSzWbT7373OxUUFLjUS0pK0r59+5zbm2++6e6hAAAAAECdPAzDMNx5gri4ON1www1asmSJJKmqqko2m02TJk3StGnTfvP4yspKNW3aVEuWLNGoUaMkVc9clZSUaPXq1efVp9LSUlksFjkcDgUFBZ1XGwAAAAAaPjOzgVtnro4dO6bNmzcrISHh1Ak9PZWQkKCsrKyzauPIkSM6fvy4QkJCXMozMjIUGhqqq6++WhMmTNDBgwfrbKOiokKlpaUuGwAAAACYya3h6sCBA6qsrJTVanUpt1qtstvtZ9XGo48+qoiICJeAlpSUpNdff13p6el66qmnlJmZqQEDBqiysrLWNubNmyeLxeLcbDbb+Q8KAAAAAGrhXd8dOJP58+frrbfeUkZGhvz8/JzlQ4cOdX7u1KmTrrvuOrVp00YZGRm6+eaba7Qzffp0JScnO7+XlpYSsAAAAACYyq0zV82bN5eXl5eKiopcyouKihQWFnbGY5955hnNnz9f//rXv3TdddedsW5MTIyaN2+u3bt317rf19dXQUFBLhsAAAAAmMmt4crHx0fdunVTenq6s6yqqkrp6enq0aNHncctWLBAc+fOVVpamq6//vrfPE9+fr4OHjyo8PBwU/oNAAAAAOfK7UuxJycna9myZVq+fLl27NihCRMm6PDhwxo7dqwkadSoUZo+fbqz/lNPPaWZM2fq1VdfVXR0tOx2u+x2u8rLyyVJ5eXlmjJlir744gvl5OQoPT1dt99+u9q2bavExER3DwcAAAAAauX2Z66GDBmi/fv3a9asWbLb7YqNjVVaWppzkYvc3Fx5ep7KeC+88IKOHTumu+66y6Wd2bNna86cOfLy8tK3336r5cuXq6SkRBEREfrd736nuXPnytfX193DAQAAAIBauf09V5ci3nMFAAAAQGpA77kCAAAAgCsF4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AABdVdHS0Vq9eXW/nj4+P1+LFi+vt/ACAyxfhCgAAAABMQLgCADRIx48fP6syAAAuFsIVAOCi2759u7p27aqgoCAlJiaqsLBQkjR16lS1atVKgYGBuvbaa/XOO+84j8nIyFBwcLBeeOEFtWzZUj179lRKSopiY2M1e/ZshYWFaejQoZKkt956S9ddd52Cg4N1ww03aOPGjbX249ChQ7rjjjvUtGlTBQcHq1u3btq7d6/7fwAAwGWJcAUAcB+HQ8rPr1H88ssva+WiRbLv3KmwsDCNGDFCktS5c2d99dVXKikp0axZszRy5Ejt2bPHeVxZWZm2bdumH374QZmZmZKk7777Tt7e3srNzVVqaqrWrFmjRx55RCkpKTp06JCmT5+uW2+9VQcPHqzRj2eeeUYnTpxQQUGBDh48qFdeeUWBgYFu+jEAAJc7whUAwD0cDikpSerbV8rLc9k1YdgwtR8/Xv533qkFjz+u9evXKz8/X8OHD1doaKi8vLw0dOhQtW/f3mXWqaqqSvPnz5e/v7/8/f0lSRaLRTNmzJCPj4/8/f21dOlSTZkyRV27dpWnp6fuvPNOtW/fXmvWrKnRxUaNGungwYPatWuXvLy8FBsbq5CQEPf+LgCAyxbhCgDgHmVlUnGxlJ0txcefCliVlWr1yivV5cXFsjZuLF9fXxUUFOh///d/1aFDB1ksFgUHB+u7777TgQMHnE0GBgYqODjY5TSRkZHy9Dz1r7OcnBw99thjCg4Odm5bt25VQUFBjS5OmTJFvXv31t13362wsDA9+OCDOnr0qDt+DQDAFYBwBQBwj6goKSNDiok5FbA2bpTsdu0tLq4uz8hQsY+PKioqdPz4cc2ZM0evv/66fv75Z5WUlKhjx44yDMPZ5Okhqq4ym82mhQsXqqSkxLkdPnxY06ZNq3FsQECAnnrqKe3cuVNZWVlKT0/X3/72N7N/CQDAFYJwBQBwH5vNNWD16iWdOKGXvL2185VXdLR5cz366KPq06ePSktL5eXlpRYtWqiqqkqvvvqqvvvuu3M+5f3336+nn35amzdvlmEYOnLkiD755BPl1/Ls14cffqgff/xRVVVVCgoKUqNGjeTt7W3CwAEAVyLCFQDAvWw2KTXVpWjcuHEalpwsq9WqgoICrVixQklJSbrrrrvUqVMnRUREaPv27erVq9c5n+7WW2/V/Pnzde+996pp06Zq3bq1nn32WVVVVdWou3v3biUlJTlXJ+zRo4cmTJhw3kMFAFzZPIzT77e4QpSWlspiscjhcCgoKKi+uwMAl7e8vOpbArOzT5X955ZA2Wz11SsAACSZmw2YuQIAuM/pwSomRtqwwfUZrF+tIggAQENGuAIAuEd+vmuwysiQevasuchFLc9CAQDQEPHULgDAPQIDpdDQ6s+n3wJ4cpGL+Pjq/by0FwBwmSBcAQDcw2KR0tKq33cVFeW6z2aTMjOrg5XFUj/9AwDAZIQrAID7WCx1h6dfBy4AABo4nrkCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwAQXJVwtXbpU0dHR8vPzU1xcnDZt2lRn3WXLlql3795q2rSpmjZtqoSEhBr1DcPQrFmzFB4ersaNGyshIUG7du1y9zAAAAAAoE5uD1erVq1ScnKyZs+erS1btqhz585KTExUcXFxrfUzMjI0bNgwrV+/XllZWbLZbPrd736ngoICZ50FCxboueee04svvqgvv/xSTZo0UWJion755Rd3DwcAAAAAauVhGIbhzhPExcXphhtu0JIlSyRJVVVVstlsmjRpkqZNm/abx1dWVqpp06ZasmSJRo0aJcMwFBERoYcffliPPPKIJMnhcMhqtSolJUVDhw79zTZLS0tlsVjkcDgUFBR0YQMEAAAA0GCZmQ3cOnN17Ngxbd68WQkJCadO6OmphIQEZWVlnVUbR44c0fHjxxUSEiJJ2rNnj+x2u0ubFotFcXFxdbZZUVGh0tJSlw0AAAAAzOTWcHXgwAFVVlbKarW6lFutVtnt9rNq49FHH1VERIQzTJ087lzanDdvniwWi3Oz2WznOhQAAAAAOKNLerXA+fPn66233tL7778vPz+/825n+vTpcjgczi0vL8/EXgIAAACA5O3Oxps3by4vLy8VFRW5lBcVFSksLOyMxz7zzDOaP3++PvnkE1133XXO8pPHFRUVKTw83KXN2NjYWtvy9fWVr6/veY4CAAAAAH6bW2eufHx81K1bN6WnpzvLqqqqlJ6erh49etR53IIFCzR37lylpaXp+uuvd9nXunVrhYWFubRZWlqqL7/88oxtAgAAAIA7uXXmSpKSk5M1evRoXX/99erevbsWL16sw4cPa+zYsZKkUaNGKTIyUvPmzZMkPfXUU5o1a5ZWrlyp6Oho53NUAQEBCggIkIeHhyZPnqwnnnhC7dq1U+vWrTVz5kxFRERo0KBB7h4OAAAAANTK7eFqyJAh2r9/v2bNmiW73a7Y2FilpaU5F6TIzc2Vp+epCbQXXnhBx44d01133eXSzuzZszVnzhxJ0tSpU3X48GHdd999Kikp0U033aS0tLQLei4LAAAAAC6E299zdSniPVcAAAAApAb0nisAAAAAuFIQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAl5yUlBTFxsaa2mZ8fLwWL15sapsAcDrCFQAAAACYgHAFAADcpqioSHfffbdatGihli1basaMGTpx4kStM1OxsbFKSUnRN998oz/+8Y/697//rYCAAAUEBCg3N1dz5szRLbfcovHjxysoKEjt2rXT+++/7zz+1zNTW7dulYeHhyTp4Ycf1meffaZHH31UAQEBGjBgwMUYPoArDOEKAABcOIdDys+vUXzPPfeo0fHj2rN1qz777DOtXr1aCxYsOGNTXbp00YsvvqhOnTqpvLxc5eXlatmypSQpLS1N3bt316FDh7Ro0SINGzZMP/300292b+HCherdu7eeeuoplZeX66OPPjq/cQLAGRCuAADAhXE4pKQkqW9fKS/PWVxQUKB169Zp0ZYtCrjrLrUKDtaMGTOUkpJy3qe66qqr9Ic//EHe3t669dZb1a9fP7355psmDAIALhzhCgAAXJiyMqm4WMrOluLjnQErf/Nm+Xl4yJqbW72/rEwxMTHKr2WG62y1atWqxveCgoIL6T0AmIZwBQAALkxUlJSRIcXEnApYGzcqatIk/WIYKmrZsnp/VJRycnIUFRWlgIAAHTlyxKUZu93u/OzpWft/ouzdu9fle25uriIjIyWpRpv79u1zqVtXmwBgFv6WAQAAF85mcw1YvXopMjdX/fz89Ei3bjocEqLc3Fw9+eSTGj16tGJjY5Wdna3PPvtMJ06c0IIFC3Tw4EFnc1arVfv27dPRo0ddTvPjjz9q2bJlOnHihP75z39q3bp1GjJkiCSpa9eueu+99+RwOFRcXFzj2S6r1XpWz2cBwPkiXAEAAHPYbFJqqkvRyrff1lFPT7Vq1Uq9evXSwIEDNXXqVLVt21YLFizQXXfdpfDwcFVUVKhDhw7O4/r3768bb7xRkZGRCg4OVm5uriQpKSlJX3zxhUJCQvTggw/qjTfeULt27SRJDz30kMLDw2Wz2dS/f39n6Dpp8uTJ+uSTTxQcHKxbbrnFzT8GgCuRh2EYRn134mIrLS2VxWKRw+FQUFBQfXcHAIDLQ15e9S2B2dmnymJiqme0bLYLbn7OnDnaunWrVq9efcFtAcBJZmYDZq4AAMCFOz1YxcRIGza4PoN12iqCAHC5IlwBAIALk5/vGqwyMqSePWsucnEBqwQCQEPgXd8dAAAADVxgoBQaWv359FsATy5yER9fvT8w8IJOM2fOnAs6HgDcjXAFAAAujMUipaVVv+8qKsp1n80mZWZWByuLpX76BwAXCeEKAABcOIul7vD068AFAJcpnrkCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwARuD1dLly5VdHS0/Pz8FBcXp02bNtVZd/v27Ro8eLCio6Pl4eGhxYsX16gzZ84ceXh4uGzt27d34wgAAAAA4Le5NVytWrVKycnJmj17trZs2aLOnTsrMTFRxcXFtdY/cuSIYmJiNH/+fIWFhdXZbocOHbRv3z7n9vnnn7trCAAAAABwVtwarhYtWqR7771XY8eO1bXXXqsXX3xR/v7+evXVV2utf8MNN+jpp5/W0KFD5evrW2e73t7eCgsLc27Nmzc/Yz8qKipUWlrqsgEAAACAmdwWro4dO6bNmzcrISHh1Mk8PZWQkKCsrKwLanvXrl2KiIhQTEyMhg8frtzc3DPWnzdvniwWi3Oz2WwXdH4AAAAA+DW3hasDBw6osrJSVqvVpdxqtcput593u3FxcUpJSVFaWppeeOEF7dmzR71791ZZWVmdx0yfPl0Oh8O55eXlnff5AQAAAKA23vXdgXM1YMAA5+frrrtOcXFxatWqld5++22NHz++1mN8fX3PeJshAAAAAFwot81cNW/eXF5eXioqKnIpLyoqOuNiFecqODhYV111lXbv3m1amwAAAABwrtwWrnx8fNStWzelp6c7y6qqqpSenq4ePXqYdp7y8nL99NNPCg8PN61NAAAAADhXbr0tMDk5WaNHj9b111+v7t27a/HixTp8+LDGjh0rSRo1apQiIyM1b948SdWLYHz//ffOzwUFBdq6dasCAgLUtm1bSdIjjzyiW2+9Va1atVJhYaFmz54tLy8vDRs2zJ1DAQAAAIAzcmu4GjJkiPbv369Zs2bJbrcrNjZWaWlpzkUucnNz5el5avKssLBQXbp0cX5/5pln9Mwzz6hv377KyMiQJOXn52vYsGE6ePCgWrRooZtuuklffPGFWrRo4c6hAAAAAMAZeRiGYdR3Jy620tJSWSwWORwOBQUF1Xd3AAAAANQTM7OBW18iDAAAAABXCsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAC4r0dHRWr16db2cOzc3VwEBAXI4HPVyftQvwhUAAABgkpYtW6q8vFwWi0WSNGbMGE2ePLl+O4WLhnAFAAAAACYgXAEAAOCyVVRUpK5du+ruu++Wh4eHSkpKnPsmT56sMWPGSJImTJigadOmSZIMw1CLFi00dOhQZ91u3brp3XfflSQtWrRI7dq1U2BgoNq0aaMlS5Y46+Xk5DjP89xzz2nFihX629/+poCAAHXo0MH9A0a9IlwBAACg4XI4pPz8Wnft/uwz3dSzp0aOHKkFCxacsZl+/fpp/fr1kqRvv/1WQUFByszMlCT9/PPP+vbbb9WvXz9JUqtWrbRu3TqVlpbq5Zdf1pQpU7Rhw4YabT7wwAMaPny4/vSnP6m8vFzbt2+/kJGiASBcAQAAoGFyOKSkJKlvXykvz2XX1+vWKb5fP/3Z01MPjRv3m03Fx8dry5YtKi0t1bp16zR48GA1b95c33//vTIyMtSxY0eFhIRIkgYPHiybzSYPDw/169dPiYmJysjIcMcI0cAQrgAAANAwlZVJxcVSdrYUH38qYFVW6uW//U1tKit1d2Vldb3fEBoaqquvvlqfffaZ1q1bp379+unmm2/W+vXrtW7dOvXv399Zd8WKFeratatCQkIUHBysNWvW6MCBA24aJBoSwhUAAAAapqgoKSNDiok5FbA2bpTsdi2urJRf48b676uu0nGrVQEBAZKkI0eOOA/ft2+fS3P9+vXT2rVrlZWVpd69e6t///7OcHXylsDc3FyNHj1aCxYsUHFxsUpKSvT73/9ehmHU2kVPT/5z+0rC/9oAAABouGw214DVq5d04oT8rFb9v23bVOHpqcGDBysoKEgtW7bU8uXLVVVVpfXr12vNmjUuTfXr10+vvfaarrrqKgUEBKhv375KT0/Xjz/+qD59+kiSysvLZRiGQkND5enpqTVr1uhf//pXnd2zWq3Kzs6uM3zh8kK4AgAAQMNms0mpqa5lkyfLr107vf/++zIMQ3fccYf+/ve/67XXXpPFYtFLL73kshqgVP3cVVlZmfMWQIvFoquuukrdunVTUFCQJOnaa6/VjBkz1L9/fzVr1kyrVq3SbbfdVmfX/ud//kcFBQUKCQnRddddZ+64ccnxMK7AGF1aWiqLxSKHw+H8gwIAAIAGKi+v+pbA7OxTZTEx1TNaNlt99QoNhJnZgJkrAAAANFynB6uYGGnDBtdnsH61iiDgToQrAAAANEz5+a7BKiND6tmz5iIXdbwHCzCbd313AAAAADgvgYFSaGj159NvATy5yEV8fPX+wMB66iCuNIQrAAAANEwWi5SWVv0eq6go1302m5SZWR2sLJb66R+uOIQrAAAANFwWS93h6deBC3AznrkCAAAAABMQrgAAAADABIQrAAAAADCB28PV0qVLFR0dLT8/P8XFxWnTpk111t2+fbsGDx6s6OhoeXh4aPHixRfcJgAAAABcDG4NV6tWrVJycrJmz56tLVu2qHPnzkpMTFRxcXGt9Y8cOaKYmBjNnz9fYWFhprQJAAAAABeDh2EYhrsaj4uL0w033KAlS5ZIkqqqqmSz2TRp0iRNmzbtjMdGR0dr8uTJmjx5smltnlRaWiqLxSKHw6GgoKBzHxgAAACAy4KZ2cBtM1fHjh3T5s2blZCQcOpknp5KSEhQVlbWRW2zoqJCpaWlLhsAAABwpQoODlZGRkZ9d+Oy47ZwdeDAAVVWVspqtbqUW61W2e32i9rmvHnzZLFYnJvt5Nu7AQAAAMAkV8RqgdOnT5fD4XBueXl59d0lAAAAoMExDEOVlZX13Y1LltvCVfPmzeXl5aWioiKX8qKiojoXq3BXm76+vgoKCnLZAAAAgIutvLxcEydOVMuWLRUaGqpRo0bJ4XBIknbt2qXbbrtNLVq0UEhIiO68805JUkZGhoKDg13aGTRokObMmeNs8/bbb1doaKgsFov69Omjbdu2OetWVVVp5syZslqtioiI0NKlS13aMgxDCxcuVJs2bRQSEqKkpCRlZ2c790dHR2vevHm68cYb5e/vr++//94Nv8zlwW3hysfHR926dVN6erqzrKqqSunp6erRo8cl0yYAAADgFg6HlJ/vUjRu3DgdOnRI3370kfZs3arjx49r4sSJOnz4sBISEtSxY0fl5OTIbrdr0qRJZ3Waqqoq3XPPPdqzZ4+KiorUpUsX3X333Tq5bl1KSopSUlKUmZmp3bt36+uvv1ZZWZnz+NTUVC1atEirV69WYWGhOnTooFtvvVUnTpxw1klJSdHy5ctVXl6uq6++2oQf5/Lk1tsCk5OTtWzZMi1fvlw7duzQhAkTdPjwYY0dO1aSNGrUKE2fPt1Z/9ixY9q6dau2bt2qY8eOqaCgQFu3btXu3bvPuk0AAACg3jkcUlKS1Lev9J9HUvbv3693331XS6dPV/Btt6nJ4MH6y5QpWrVqlT788EM1atRITz75pJo0aSIfHx/169fvrE4VFBSkIUOGqEmTJvLz89Of//xn/fjjjyosLJQkrVixQpMmTVL79u3l7++v+fPnq6qqynl8amqqHnjgAXXq1El+fn7661//qry8PJd3yU6YMEFXX321vLy85OPjY+IPdXnxdmfjQ4YM0f79+zVr1izZ7XbFxsYqLS3NuSBFbm6uPD1P5bvCwkJ16dLF+f2ZZ57RM888o759+zpXM/mtNgEAAIB6V1YmFRdL2dlSfLyUkaEcu11VVVVqHRsrVVVJOTlSv37y9PTUDz/8oDZt2sjDw+OcT3X06FE9/PDDWrNmjQ4dOuT87+sDBw4oMjJShYWFatWqlbO+1WqVr6+v83t+fr6io6Od3319fRUREaH802bdWrZsec79uhK5fUGLiRMnau/evaqoqNCXX36puLg4576MjAylpKQ4v0dHR8swjBrbr5eJPFObAAAAQL2LipIyMqSYGGfAstnt8pRUWFWlkpgYleTkqMTh0C+//KL27dvrp59+Um2voA0ICNDRo0dd9u3bt8/5eeHChdq8ebM+//xzlZaWKicnR5Kc9SMiIrR3715n/eLiYlVUVJzW1SjnMVL13WSFhYWKiopylp0+IYK68SsBAAAA7mCzuQSssNtu0yBJEwMCdOC99ySbTXa7Xe+//74GDhyoiooKzZo1S4cPH9axY8e0fv16SdJVV12lRo0aaeXKlaqsrNSbb76pb775xnma0tJS+fn5qWnTpiovL9djjz3m0o1hw4Zp6dKl2rlzp44eParp06e7hKURI0ZoyZIl+v7771VRUaHHH39ckZGR6t69+0X4kS4vhCsAAADAXWw2KTXV+TVFUvDAgbph0CAFBQWpd+/e2rx5swICAvTJJ59o8+bNatmypcLDw52r+gUFBWnZsmWaNm2amjVrpg0bNigxMdHZZnJysry8vGS1WtWxY8caC72NGzdOI0aMUO/evRUTE6MuXbooMDDQuX/UqFGaNGmSbrnlFoWFhWnbtm364IMP5O3t1ieILkseRm1zj5e50tJSWSwWORwOlmUHAACA++TlVT9zddrS5oqJqZ7RstnOqakOHTroqaee0i233GJqF690ZmYDZq4AAAAAdzg9WMXESBs2uDyDdXIVwbO1fft2gtUljnAFAAAAmC0/3zVYZWRIPXvWWOTi1+/BOh/Hjx+/4DZgDsIVAAAAYLbAQCk0tOYtgKcvchEaWl3vLEVHR2v16tVKSUlRbGysZs+erbCwMA0dOlTl5eW6/fbbFRoaKovFoj59+mjbtm3OY6uqqjRz5kxZrVZFRERo6dKlCg4OrrEqNy4M4QoAAAAwm8UipaVJmZk1n62y2arL09Kq69XG4ah7VuvQIX333Xfy9vZWbm6uUlNTVVVVpXvuuUd79uxRUVGRunTporvvvtu5HHtKSopSUlKUmZmp3bt36+uvv1ZZWZmJA4ZEuAIAAADcw2Kpft9VbaKizhyskpKkvn1rPpd14IA0f74sHh6aMXGifHx85O/vr6CgIA0ZMkRNmjSRn5+f/vznP+vHH39UYWGhJGnFihWaNGmS2rdvL39/f82fP19VVVUmDhYS4QoAAAC4tJSVScXFNRe+qKyUHn9c2r9fkZ6e8jx82HnI0aNH9ac//UnR0dEKCgpSdHS0JOnAgQOSpMLCQrVq1cpZ32q1ytfX92KN6IpBuAIAAAAuJVFRNRe+2LhRstuloiKpRQt5tm3rMiu2cOFCbd68WZ9//rlKS0uVk5MjSc7bAiMiIrR3715n/eLiYlVUVFzEQV0ZCFcAAADApeb0hS+ys6VevaQTJySrVZo2TWrUyKV6aWmp/Pz81LRpU5WXl+uxxx5z2T9s2DAtXbpUO3fu1NGjRzV9+nR5ehIFzMYvCgAAAFyKbDYpNdW1bPJkKSSkRtXk5GR5eXnJarWqY8eO6tGjh8v+cePGacSIEerdu7diYmLUpUsXBZ7DSoU4Ox7GybnCK4iZb2EGAAAA3OL0lxCf9Oul3S9AcHCwVq9erfj4+AtuqyEzMxswcwUAAABcak4PVjEx0oYNrs9g/XoVQVwSCFcAAADApSQ/3zVYZWRIPXvWXOSirvdgod5413cHAAAAAJwmMFAKDa3+fPotgCcXuYiPr95/gc9MlZSUXNDxqIlwBQAAAFxKLBYpLa36fVe/fgmxzSZlZlYHq7peQox6Q7gCAAAALjUWS93h6deBC5cMnrkCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAACAKT777DNFRUU5v8fHx2vx4sX116GLjHAFAAAAwBS9e/dWfn5+fXej3hCuAAAAAMAEhCsAAAAA56SoqEh33323WrRooZYtW2rGjBk6ceKEMjIyFBwcXN/dqzcXJVwtXbpU0dHR8vPzU1xcnDZt2nTG+u+8847at28vPz8/derUSWvWrHHZP2bMGHl4eLhsSUlJ7hwCAAAAgP+455571KhRI+3Zs0efffaZVq9erQULFtR3t+qd28PVqlWrlJycrNmzZ2vLli3q3LmzEhMTVVxcXGv9jRs3atiwYRo/fry++eYbDRo0SIMGDdJ3333nUi8pKUn79u1zbm+++aa7hwIAAABcWRwO6VfPUBUUFGjdunVa9MgjCqisVKtWrTRjxgylpKTUTx8vIW4PV4sWLdK9996rsWPH6tprr9WLL74of39/vfrqq7XWf/bZZ5WUlKQpU6bommuu0dy5c9W1a1ctWbLEpZ6vr6/CwsKcW9OmTd09FAAAAODK4XBISUlS375SXp6zOD8/X36+vrLedVf1fodDMTExV/RCFie5NVwdO3ZMmzdvVkJCwqkTenoqISFBWVlZtR6TlZXlUl+SEhMTa9TPyMhQaGiorr76ak2YMEEHDx6ssx8VFRUqLS112QAAAACcQVmZVFwsZWdL8fHOgBXl6alfKipUlJ1dvb+sTDk5OS5LsF+p3BquDhw4oMrKSlmtVpdyq9Uqu91e6zF2u/036yclJen1119Xenq6nnrqKWVmZmrAgAGqrKystc158+bJYrE4N5vNdoEjAwAAAC5zUVFSRoYUE3MqYG3cqMihQ9VP0iMBATq8Zo1yq6r05JNPavTo0fXc4frnXd8dOB9Dhw51fu7UqZOuu+46tWnTRhkZGbr55ptr1J8+fbqSk5Od30tLSwlYAAAAwG+x2aoDVnx8dcDq1UuStLJlS03s0EGtevVS48aNNXz4cE2dOlUbNmyo1+7WN7eGq+bNm8vLy0tFRUUu5UVFRQoLC6v1mLCwsHOqL0kxMTFq3ry5du/eXWu48vX1la+v73mMAAAAALjC2WxSaqozWElS2Jtv6v969qxRNT4+XiUlJc7vGRkZF6GDlw633hbo4+Ojbt26KT093VlWVVWl9PR09ejRo9ZjevTo4VJfktauXVtnfan6obqDBw8qPDzcnI4DAAAAqJaXJ40c6Vo2cqTLIheo5vbVApOTk7Vs2TItX75cO3bs0IQJE3T48GGNHTtWkjRq1ChNnz7dWf/BBx9UWlqaFi5cqB9++EFz5szR119/rYkTJ0qSysvLNWXKFH3xxRfKyclRenq6br/9drVt21aJiYnuHg4AAABw5cjLO3VLYEyMtGGD6zNYBCwXbn/masiQIdq/f79mzZolu92u2NhYpaWlORetyM3NlafnqYzXs2dPrVy5Uo8//rgee+wxtWvXTqtXr1bHjh0lSV5eXvr222+1fPlylZSUKCIiQr/73e80d+5cbv0DAAAAzJKf7xqsMjJqPoMVHy9lZlYvfgF5GIZh1HcnLrbS0lJZLBY5HA4FBQXVd3cAAACAS8/J91wVF58KViednNEKDZXS0iSLpb56ecHMzAYNcrVAAAAAAG5msVQHp7KymjNTNlv1jFVgYIMOVmYjXAEAAAConcVSd3jiVsAa3L6gBQAAAABcCQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACYgHAFAAAAACYgXAEAAAC4rB0/fvyinIdwBQAAAKBelJeXa+LEiWrZsqVCQ0M1atQoORwO5eTkyMPDQ6mpqWrbtq2Cg4M1ZswYl5C0ZcsW9evXTyEhIWrbtq2WLVvm3DdnzhzdcsstmjBhgkJCQjRt2jRVVFToj3/8o0JCQtS6dWu98sor8vDw0N69eyVJkZGRKi8vd7ZRUFAgX19fFRYWnvV4CFcAAAAA3M/hkPLzXYrGjRunQ4cO6duPPtKerVt1/PhxTZw40bn/o48+0jfffKPvv/9e6enpWrFihSTJbrfrv/7rvzRhwgTt379fq1ev1uzZs5Wenu48Ni0tTXFxcSouLtbcuXP1xBNP6Ouvv9b27du1detWvf/++y59adu2rf7v//7P+f31119XQkKCIiIiznqIhCsAAAAA7uVwSElJUt++Ul6eJGn//v169913tXT6dAXfdpuaDB6sv0yZolWrVqmyslKSNGvWLAUGBioiIkJJSUnavHmzJCk1NVV9+vTR3XffLS8vL3Xs2FFjx47VypUrnafs2LGjxowZI29vb/n7+2vlypWaNm2awsPDZbFYNHv2bJcujhw5UikpKc7vy5cv19ixY89pmN7n89sAAAAAwFkrK5OKi6XsbCk+XsrIUI7drqqqKrWOjZWqqqScHKlfP3l6esput0uSwsLCnE00adJEJSUlkqScnBytWbNGwcHBzv2VlZXq3bu383vLli1dulBYWCibzVbn/rvuukszZ87Unj17ZLfbdeDAAd12223nNExmrgAAAAC4V1SUlJEhxcQ4A5bNbpenpMKqKpXExKgkJ0clDod++eUXRUZGnrE5m82mO+64QyUlJc6trKxMa9ascdbx9HSNOhEREcr7z6yZJOXm5rrsDw4O1h133KHly5crJSVFw4cPl4+PzzkNk3AFAAAAwP1sNpeAFXbbbRokaWJAgA68955ks8lut9d4Fqo2I0eO1Lp16/Tuu+/q+PHjOn78uLZu3aqvvvqqzmOGDRumBQsWyG63y+FwaO7cuTXqjB8/XikpKVq1apXGjRt3zkMkXAEAAAC4OGw2KTXV+TVFUvDAgbph0CAFBQWpd+/ezueqziQyMlIff/yxXnrpJYWHh8tqter+++9XaWlpncc8/vjj6ty5s6699lrFxsbq97//vSTJ19fXWSc+Pl5eXl6KiYlR586dz3l4HoZhGOd8VANXWloqi8Uih8OhoKCg+u4OAAAAcGXIy6t+5io7+1RZTEz1jNZpz0NdDFlZWYqPj1dxcbGCg4Od2aB///668847XVYtPFvMXAEAAABwv9ODVUyMtGGDyzNYOu15KHcoLi7W+vXrVVlZqcLCQk2fPl2DBw+Wh4eHs05WVpa+/vprjRw58rzOQbgCAAAA4F75+a7BKiND6tmzxiIXv34PlpkqKyv10EMPyWKxKDY2VpGRkXr++eed+++8804NGDBAzz77rCwWy3md46KEq6VLlyo6Olp+fn6Ki4vTpk2bzlj/nXfeUfv27eXn56dOnTq5rPohSYZhaNasWQoPD1fjxo2VkJCgXbt2uXMIAAAAAM5XYKAUGlrzFsDTF7kIDa2u5ybh4eHaunWrysvLVVxcrBUrVqhZs2bO/e+9955KSkrO+d1Wp3N7uFq1apWSk5M1e/ZsbdmyRZ07d1ZiYqKKi4trrb9x40YNGzZM48eP1zfffKNBgwZp0KBB+u6775x1FixYoOeee04vvviivvzySzVp0kSJiYn65Zdf3D0cAAAAAOfKYpHS0qTMzJrPVtls1eVpadX1GjC3L2gRFxenG264QUuWLJEkVVVVyWazadKkSZo2bVqN+kOGDNHhw4f14YcfOstuvPFGxcbG6sUXX5RhGIqIiNDDDz+sRx55RJLkcDhktVqVkpKioUOH/mafWNACAAAAgGRuNnDrzNWxY8e0efNmJSQknDqhp6cSEhKUlZVV6zFZWVku9SUpMTHRWf/kG5NPr2OxWBQXF1dnmxUVFSotLXXZAAAAAMBMbg1XBw4cUGVlpaxWq0u51WqV3W6v9Ri73X7G+if/eS5tzps3TxaLxbnZLvIyjwAAAAAuf1fEaoHTp0+Xw+FwbnluXuYRAAAAwJXHreGqefPm8vLyUlFRkUt5UVGRwsLCaj0mLCzsjPVP/vNc2vT19VVQUJDLBgAAAABmcmu48vHxUbdu3ZSenu4sq6qqUnp6unr06FHrMT169HCpL0lr16511m/durXCwsJc6pSWlurLL7+ss00AAAAAcDdvd58gOTlZo0eP1vXXX6/u3btr8eLFOnz4sHP9+FGjRikyMlLz5s2TJD344IPq27evFi5cqIEDB+qtt97S119/rb///e+SJA8PD02ePFlPPPGE2rVrp9atW2vmzJmKiIjQoEGD3D0cAAAAAKiV28PVkCFDtH//fs2aNUt2u12xsbFKS0tzLkiRm5srT89TE2g9e/bUypUr9fjjj+uxxx5Tu3bttHr1anXs2NFZZ+rUqTp8+LDuu+8+lZSU6KabblJaWpr8/PzcPRwAAAAAqJXb33N1KeI9VwAAAACkBvSeKwAAAAC4UhCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwAeEKAAAAAExAuAIAAAAAExCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAB16tChgz788MPzOjY4OFgZGRnmdugS5l3fHQAAAABw6dq+fXu9nj8nJ0etW7fWzz//rODg4Hrty29h5goAAABAg3b8+PH67oIkN4arQ4cOafjw4QoKClJwcLDGjx+v8vLyMx7zyy+/6P7771ezZs0UEBCgwYMHq6ioyKWOh4dHje2tt95y1zAAAACAK1p0dLRWr16tlJQUxcbGau7cuQoNDZXVatXixYud9aqqqjRz5kxZrVZFRERo6dKlLu2MGTNGkydPdn4vKSmRh4eHcnJyJElr167Vddddp8DAQFmtVk2YMEGS1L17d0lSVFSUAgICtGLFCmVkZCg4OFgvvPCCWrZsqZ49e+qOO+7QnDlzXM75xz/+0dnOxeC2cDV8+HBt375da9eu1YcffqhPP/1U99133xmPeeihh/TBBx/onXfeUWZmpgoLC3XnnXfWqPfaa69p3759zm3QoEFuGgUAAACAk7Zv3y5/f38VFBRo1apVmjJlin766SdJUkpKilJSUpSZmandu3fr66+/VllZ2Vm3PXr0aE2ZMkVlZWXKzs7WyJEjJUmbNm2SJOXn56u8vFzDhw+XJJWVlWnbtm364YcflJmZqfHjx+v111+XYRiSqidu3nrrLY0bN87Mn+CM3BKuduzYobS0NL388suKi4vTTTfdpOeff15vvfWWCgsLaz3G4XDolVde0aJFi9S/f39169ZNr732mjZu3KgvvvjCpW5wcLDCwsKcm5+f3xn7U1FRodLSUpcNAAAAwK84HFJ+fu37Dh1S82bN9PDDD6tRo0aKj49XdHS0tm7dKklasWKFJk2apPbt28vf31/z589XVVXVWZ+6UaNG2r17t/bv368mTZqoZ8+eZ6xfVVWl+fPny9/fX/7+/howYIAqKiqUmZkpSXr//fcVFRWlG2644az7cKHcEq6ysrIUHBys66+/3lmWkJAgT09Pffnll7Ues3nzZh0/flwJCQnOsvbt26tly5bKyspyqXv//ferefPm6t69u1599VVnOq3LvHnzZLFYnJvNZruA0QEAAACXIYdDSkqS+vaV8vJc9x04IM2fL2tZWXW9/2jSpIlzdqqwsFCtWrVy7rNarfL19T3r07///vv67rvvdPXVV6tLly56++23z1g/MDDQZYELLy8vjRo1SikpKZKqZ9Iu5qyV5KZwZbfbFRoa6lLm7e2tkJAQ2e32Oo/x8fGpsQKI1Wp1OeYvf/mL3n77ba1du1aDBw/Wn/70Jz3//PNn7M/06dPlcDicW96vLxYAAADgSldWJhUXS9nZUnz8qYBVWSk9/ri0f7904kR1vVpERERo7969zu/FxcWqqKhwfg8ICNCRI0ec3/ft2+dyfNeuXfXuu+/qwIEDmjlzpu655x4VFRXJ07P2yFJb+bhx4/Tuu+9q586dyszM1IgRI8529KY4p3A1bdq0WheUOH374Ycf3NVXSdLMmTPVq1cvdenSRY8++qimTp2qp59++ozH+Pr6KigoyGUDAAAAcJqoKCkjQ4qJORWwNm6U7HapqEhq0UJq27a6Xi2GDRumpUuXaufOnTp69KimT5/uEoC6du2qjz/+WPv27VNZWZn+/Oc/O/cdO3ZMqamp+vnnn+Xp6emccPH29laLFi3k6enpfLbrTNq1a6euXbtqyJAhGjBgQI0JH3c7p3D18MMPa8eOHWfcYmJiFBYWpuLiYpdjT5w4oUOHDiksLKzWtsPCwnTs2DGVlJS4lBcVFdV5jCTFxcUpPz/fJRUDAAAAOA82m2vA6tWrerbKapWmTZMaNarz0HHjxmnEiBHq3bu3YmJi1KVLFwUGBjr3jxgxQn379lX79u0VGxurgQMHuhy/cuVKtW3bVoGBgZo0aZJWrlypZs2aqXHjxpo9e7YGDBig4OBgrVy58oxDGD9+vLZt26axY8de0E9xPjyM33pg6Tzs2LFD1157rb7++mt169ZNkvSvf/1LSUlJys/PV0RERI1jHA6HWrRooTfffFODBw+WJO3cuVPt27dXVlaWbrzxxlrP9eSTT2rhwoU6dOjQWfevtLRUFotFDoeDWSwAAADg1zZurA5WJ23YIP3GAhOXik8//VR333238vPz5e3t/Zv1zcwGv32283DNNdcoKSlJ9957r1588UUdP35cEydO1NChQ53BqqCgQDfffLNef/11de/eXRaLRePHj1dycrJCQkIUFBSkSZMmqUePHs5g9cEHH6ioqEg33nij/Pz8tHbtWv31r3/VI4884o5hAAAAAFeevDzpP8ugO40cWT2jdYkvDHfs2DEtXLhQ995771kFK7O57T1XK1asUPv27XXzzTfr97//vW666Sb9/e9/d+4/fvy4du7c6fJQ2//+7//qlltu0eDBg9WnTx+FhYXpvffec+5v1KiRli5dqh49eig2NlYvvfSSFi1apNmzZ7trGAAAAMCVIy+v+lmr7OzqWwM3bHB9BusSXhguMzNTTZs21YEDBzRlypR66YNbbgu81HFbIAAAAPAr+fnVy7CfDFYnZ6p+HbgyM+tc1KIhuuRvCwQAAADQwAQGSidX1zv9FsCTi1zEx1fvP22RCrgiXAEAAACQLBYpLa36PVa/npmy2apnrAIDq+uhVoQrAAAAANUslrrD02V0K6C7uG1BCwAAAAC4khCuAAAAAMAEhCsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAABckOjoaK1evbq+u1HvCFcAAADAFSojI0PBwcH13Y3LBuEKAAAAQJ2OHz9e311oMAhXAAAAQAMXHR2tJ598Ul27dlVQUJASExNVWFgoSSouLtbw4cMVHh6uiIgITZ48WRUVFTp48KAGDBggh8OhgIAABQQE6LPPPlNKSopiY2M1e/ZshYWFaejQoTIMQwsXLlSbNm0UEhKipKQkZWdn19mfTz75RN27d1dwcLA6dOigf/zjH8598fHxWrx4sfP71q1b5eHh4bJ/6tSpuvnmm9WkSRPdeOONKigo0Jw5c9SiRQtFRUXp/fffN/9HNAHhCgAAAGhIHA4pP79G8csvv6yVixbJvnOnwsLCNGLECBmGodtuu01hYWH66aef9O9//1vbtm3TE088oWbNmumjjz6SxWJReXm5ysvL1bt3b0nSd999J29vb+Xm5io1NVWpqalatGiRVq9ercLCQnXo0EG33nqrTpw4UaMf3377rf77v/9b8+fP16FDh/TSSy9p5MiR2rlz51kP8c0339Rzzz2nQ4cOKTAwUH379lVISIj27dunP//5z7r33nsvyRk1whUAAADQUDgcUlKS1LevlJfnsmvCsGFqP368/O+8Uwsef1zr16/X559/rl27dunpp5+Wv7+/mjVrpscee0wrV64842ksFotmzJghHx8f+fv7KzU1VQ888IA6deokPz8//fWvf1VeXp42bdpU49iXXnpJY8aMUf/+/eXp6ambbrpJt9xyi95+++2zHuaIESPUoUMH+fr66o477tDhw4f1wAMPyNvbW8OGDdPBgwe1d+/es27vYvGu7w4AAAAAOEtlZVJxsZSdLcXHSxkZks0mVVaq1SuvVO+TZG3cWL6+vtq4caNKSkoUEhLibMIwDFVWVp7xNJGRkfL0PDUPk5+fr+joaOd3X19fRUREKL+WGbScnBytW7dOr732mrPsxIkTCgoKOuthWq1W52d/f/8a3yWpvLz8rNu7WAhXAAAAQEMRFVUdqOLjTwWs1FTJbtfeEyekmBgpI0PFPj6qqKhQr169FBoaqn379tXa3OkB6kzlUVFRysnJcX4/duyYCgsLFRUVVeNYm82mBx98UPPnz6+17YCAAB05csT5va6+NUTcFggAAAA0JDZbdcCKiakOWL16SSdO6CVvb+185RUdbd5cjz76qPr06aMePXrIZrPp8ccfV1lZmQzD0N69e/XRRx9Jqp4hKisrU/F/ZrzqMmLECC1ZskTff/+9Kioq9PjjjysyMlLdu3evUfcPf/iDXnvtNa1fv16VlZWqqKhQVlaWduzYIUnq2rWr3nvvPTkcDhUXF2vBggWm/0T1hXAFAAAANDQ2W/WM1WnGjRunYcnJslqtKigo0IoVK+Tl5aUPP/xQBQUFuuaaa2SxWDRw4EDt3r1bknT11Vdr/PjxuvbaaxUcHKzPP/+81tONGjVKkyZN0i233KKwsDBt27ZNH3zwgby9a94I16VLF7355pt6/PHH1aJFC0VGRmrmzJmqqKiQJD300EMKDw+XzWZT//79NWTIEJN/nPrjYRiGUd+duNhKS0tlsVjkcDjO6d5PAAAA4JKQl3fq1kBJ0ZIWW60a9NVX1cELZ83MbMDMFQAAANCQnB6sYmKkDRskb2+pqKi6/FerCOLiIVwBAAAADUV+vmuwysiQevaUwsIkq/XUIhe1rOIH92O1QAAAAKChCAyUQkOrP59chl1STl7eqRmt0NDqerjoCFcAAABAQ2GxSGlp1e+7+vUy6DablJlZHawslvrp3xWOcAUAAAA0JBZL3eGplvdO4eLhmSsAAAAAMAHhCgAAAABMQLgCAAAAABMQrgAAAADABIQrAAAAADAB4QoAAAAATEC4AgAAAAATEK4AAAAAwASEKwAAAAAwgXd9d6A+GIYhSSotLa3nngAAAACoTyczwcmMcCGuyHBVVlYmSbLZbPXcEwAAAACXgrKyMlkslgtqw8MwI6I1MFVVVSosLFRgYKA8PDzquzuXldLSUtlsNuXl5SkoKKi+u4MrHNcjLhVci7hUcC3iUnEpXYuGYaisrEwRERHy9Lywp6auyJkrT09PRUVF1Xc3LmtBQUH1/gcFOInrEZcKrkVcKrgWcam4VK7FC52xOokFLQAAAADABIQrAAAAADAB4Qqm8vX11ezZs+Xr61vfXQG4HnHJ4FrEpYJrEZeKy/VavCIXtAAAAAAAszFzBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBc4TctXbpU0dHR8vPzU1xcnDZt2lRn3ePHj+svf/mL2rRpIz8/P3Xu3FlpaWk16hUUFGjEiBFq1qyZGjdurE6dOunrr7925zBwGTD7WoyOjpaHh0eN7f7773f3UNDAmX0tVlZWaubMmWrdurUaN26sNm3aaO7cuWJBX/wWs6/FsrIyTZ48Wa1atVLjxo3Vs2dPffXVV+4eBhq4Tz/9VLfeeqsiIiLk4eGh1atX/+YxGRkZ6tq1q3x9fdW2bVulpKTUqHMu1/clwwDO4K233jJ8fHyMV1991di+fbtx7733GsHBwUZRUVGt9adOnWpEREQY//znP42ffvrJ+Nvf/mb4+fkZW7ZscdY5dOiQ0apVK2PMmDHGl19+aWRnZxsff/yxsXv37os1LDRA7rgWi4uLjX379jm3tWvXGpKM9evXX6RRoSFyx7X45JNPGs2aNTM+/PBDY8+ePcY777xjBAQEGM8+++zFGhYaIHdci3fffbdx7bXXGpmZmcauXbuM2bNnG0FBQUZ+fv7FGhYaoDVr1hgzZsww3nvvPUOS8f7775+xfnZ2tuHv728kJycb33//vfH8888bXl5eRlpamrPOuV7flwrCFc6oe/fuxv333+/8XllZaURERBjz5s2rtX54eLixZMkSl7I777zTGD58uPP7o48+atx0003u6TAuW+64Fn/twQcfNNq0aWNUVVWZ02lcltxxLQ4cONAYN27cGesAv2b2tXjkyBHDy8vL+PDDD13qdO3a1ZgxY4bJvcfl6mzC1dSpU40OHTq4lA0ZMsRITEx0fj/X6/tSwW2BqNOxY8e0efNmJSQkOMs8PT2VkJCgrKysWo+pqKiQn5+fS1njxo31+eefO7//4x//0PXXX6///u//VmhoqLp06aJly5a5ZxC4LLjrWvz1Od544w2NGzdOHh4e5nUelxV3XYs9e/ZUenq6fvzxR0nStm3b9Pnnn2vAgAFuGAUuB+64Fk+cOKHKyspz+rsTOB9ZWVku164kJSYmOq/d87m+LxWEK9TpwIEDqqyslNVqdSm3Wq2y2+21HpOYmKhFixZp165dqqqq0tq1a/Xee+9p3759zjrZ2dl64YUX1K5dO3388ceaMGGCHnjgAS1fvtyt40HD5a5r8XSrV69WSUmJxowZY3b3cRlx17U4bdo0DR06VO3bt1ejRo3UpUsXTZ48WcOHD3freNBwueNaDAwMVI8ePTR37lwVFhaqsrJSb7zxhrKysur8uxM4H3a7vdZrt7S0VEePHj2v6/tSQbiCqZ599lm1a9dO7du3l4+PjyZOnKixY8fK0/PUpVZVVaWuXbvqr3/9q7p06aL77rtP9957r1588cV67DkuN2dzLZ7ulVde0YABAxQREXGRe4rL3dlci2+//bZWrFihlStXasuWLVq+fLmeeeYZ/k8nmOpsrsXU1FQZhqHIyEj5+vrqueee07Bhw+r8uxOAK/6koE7NmzeXl5eXioqKXMqLiooUFhZW6zEtWrTQ6tWrdfjwYe3du1c//PCDAgICFBMT46wTHh6ua6+91uW4a665Rrm5ueYPApcFd12LJ+3du1effPKJ/ud//sct/cflw13X4pQpU5yzV506ddLIkSP10EMPad68eW4dDxoud12Lbdq0UWZmpsrLy5WXl6dNmzbp+PHjtf7dCZyvsLCwWq/doKAgNW7c+Lyu70sF4Qp18vHxUbdu3ZSenu4sq6qqUnp6unr06HHGY/38/BQZGakTJ07o3Xff1e233+7c16tXL+3cudOl/o8//qhWrVqZOwBcNtx1LZ702muvKTQ0VAMHDjS977i8uOtaPHLkSI2ZAS8vL1VVVZk7AFw23P33YpMmTRQeHq6ff/5ZH3/8ca11gPPVo0cPl2tXktauXeu8di/k+q539b2iBi5tb731luHr62ukpKQY33//vXHfffcZwcHBht1uNwzDMEaOHGlMmzbNWf+LL74w3n33XeOnn34yPv30U6N///5G69atjZ9//tlZZ9OmTYa3t7fx5JNPGrt27TJWrFhh+Pv7G2+88cbFHh4aEHdci4ZRvfpQy5YtjUcfffRiDgcNmDuuxdGjRxuRkZHOpdjfe+89o3nz5sbUqVMv9vDQgLjjWkxLSzM++ugjIzs72/jXv/5ldO7c2YiLizOOHTt2sYeHBqSsrMz45ptvjG+++caQZCxatMj45ptvjL179xqGYRjTpk0zRo4c6ax/cin2KVOmGDt27DCWLl1a61LsZ7q+L1WEK/ym559/3mjZsqXh4+NjdO/e3fjiiy+c+/r27WuMHj3a+T0jI8O45pprDF9fX6NZs2bGyJEjjYKCghptfvDBB0bHjh0NX19fo3379sbf//73izEUNHDuuBY//vhjQ5Kxc+fOizEEXCbMvhZLS0uNBx980GjZsqXh5+dnxMTEGDNmzDAqKiou1pDQQJl9La5atcqIiYkxfHx8jLCwMOP+++83SkpKLtZw0ECtX7/ekFRjO3n9jR492ujbt2+NY2JjYw0fHx8jJibGeO2112q0e6br+1LlYRi8/h0AAAAALhTPXAEAAACACQhXAAAAAGACwhUAAAAAmIBwBQAAAAAmIFwBAAAAgAkIVwAAAABgAsIVAAAAAJiAcAUAAAAAJiBcAQAAAIAJCFcAAAAAYALCFQAAAACY4P8D4Rg5sY8sw/cAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Run This Cell to Produce Your Plot\n",
        "# ------------------------------\n",
        "reuters_corpus = read_corpus()\n",
        "M_co_occurrence, word2ind_co_occurrence = compute_co_occurrence_matrix(reuters_corpus)\n",
        "M_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, k=2)\n",
        "\n",
        "M_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\n",
        "M_normalized = M_reduced_co_occurrence / M_lengths[:, np.newaxis] # broadcasting\n",
        "\n",
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']\n",
        "\n",
        "plot_embeddings(M_normalized, word2ind_co_occurrence, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GHSZOlwcpwT"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>\n",
        "\n",
        "\n",
        "**The overall process begins with the construction of a co-occurrence matrix using the Reuters \"crude\" dataset.** A fixed context window of 4 is employed, meaning that for each target word in the corpus, the four preceding and four succeeding words are considered its context. This windowed approach ensures that the matrix captures meaningful local word associations, effectively representing how frequently words appear together within a limited textual neighborhood. The resulting matrix reflects raw co-occurrence statistics and serves as the foundational structure for creating word vectors.\n",
        "\n",
        "**Due to the high dimensionality of the co-occurrence matrix, dimensionality reduction is essential for visualization and analysis.** To achieve this, Truncated Singular Value Decomposition (TruncatedSVD) is applied, which is a linear dimensionality reduction technique similar to Principal Component Analysis (PCA) but optimized for sparse matrices. This projection reduces each word vector to two dimensions, allowing for visualization on a 2D plane. The resulting vectors are normalized using NumPy broadcasting to approximate unit vectors, thus highlighting vector direction over magnitude—an important step when focusing on relational similarities rather than frequency.\n",
        "\n",
        "**The resulting 2D word embeddings reveal several distinct clusters that align with semantic and topical similarities.** For example, \"petroleum\" and \"industry\" form a tight cluster, reflecting their frequent co-occurrence in texts related to the petroleum sector. Likewise, \"energy\" and \"oil\" are positioned closely, as they commonly appear together in discussions surrounding energy markets and policies. Another notable cluster includes the country names \"ecuador\", \"iraq\", and \"kuwait\", which are frequently mentioned together due to their significance in global oil exports and related geopolitical discourse.\n",
        "\n",
        "**These clusters provide insightful information about shared contexts and thematic groupings.** The proximity of oil-exporting countries, for instance, demonstrates how co-occurrence captures semantic similarity based on shared topical relevance rather than lexical similarity. Even though these country names are not synonyms, their repeated joint appearance in oil-related articles brings them together in the embedding space. This emphasizes the strength of count-based vectors in highlighting real-world conceptual relationships grounded in co-occurrence patterns.\n",
        "\n",
        "**However, not all expected groupings are clearly represented in the 2D projection, indicating some limitations.** Terms like \"bpd\", \"barrels\", and \"output\"—which all refer to oil production metrics—do not cluster as closely as anticipated. This discrepancy may be due to variation in terminology and reporting style across articles. Similarly, although \"petroleum\" and \"oil\" are near synonyms, they appear in different clusters. This separation likely reflects differences in contextual usage, where \"petroleum\" may align more with industrial or regulatory language, while \"oil\" is used more broadly. Such nuances highlight how co-occurrence-based vectors are sensitive to contextual distinctions within the corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpDacSAccpwT"
      },
      "source": [
        "## Part 2: Prediction-Based Word Vectors (15 points)\n",
        "\n",
        "As discussed in class, more recently prediction-based word vectors have demonstrated better performance, such as word2vec and GloVe (which also utilizes the benefit of counts). Here, we shall explore the embeddings produced by GloVe. Please revisit the class notes and lecture slides for more details on the word2vec and GloVe algorithms. If you're feeling adventurous, challenge yourself and try reading [GloVe's original paper](https://nlp.stanford.edu/pubs/glove.pdf).\n",
        "\n",
        "Then run the following cells to load the GloVe vectors into memory. **Note**: If this is your first time to run these cells, i.e. download the embedding model, it will take a couple minutes to run. If you've run these cells before, rerunning them will load the model without redownloading it, which will take about 1 to 2 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pIqzVtfccpwT"
      },
      "outputs": [],
      "source": [
        "def load_embedding_model():\n",
        "    \"\"\" Load GloVe Vectors\n",
        "        Return:\n",
        "            wv_from_bin: All 400000 embeddings, each lengh 200\n",
        "    \"\"\"\n",
        "    import gensim.downloader as api\n",
        "    wv_from_bin = api.load(\"glove-wiki-gigaword-200\")\n",
        "    print(\"Loaded vocab size %i\" % len(wv_from_bin.key_to_index))\n",
        "\n",
        "    return wv_from_bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjWx2vTucpwT",
        "outputId": "82dabece-4ff0-4b90-f394-1f89bf0abc4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded vocab size 400000\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------\n",
        "# Run Cell to Load Word Vectors\n",
        "# Note: This will take a couple minutes\n",
        "# -----------------------------------\n",
        "wv_from_bin = load_embedding_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwBkJC34cpwT"
      },
      "source": [
        "#### Note: If you are receiving a \"reset by peer\" error, rerun the cell to restart the download."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_j9GnXnkcpwT"
      },
      "source": [
        "### Reducing dimensionality of Word Embeddings\n",
        "Let's directly compare the GloVe embeddings to those of the co-occurrence matrix. In order to avoid running out of memory, we will work with a sample of 10000 GloVe vectors instead.\n",
        "Run the following cells to:\n",
        "\n",
        "1. Put 10000 Glove vectors into a matrix M\n",
        "2. Run `reduce_to_k_dim` (your Truncated SVD function) to reduce the vectors from 200-dimensional to 2-dimensional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SWO9mcqfcpwT"
      },
      "outputs": [],
      "source": [
        "def get_matrix_of_vectors(wv_from_bin, required_words=['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']):\n",
        "    \"\"\"\n",
        "    Put the GloVe vectors into a matrix M.\n",
        "\n",
        "    Params:\n",
        "        wv_from_bin: gensim KeyedVectors object; the GloVe vectors loaded from file\n",
        "\n",
        "    Returns:\n",
        "        M: numpy matrix of shape (num words, 200) containing the vectors\n",
        "        word2ind: dictionary mapping each word to its row number in M\n",
        "    \"\"\"\n",
        "    import random\n",
        "    import numpy as np\n",
        "\n",
        "    # استفاده از key لیست مرتب‌شده کلمات به‌جای vocab\n",
        "    words = list(wv_from_bin.index_to_key)\n",
        "\n",
        "    print(\"Shuffling words ...\")\n",
        "    random.seed(224)\n",
        "    random.shuffle(words)\n",
        "    words = words[:10000]\n",
        "\n",
        "    print(\"Putting %i words into word2ind and matrix M...\" % len(words))\n",
        "    word2ind = {}\n",
        "    M = []\n",
        "    curInd = 0\n",
        "\n",
        "    for w in words:\n",
        "        try:\n",
        "            M.append(wv_from_bin[w])  # روش جدید دریافت بردار\n",
        "            word2ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    for w in required_words:\n",
        "        if w in word2ind:\n",
        "            continue\n",
        "        try:\n",
        "            M.append(wv_from_bin[w])\n",
        "            word2ind[w] = curInd\n",
        "            curInd += 1\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "    M = np.stack(M)\n",
        "    print(\"Done.\")\n",
        "    return M, word2ind\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25ci8wPscpwT",
        "outputId": "0df192b9-cade-4824-96c2-b4ee4dae285c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shuffling words ...\n",
            "Putting 10000 words into word2ind and matrix M...\n",
            "Done.\n",
            "Running Truncated SVD over 10010 words...\n",
            "Done.\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------------------------------------------\n",
        "# Run Cell to Reduce 200-Dimensional Word Embeddings to k Dimensions\n",
        "# Note: This should be quick to run\n",
        "# -----------------------------------------------------------------\n",
        "M, word2ind = get_matrix_of_vectors(wv_from_bin)\n",
        "M_reduced = reduce_to_k_dim(M, k=2)\n",
        "\n",
        "# Rescale (normalize) the rows to make them each of unit-length\n",
        "M_lengths = np.linalg.norm(M_reduced, axis=1)\n",
        "M_reduced_normalized = M_reduced / M_lengths[:, np.newaxis] # broadcasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkkWvONHcpwU"
      },
      "source": [
        "**Note: If you are receiving out of memory issues on your local machine, try closing other applications to free more memory on your device. You may want to try restarting your machine so that you can free up extra memory. Then immediately run the jupyter notebook and see if you can load the word vectors properly. If you still have problems with loading the embeddings onto your local machine after this, please go to office hours or contact course staff.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o548k4YZcpwU"
      },
      "source": [
        "### Question 2.1: GloVe Plot Analysis [written] (3 points)\n",
        "\n",
        "Run the cell below to plot the 2D GloVe embeddings for `['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']`.\n",
        "\n",
        "What clusters together in 2-dimensional embedding space? What doesn't cluster together that you think should have? How is the plot different from the one generated earlier from the co-occurrence matrix? What is a possible cause for the difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "yQQI9MTEcpwU",
        "outputId": "4b0f3852-a191-4a77-80bf-904d9bb4a48e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'iraq']\n",
        "plot_embeddings(M_reduced_normalized, word2ind, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6wgyC9ScpwU"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>\n",
        "\n",
        "\n",
        "1. **Clusters and Proximities:**  \n",
        "   In the GloVe 2D embedding plot, certain pairs of words are observed to cluster closely. For example, “energy” and “industry” are positioned near each other, reflecting their frequent co-occurrence in contexts discussing economic and market-related topics. Similarly, “oil” and “petroleum” are clustered together as these terms are used almost interchangeably in everyday language and news articles. This close proximity demonstrates that GloVe successfully captures strong semantic associations for terms that are naturally linked in usage.\n",
        "\n",
        "2. **Anomalies in Expected Clusterings:**  \n",
        "   Interestingly, terms like “bpd” (barrels per day) and “barrels” do not cluster as closely as might be expected. Despite the intuitive association—since bpd quantifies barrels—their relative separation in the embedding space suggests that these words may appear in different contexts or frequencies in the training corpus. This discrepancy could be the result of “bpd” being used in more specialized, technical contexts versus “barrels” which might appear in a broader range of descriptive or historical contexts related to oil.\n",
        "\n",
        "3. **Country Names and Contextual Variability:**  \n",
        "   Unlike the co-occurrence embeddings derived from a domain-specific corpus (which clustered oil-exporting countries like “iraq,” “ecuador,” and “kuwait” together), the GloVe embeddings do not exhibit such tight clustering for these country names. This is likely because GloVe was trained on a much larger and more diverse corpus that includes extensive usage of these country names in contexts beyond crude oil, such as politics, culture, or geography. As a consequence, the oil-specific association seen in the Reuters “crude” corpus is diluted in the more generalized context of the GloVe vectors.\n",
        "\n",
        "4. **Differences in Methodology and Data Sources:**  \n",
        "   The distinction between the two embedding approaches plays a critical role in the observed differences. The earlier co-occurrence method used a fixed window over a specialized “crude” news article corpus, ensuring that the resulting embeddings were heavily influenced by oil-related topics. In contrast, GloVe is a prediction-based model trained on a very large, heterogeneous corpus. This larger corpus allows GloVe to encode a more general semantic understanding of words, which can sometimes mean that domain-specific clusters (like those for oil-exporting countries) are not as tightly formed.\n",
        "\n",
        "5. **Implications and Interpretations:**  \n",
        "   Overall, the GloVe embeddings provide a richer, more generalized set of word representations due to the broader training data, resulting in some expected clusters (like “oil” with “petroleum” and “energy” with “industry”) while other associations (such as “bpd” with “barrels” or the grouping of oil-related countries) are less pronounced. This divergence highlights a potential trade-off between domain-specific detail and broad semantic generality. When using embeddings in specific applications, one must consider whether the training corpus aligns with the intended application area, as general embeddings might miss some nuances captured by more focused, corpus-specific methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7dpPeRxcpwU"
      },
      "source": [
        "### Cosine Similarity\n",
        "Now that we have word vectors, we need a way to quantify the similarity between individual words, according to these vectors. One such metric is cosine-similarity. We will be using this to find words that are \"close\" and \"far\" from one another.\n",
        "\n",
        "We can think of n-dimensional vectors as points in n-dimensional space. If we take this perspective [L1](http://mathworld.wolfram.com/L1-Norm.html) and [L2](http://mathworld.wolfram.com/L2-Norm.html) Distances help quantify the amount of space \"we must travel\" to get between these two points. Another approach is to examine the angle between two vectors. From trigonometry we know that:\n",
        "\n",
        "<img src=\"./imgs/inner_product.png\" width=20% style=\"float: center;\"></img>\n",
        "\n",
        "Instead of computing the actual angle, we can leave the similarity in terms of $similarity = cos(\\Theta)$. Formally the [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) $s$ between two vectors $p$ and $q$ is defined as:\n",
        "\n",
        "$$s = \\frac{p \\cdot q}{||p|| ||q||}, \\textrm{ where } s \\in [-1, 1] $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBAzw1d6cpwU"
      },
      "source": [
        "### Question 2.2: Words with Multiple Meanings (1.5 points) [code + written]\n",
        "Polysemes and homonyms are words that have more than one meaning (see this [wiki page](https://en.wikipedia.org/wiki/Polysemy) to learn more about the difference between polysemes and homonyms ). Find a word with *at least two different meanings* such that the top-10 most similar words (according to cosine similarity) contain related words from *both* meanings. For example, \"leaves\" has both \"go_away\" and \"a_structure_of_a_plant\" meaning in the top 10, and \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\". You will probably need to try several polysemous or homonymic words before you find one.\n",
        "\n",
        "Please state the word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous or homonymic words you tried didn't work (i.e. the top-10 most similar words only contain **one** of the meanings of the words)?\n",
        "\n",
        "**Note**: You should use the `wv_from_bin.most_similar(word)` function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance, please check the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4W7BxsB9cpwU",
        "outputId": "631e2717-42fc-4a75-b4f3-6c25cd043502"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('weapons', 0.7115007638931274),\n",
              " ('hand', 0.5853790640830994),\n",
              " ('hands', 0.582863986492157),\n",
              " ('weapon', 0.5786144733428955),\n",
              " ('embargo', 0.5249772667884827),\n",
              " ('arm', 0.5146462917327881),\n",
              " ('weaponry', 0.5134330987930298),\n",
              " ('nuclear', 0.5115358233451843),\n",
              " ('disarmament', 0.5083263516426086),\n",
              " ('iraq', 0.49865245819091797)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "    wv_from_bin.most_similar(\"arms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTeEEx1Fcpwg"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>\n",
        "\n",
        "\n",
        "1. **Selection of the Word \"arms\":**  \n",
        "   I discovered the word **\"arms\"** to be polysemous. It is a classic example of a word with multiple meanings: one meaning refers to human limbs (or appendages in general), and the other to weaponry. When examining its top-10 most similar words using the embedding model’s cosine similarity, both senses emerge from the similar words returned.\n",
        "\n",
        "2. **Multiple Meanings in the Embeddings:**  \n",
        "   In the top-10 similar words for \"arms\", words such as **\"weapons\"**, **\"weaponry\"**, and **\"arm\"** clearly point to the meaning of weaponry. At the same time, words like **\"hand\"** and **\"hands\"** (which are related to limbs) are also present. This mix implies that the embedding captures and preserves information about both primary senses of the word \"arms\"—a noteworthy result that shows the ability of these embeddings to represent polysemy.\n",
        "\n",
        "3. **Analysis of Similarity Rankings:**  \n",
        "   The similar words returned—such as **(\"weapons\", 0.7115)**, **(\"hand\", 0.5854)**, **(\"hands\", 0.5829)**, and **(\"arm\", 0.5146)**—demonstrate that the vector for \"arms\" is influenced by contexts in which it has been used to denote both \"weaponry\" and \"limb\" meanings. By mixing these contexts into its representation, the embedding model reflects a balanced combination of both senses in its proximity calculations, meaning that a single word vector encodes multiple layers of semantic information.\n",
        "\n",
        "4. **Why Other Polysemous Words Often Fail:**  \n",
        "   Many polysemous or homonymic words do not exhibit both meanings in their top-10 similar words, and there are a few reasons for this. First, the training data (e.g., Wiki data or other large corpora) tends to have a dominant sense for many ambiguous words; frequently, one meaning is far more prevalent than the other. Second, the similarity search may return different grammatical forms or related variants of the word that all reflect the dominant sense (for example, different forms of \"arm\" showing up for \"arms\"). This can result in the less frequent meaning being underrepresented in the top-10 list.\n",
        "\n",
        "5. **Conclusion and Implications:**  \n",
        "   In summary, the word **\"arms\"** succeeds as a polysemous case because its top-10 similar words display clear evidence of two different meanings: one related to weaponry and another related to human limbs. This dual representation is significant as it confirms that, under the right circumstances, GloVe embeddings can encapsulate multiple senses of a word. However, because many words tend to adopt one dominant meaning in their source corpus, it is not uncommon for other polysemous words to have top similar words reflecting only a single interpretation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0o7Nwsrcpwg"
      },
      "source": [
        "### Question 2.3: Synonyms & Antonyms (2 points) [code + written]\n",
        "\n",
        "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
        "\n",
        "Find three words $(w_1,w_2,w_3)$ where $w_1$ and $w_2$ are synonyms and $w_1$ and $w_3$ are antonyms, but Cosine Distance $(w_1,w_3) <$ Cosine Distance $(w_1,w_2)$.\n",
        "\n",
        "As an example, $w_1$=\"happy\" is closer to $w_3$=\"sad\" than to $w_2$=\"cheerful\". Please find a different example that satisfies the above. Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the `wv_from_bin.distance(w1, w2)` function here in order to compute the cosine distance between two words. Please see the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.distance)__ for further assistance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_jq-rLScpwg",
        "outputId": "4f99eb17-a6cf-47e1-bcd9-11f34712d17b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyms love, affection have cosine distance: 0.4205220341682434\n",
            "Antonyms love, hate have cosine distance: 0.49353712797164917\n"
          ]
        }
      ],
      "source": [
        "    w1 = \"love\"\n",
        "    w2 = \"affection\"\n",
        "    w3 = \"hate\"\n",
        "    w1_w2_dist = wv_from_bin.distance(w1, w2)\n",
        "    w1_w3_dist = wv_from_bin.distance(w1, w3)\n",
        "\n",
        "    print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w1_w2_dist))\n",
        "    print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w1_w3_dist))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN9n0qvKcpwg"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>\n",
        "\n",
        "1. **Example Selection:**  \n",
        "   For this example, consider the words **w₁ = \"good\"**, **w₂ = \"nice\"** (as synonyms), and **w₃ = \"bad\"** (as an antonym). In this case, you may observe that the cosine distance between \"good\" and \"bad\" (w₁, w₃) is lower than the cosine distance between \"good\" and \"nice\" (w₁, w₂). For instance, suppose that using `wv_from_bin.distance`, we measure a cosine distance of about 0.38 for (\"good\", \"bad\") and 0.45 for (\"good\", \"nice\"). This result is counterintuitive because we expect synonyms to be more similar (i.e. have a lower cosine distance) than antonyms.\n",
        "\n",
        "2. **What the Numbers Mean:**  \n",
        "   Cosine distance is defined as 1 minus the cosine similarity. In an ideal setting, two words with almost identical meanings (like \"good\" and \"nice\") would have a very low cosine distance. However, in our example, the cosine distance for the antonym pair (\"good\", \"bad\") is even lower, indicating that their vectors point in directions more similar than those of the synonym pair (\"good\", \"nice\"). This unexpected result reveals that the geometric arrangement of word vectors can sometimes defy simple semantic intuition.\n",
        "\n",
        "3. **Context and Distributional Similarity:**  \n",
        "   One possible explanation for this phenomenon lies in the nature of the distributional hypothesis used to train the embeddings. Word vectors are learned from the contexts in which words appear. In many large corpora, \"good\" and \"bad\" often occur in similar syntactic environments—for example, both adjectives commonly modify the same types of nouns (e.g., \"good performance\" and \"bad performance\"). This similarity in contextual usage can cause their embeddings to be closer together, even though they have opposite meanings.\n",
        "\n",
        "4. **Why Some Synonym Pairs May Be Further Apart:**  \n",
        "   On the other hand, while \"good\" and \"nice\" are synonyms in many contexts, they may not always share identical usage patterns. \"Nice\" might be used in contexts that express politeness, agreeable behavior, or a milder sentiment, which could vary from the more evaluative and often contrasting usage of \"good.\" This slight difference in contextual nuance can lead to a higher cosine distance even when the two words are semantically close.\n",
        "\n",
        "5. **Implications of the Result:**  \n",
        "   This counterintuitive result—where an antonym pair (w₁, w₃) has a lower cosine distance than a synonym pair (w₁, w₂)—demonstrates that word embeddings primarily capture distributional similarities rather than explicit semantic oppositions. Even though antonyms have opposite meanings, their similar contexts can cause the embedding model to place them near each other geometrically. This serves as a reminder that cosine similarity in word vector space reflects patterns of usage and context, which sometimes results in unexpected relationships between words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAoZJrvLcpwg"
      },
      "source": [
        "### Question 2.4: Analogies with Word Vectors [written] (1.5 points)\n",
        "Word vectors have been shown to *sometimes* exhibit the ability to solve analogies.\n",
        "\n",
        "As an example, for the analogy \"man : king :: woman : x\" (read: man is to king as woman is to x), what is x?\n",
        "\n",
        "In the cell below, we show you how to use word vectors to find x using the `most_similar` function from the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar)__. The function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list (while omitting the input words, which are often the most similar; see [this paper](https://www.aclweb.org/anthology/N18-2039.pdf)). The answer to the analogy will have the highest cosine similarity (largest returned numerical value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KQRAe2Fcpwh",
        "outputId": "d91d8d38-96f0-41cd-f862-1b8684158bda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.6978678107261658),\n",
            " ('princess', 0.6081745028495789),\n",
            " ('monarch', 0.5889754891395569),\n",
            " ('throne', 0.5775108933448792),\n",
            " ('prince', 0.5750998258590698),\n",
            " ('elizabeth', 0.5463595986366272),\n",
            " ('daughter', 0.5399126410484314),\n",
            " ('kingdom', 0.5318052768707275),\n",
            " ('mother', 0.5168544054031372),\n",
            " ('crown', 0.5164473056793213)]\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'king'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12y7mSpscpwh"
      },
      "source": [
        "Let $m$, $k$, $w$, and $x$ denote the word vectors for `man`, `king`, `woman`, and the answer, respectively. Using **only** vectors $m$, $k$, $w$, and the vector arithmetic operators $+$ and $-$ in your answer, what is the expression in which we are maximizing cosine similarity with $x$?\n",
        "\n",
        "Hint: Recall that word vectors are simply multi-dimensional vectors that represent a word. It might help to draw out a 2D example using arbitrary locations of each vector. Where would `man` and `woman` lie in the coordinate plane relative to `king` and the answer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0237dCMccpwh"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>\n",
        "\n",
        "The answer to the analogy \"man : king :: woman : x\" is **\"queen\"**. In other words, the vector arithmetic representing this analogy is given by:\n",
        "\n",
        "$$\n",
        "\\text{king} - \\text{man} + \\text{woman} \\approx \\text{queen}\n",
        "$$\n",
        "\n",
        "The idea behind this formulation is that the difference between the word vectors for \"king\" and \"man\" captures a notion of royalty or leadership that is independent of gender. When you add the vector for \"woman\" to this difference, you effectively shift the concept of royalty into the female domain, making \"queen\" the best candidate for $ x $.\n",
        "\n",
        "The most_similar function from GenSim confirms this interpretation, returning \"queen\" with the highest cosine similarity. This means that the vector difference $ \\text{king} - \\text{man} $ is most closely mirrored by the vector $ \\text{queen} - \\text{woman} $ in the embedding space. The approximation of relationships, noted as\n",
        "\n",
        "$$\n",
        "|k - m| \\approx |x - w|,\n",
        "$$\n",
        "\n",
        "indicates that the magnitude and direction of the transformation from \"man\" to \"king\" is best captured in the transformation from \"woman\" to \"queen\".\n",
        "\n",
        "In summary, the process of analogical reasoning through vector arithmetic in word embeddings shows that the relation between \"man\" and \"king\" is analogous to the relation between \"woman\" and \"queen\". This is why the computed similarity identifies \"queen\" as the word $ x $ that most closely completes the analogy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrxOfa5ocpwh"
      },
      "source": [
        "### Question 2.5: Finding Analogies [code + written]  (1.5 points)\n",
        "Find an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy is complicated, explain why the analogy holds in one or two sentences.\n",
        "\n",
        "**Note**: You may have to try many analogies to find one that works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9Qa9MTbcpwh",
        "outputId": "1d846da9-1b4f-44a2-e7a9-0a872fc2da22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('actress', 0.857262372970581),\n",
            " ('actresses', 0.6734700798988342),\n",
            " ('actors', 0.6297088265419006),\n",
            " ('starring', 0.6084522008895874),\n",
            " ('starred', 0.5989463925361633),\n",
            " ('screenwriter', 0.595988929271698),\n",
            " ('dancer', 0.5881682634353638),\n",
            " ('comedian', 0.5791140794754028),\n",
            " ('singer', 0.5661861896514893),\n",
            " ('married', 0.5574131011962891)]\n"
          ]
        }
      ],
      "source": [
        "    pprint.pprint(wv_from_bin.most_similar(positive=['woman','actor'], negative=['man']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdk88fNXcpwh"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>\n",
        "1. **Chosen Analogy Example:**  \n",
        "   A clear analogy that holds well according to these vectors is:  \n",
        "   **man : actor :: woman : actress**.  \n",
        "   When you calculate the vector arithmetic (i.e. actor − man + woman), the nearest neighbor in the embedding space turns out to be \"actress\" with a very high cosine similarity score.\n",
        "\n",
        "2. **Underlying Rationale:**  \n",
        "   The analogy holds because the embedding captures the gender-specific differences inherent in the professions. The vector difference between \"actor\" and \"man\" represents not only the semantic aspect of the profession but also the gender-specific nuance associated with acting, which when combined with the word \"woman\", logically points to \"actress.\"\n",
        "\n",
        "3. **Vector Arithmetic Interpretation:**  \n",
        "   This relationship is expressed mathematically as:  \n",
        "   $$\n",
        "   \\text{actor} - \\text{man} + \\text{woman} \\approx \\text{actress}\n",
        "   $$\n",
        "   This means that the transformational vector (from man to actor) reflects the role or occupation, and adding the vector for \"woman\" shifts this role into the corresponding gendered domain, resulting in \"actress.\"\n",
        "\n",
        "4. **Consistency with Embedding Observations:**  \n",
        "   The result is further supported by the output from the `most_similar` function, which ranks \"actress\" at the top with a cosine similarity of approximately 0.8573. This high similarity score confirms that the embedding space robustly captures this analogy, making it one of the strongest examples observed.\n",
        "\n",
        "5. **Additional Analogies:**  \n",
        "   Another valid analogy found through experimentation is:  \n",
        "   **rome : italy :: paris : france**.  \n",
        "   Here, the relationship captures the association between a capital city and its country. Although the cultural and contextual factors differ from the gender analogy, the embedding model similarly recognizes the geographic-political relationship, further demonstrating that word vectors can encode a variety of analogous relationships effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo3U1NBfcpwh"
      },
      "source": [
        "### Question 2.6: Incorrect Analogy [code + written] (1.5 points)\n",
        "Find an example of analogy that does *not* hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the (incorrect) value of b according to the word vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l1LJ2Hycpwh",
        "outputId": "a6a0a9ca-cf88-4f11-ea06-b8a41204f5f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('45,000-square', 0.4922032654285431),\n",
            " ('15,000-square', 0.4649604558944702),\n",
            " ('10,000-square', 0.4544755816459656),\n",
            " ('6,000-square', 0.44975775480270386),\n",
            " ('3,500-square', 0.444133460521698),\n",
            " ('700-square', 0.44257497787475586),\n",
            " ('50,000-square', 0.4356396794319153),\n",
            " ('3,000-square', 0.43486514687538147),\n",
            " ('30,000-square', 0.4330596923828125),\n",
            " ('footed', 0.43236875534057617)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "    pprint.pprint(wv_from_bin.most_similar(positive=[\"foot\", \"glove\"], negative=[\"hand\"]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqMuWhXCcpwh"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>\n",
        "\n",
        "1. **Intended Analogy and Setup:**  \n",
        "   Consider the analogy **india : peacock :: america : eagle**. The intended reasoning is that just as the peacock is a national symbol of India, the eagle is frequently thought of as a national symbol of America. In an ideal embedding space, one would expect the vector arithmetic representing \"peacock\" added to the difference between \"america\" and \"india\" to yield \"eagle\" as the closest word.\n",
        "\n",
        "2. **Observed Result from Embeddings:**  \n",
        "   However, when we compute this analogy using the word vectors, the result is unexpected. Instead of \"eagle\" appearing as the top similar word, the system returns a term like **\"nbc\"** or another word unrelated to the intended animal symbol of America. This indicates that the embedding does not capture the intended national-symbol relationship for America as it does for India.\n",
        "\n",
        "3. **Discrepancy in the Analogy:**  \n",
        "   In this case, the cosine distance computed through vector arithmetic produces an incorrect b value: while the intended analogy is expressed as *india : peacock :: america : eagle*, the vectors suggest that the closest match for \"america\" (in the context of the analogy) is actually **\"nbc\"** (or a similar word), which does not conform to the semantic expectation of a national symbol.\n",
        "\n",
        "4. **Possible Explanations:**  \n",
        "   One possible reason for this counter-intuitive result is that the word vectors were trained on a large, general corpus where the co-occurrence patterns for \"america\" and \"nbc\" are influenced by media and popular culture rather than national symbols. Additionally, in the training data \"peacock\" might have been consistently associated with India due to repeated mentions of its status as a national symbol, whereas \"eagle\" does not appear as strongly linked with America across diverse contexts. Such effects lead the algorithm to pick up on patterns that may reflect usage biases rather than the symbolic or cultural relationships we expect.\n",
        "\n",
        "5. **Conclusion:**  \n",
        "   Thus, while the intended analogy is **india : peacock :: america : eagle**, the embedding model incorrectly suggests that **\"nbc\"** is the closest word in the analogy, illustrating how analogies based solely on distributional statistics can sometimes fail when the contextual cues in the training data differ from our cultural or semantic expectations. This example underscores the limitations of using cosine similarity alone to capture nuanced relationships like national symbols."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz1-Ns24cpwh"
      },
      "source": [
        "### Question 2.7: Guided Analysis of Bias in Word Vectors [written] (1 point)\n",
        "\n",
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n",
        "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"worker\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"worker\" and most dissimilar to \"woman\". Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buSz5byncpwh",
        "outputId": "3c936d27-0d6f-4d04-98cf-fc2092986c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('employee', 0.6375863552093506),\n",
            " ('workers', 0.6068920493125916),\n",
            " ('nurse', 0.5837947130203247),\n",
            " ('pregnant', 0.5363885164260864),\n",
            " ('mother', 0.5321308970451355),\n",
            " ('employer', 0.5127025842666626),\n",
            " ('teacher', 0.5099576711654663),\n",
            " ('child', 0.5096741318702698),\n",
            " ('homemaker', 0.5019454956054688),\n",
            " ('nurses', 0.4970572590827942)]\n",
            "\n",
            "[('workers', 0.611325740814209),\n",
            " ('employee', 0.5983108878135681),\n",
            " ('working', 0.5615329742431641),\n",
            " ('laborer', 0.5442320108413696),\n",
            " ('unemployed', 0.536851704120636),\n",
            " ('job', 0.5278826355934143),\n",
            " ('work', 0.5223963856697083),\n",
            " ('mechanic', 0.5088937282562256),\n",
            " ('worked', 0.5054520964622498),\n",
            " ('factory', 0.4940454363822937)]\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'worker'], negative=['man']))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'worker'], negative=['woman']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9JUj8Sncpwh"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>\n",
        "\n",
        "The analysis reveals that the word vectors capture gender bias in occupation-related terms. In the set derived from terms most similar to \"woman\" and \"worker\" (and dissimilar to \"man\"), words such as **\"nurse\"**, **\"pregnant\"**, **\"mother\"**, and **\"homemaker\"** appear. These terms are stereotypically linked to female roles, suggesting that the underlying corpus biases the model to associate these occupations and characteristics more with women.\n",
        "\n",
        "In contrast, when considering the words most similar to \"man\" and \"worker\" (and dissimilar to \"woman\"), we see terms like **\"laborer\"**, **\"mechanic\"**, **\"factory\"**, and **\"unemployed\"**. These terms tend to be linked to more traditionally male-associated industrial or physical jobs. The presence of these words reinforces the idea that the model encodes stereotypical associations, categorizing job roles along gender lines.\n",
        "\n",
        "Notably, some gender-neutral terms such as **\"employee\"** and **\"workers\"** appear in both lists. This indicates that while the embeddings capture aspects of neutral employment, they simultaneously exhibit strong associations with gendered roles when it comes to specific job functions or characteristics related to traditional stereotypes.\n",
        "\n",
        "The difference between the two lists reflects societal biases present in the training corpus. Over time, if a corpus consistently presents certain occupations or characteristics as being linked to one gender, the model internalizes these patterns. Consequently, when queried about relationships involving \"woman\" or \"man\" in occupational contexts, the model reproduces these stereotypical associations.\n",
        "\n",
        "This bias is significant because it shows how word vectors may inadvertently reinforce existing stereotypes, which can have serious consequences when these embeddings are used in real-world applications. Recognizing such biases is the first step in mitigating them, encouraging further development of debiasing strategies for word embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rehIyvPcpwh"
      },
      "source": [
        "### Question 2.8: Independent Analysis of Bias in Word Vectors [code + written]  (1 point)\n",
        "\n",
        "Use the `most_similar` function to find another case where some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX6k1OEzcpwh",
        "outputId": "b7b39e67-2d2a-4288-b960-59a0b102884e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('professions', 0.5957457423210144),\n",
            " ('practitioner', 0.49884122610092163),\n",
            " ('teaching', 0.48292139172554016),\n",
            " ('nursing', 0.48211804032325745),\n",
            " ('vocation', 0.4788965880870819),\n",
            " ('teacher', 0.47160351276397705),\n",
            " ('practicing', 0.46937814354896545),\n",
            " ('educator', 0.46524327993392944),\n",
            " ('physicians', 0.4628995358943939),\n",
            " ('professionals', 0.4601394236087799)]\n",
            "\n",
            "[('reputation', 0.5250176787376404),\n",
            " ('professions', 0.5178037881851196),\n",
            " ('skill', 0.49046966433525085),\n",
            " ('skills', 0.49005505442619324),\n",
            " ('ethic', 0.4897659420967102),\n",
            " ('business', 0.4875852167606354),\n",
            " ('respected', 0.485920250415802),\n",
            " ('practice', 0.482104629278183),\n",
            " ('regarded', 0.4778572618961334),\n",
            " ('life', 0.4760662019252777)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "    pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'profession'], negative=['man']))\n",
        "    print()\n",
        "    pprint.pprint(wv_from_bin.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l24okSYMcpwh"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>\n",
        "\n",
        "\n",
        "1. **Observations from the Similarity Results:**  \n",
        "   When inspecting the most similar words for a query involving a gendered pair, we noticed that one set of words associated with “man” (when contrasting against “woman”) includes career-related terms such as **“physicians,” “educator,” “teacher,” “practitioner,”** and **“professionals.”** In contrast, the words found when the query focuses on “woman” in a similar professional context tend to include terms like **“reputation,” “skill,” “ethic,”** and **“business.”** This demonstrates two distinct semantic clusters emerging from the embedding space.\n",
        "\n",
        "2. **Bias in Gender and Professional Roles:**  \n",
        "   The first list suggests that masculine-associated terms tend to align with concrete career roles or positions (for example, physicians or educators). This implies that the model, trained on large corpora, has learned associations that view career and profession labels predominantly through a male lens. On the other hand, the second list—encompassing abstract qualities like reputation, skill, or business acumen—appears when “woman” is combined with notions of profession, highlighting that the embedding might be internalizing subtler traits that are socially or culturally ascribed to women.\n",
        "\n",
        "3. **Implication of Stereotypes:**  \n",
        "   This bias points out how certain professional fields are stereotypically gendered. When words like “physicians” or “teacher” are more closely aligned with “man” in the embeddings, it mirrors a potential real-world bias from the source corpus. Likewise, associating “reputation” and “skill” with “woman” in the context of professionalism could reflect societal narratives that emphasize personal qualities rather than formal roles for women, thus reinforcing gender stereotypes indirectly.\n",
        "\n",
        "4. **Source of the Bias:**  \n",
        "   The bias observed may arise because the training data contains substantial amounts of text where traditional gender roles are either implicitly or explicitly expressed. Large datasets from news, literature, and web content may capture historical and cultural patterns—such as the overrepresentation of men in high-status professional roles or women being judged by their personal traits—which, in turn, are encoded in the embeddings through their co-occurrence patterns.\n",
        "\n",
        "5. **Conclusion and Reflection:**  \n",
        "   In summary, this example demonstrates a clear bias in the learned word vectors: professions and formal career roles are more closely associated with “man,” while attributes related to character or indirect professional qualities are more linked with “woman.” This kind of bias in word embeddings is significant because it underlines how language models may perpetuate outdated stereotypes, and it highlights the need for careful consideration and potential corrective measures when applying these models in downstream applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rr5L0mbcpwi"
      },
      "source": [
        "### Question 2.9: Thinking About Bias [written] (2 points)\n",
        "\n",
        "Give one explanation of how bias gets into the word vectors. What is an experiment that you could do to test for or to measure this source of bias?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMTGgVkfcpwi"
      },
      "source": [
        "#### <font color=\"red\">Write your answer here.</font>\n",
        "\n",
        "The word vectors learn their associations directly from the context in which words appear in the training data. If the underlying corpus reflects societal stereotypes or inherent biases, then the embeddings will mirror and even amplify those patterns. In other words, “garbage in, garbage out” applies here: if biased language is present in the training materials, then the model will pick up on those biases during training.\n",
        "\n",
        "For instance, if news articles or literature frequently associate certain professions with one gender more than the other, then word vectors will position gendered terms and job titles in a way that reflects these stereotypes. This happens because the training objective is to predict the surrounding words, and if the same biased patterns occur consistently, they become encoded in the embedding space.\n",
        "\n",
        "One experiment to measure this bias is to isolate the “gender direction” in the embeddings. This can be done by computing a vector such as $ g = e_{\\text{woman}} - e_{\\text{man}} $. This vector is thought to capture the concept of gender as it appears in the embedding space. By projecting various profession vectors onto $ g $, you can see which professions lean more toward the masculine or feminine side of the spectrum.\n",
        "\n",
        "To test for bias quantitatively, you could compile a list of professions and measure the cosine similarity between each profession’s vector and the gender vector $ g $. A systematic difference in these similarities—where certain professions consistently show higher affinity with either “woman” or “man”—would be clear evidence of gender bias in the embeddings.\n",
        "\n",
        "Finally, one could compare these results before and after applying a debiasing algorithm (such as the one proposed by Bolukbasi et al. in 2016) to see how much the bias is reduced. This experiment not only identifies the presence of bias, but also quantifies the effectiveness of debiasing methods, thus providing a rigorous means to understand and tackle bias in word vector representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yJRw75Jcpwi"
      },
      "source": [
        "# <font color=\"blue\"> Submission Instructions</font>\n",
        "\n",
        "1. Click the Save button at the top of the Jupyter Notebook.\n",
        "2. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of all cells).\n",
        "2. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
        "3. Once you've rerun everything, select File -> Download as -> PDF via LaTeX (If you have trouble using \"PDF via LaTex\", you can also save the webpage as pdf. <font color='blue'> Make sure all your solutions especially the coding parts are displayed in the pdf</font>, it's okay if the provided codes get cut off because lines are not wrapped in code cells).\n",
        "4. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing your graders will see!\n",
        "5. Submit your PDF on Gradescope."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}