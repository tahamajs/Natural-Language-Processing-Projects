{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cover_header",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"text-align: center; padding: 20px; font-family: Vazir;\">\n",
    "<h1 align=\"center\" style=\"font-size: 28px; color:rgb(64, 244, 202); width: 100%;\">โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ<br>ุชูุฑู ฒ<br>โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ</h1>\n",
    "<h2 style=\"color:rgb(90, 255, 184); font-size: 20px;\">Logistic Regression and Naive Bayes</h2>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">ูุฑูุงุฏ ูุตุฑ - ุนูุฑุถุง ุฒูุงู</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">farhadnasri999@gmail.com - shigzv@gmail.com</p>\n",
    "\n",
    "<div dir=\"rtl\" style=\"border: 2px dashed rgb(90, 255, 184); border-radius: 8px; padding: 20px; margin: 20px auto; max-width: 500px; text-align: right;\">\n",
    "<p style=\"color: rgb(64, 244, 202); font-size: 18px; margin-bottom: 15px;\">๐ ูุดุฎุตุงุช ุฏุงูุดุฌู:</p>\n",
    "<p style=\"color: #666; margin: 5px;\">ูุงู ู ูุงู ุฎุงููุงุฏฺฏ: ุทุงูุง ูุฌูุณ</p>\n",
    "<p style=\"color: #666; margin: 5px;\">ุดูุงุฑู ุฏุงูุดุฌู: 810101504</p>\n",
    "<p style=\"color: #666; margin: 5px;\">ุชุงุฑุฎ ุงุฑุณุงู: ----</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; padding: 25px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<div style=\"padding-right:100px\">\n",
    "๐ <b>ุณุงุฎุชุงุฑ ุชูุฑู:</b>\n",
    "<li><b>ุณูุงู ุงูู - <span dir=\"ltr\">Logistic Regression and Naive Bayes (from scratch)</span> (60)</b></li>\n",
    "<ul>\n",
    "<li>ุจุฎุด ุงูู: ูพุด ูพุฑุฏุงุฒุด ุฏุงุฏูโูุง ูุชู ู ุงุณุชูุงุฏู ุงุฒ Bag of Words</li>\n",
    "<li>ุจุฎุด ุฏูู: ุฌุฏุงุณุงุฒ ุฏุงุฏูโูุง ุขููุฒุด ู ุขุฒูุงุด</li>\n",
    "<li>ุจุฎุด ุณูู: ูพุงุฏูโุณุงุฒ ูุนุงุฑโูุง ุงุฑุฒุงุจ</li>\n",
    "<li>ุจุฎุด ฺูุงุฑู: ูพุงุฏูโุณุงุฒ Logistic Regression</li>\n",
    "<li>ุจุฎุด ูพูุฌู: ูพุงุฏูโุณุงุฒ Naive Bayes</li>\n",
    "<li>ุจุฎุด ุดุดู: ุชุญูู ูุชุงุฌ</li>\n",
    "</ul>\n",
    "<li><b>ุณูุงู ุฏูู - <span dir=\"ltr\">Logistic Regression and Naive Bayes (e.g. \n",
    "sklearn)</span> (40)</b></li>\n",
    "<ul>\n",
    "<li>ุจุฎุด ุงูู: ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ุณุงุฎุชุงุฑ</li>\n",
    "<li>ุจุฎุด ุฏูู: ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ุขูุงุฑ ู ูุญุชูุง</li>\n",
    "<li>ุจุฎุด ุณูู: ุงุณุชุฎุฑุงุฌ ูฺฺฏโูุง ุฏูุฎูุงู</li>\n",
    "<li>ุจุฎุด ฺูุงุฑู: ุชุฑฺฉุจ ููู ูฺฺฏโูุง ุงุณุชุฎุฑุงุฌโุดุฏู</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "<div dir='rtl' style=\"line-height: 1.8; font-family: Vazir; font-size: 16px; margin-top: 20px; background-color: #e8eaf6; padding: 15px; border-radius: 8px; color:black\">\n",
    "๐ก <b>ูฺฉุงุช ููู:</b>\n",
    "<br>\n",
    "ุฏุฑ ุณูุงู ุฏู ูุฌุงุฒ ุจู ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจุฎุงููโูุง ุขูุงุฏู ุจุฑุง ูุฏูโูุง ู ูุชุฑฺฉโูุง ูุณุชุฏ ูู ุฏุฑ ุณูุงู ฺฉุ ููุงูโุทูุฑ ฺฉู ุฏุฑ ูุชู ุณูุงู ูู ุฐฺฉุฑ ุดุฏูุ ุจุงุฏ ุงุฒ ูพุงู ูพุงุฏูโุณุงุฒ ฺฉูุฏ.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_title"
   },
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ<br>ุณูุงู ุงูู: ูพุงุฏูโุณุงุฒ Logistic Regression ู Naive Bayes <br>โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "ุฏุฑ ุงู ุณูุงูุ ุดูุง ุจุงุฏ ุงู ุฏู ูุฏู ูพุงูโุง ุจุฑุง ุทุจููโุจูุฏ ูุชูู ุฑุง ุงุฒ ุตูุฑ (ุจุฏูู ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจุฎุงููโูุง ุขูุงุฏู) ูพุงุฏูโุณุงุฒ ฺฉูุฏ. ููโฺูู ูุงุฒ ุงุณุชุ ูพุดโูพุฑุฏุงุฒุด ุฏุงุฏูโูุง ู ุงุฑุฒุงุจ ุนููฺฉุฑุฏ ูุฏูโูุง ุฑุง ูุฒ ุงูุฌุงู ุฏูุฏ.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุจุฎุด ุงูู: ูพุด ูพุฑุฏุงุฒุด ุฏุงุฏูโูุง ูุชู ู ุงุณุชูุงุฏู ุงุฒ Bag of Words</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุฏุฑ ฺฏุงู ูพุดโูพุฑุฏุงุฒุดุ ูุฏู ูุง ุขูุงุฏูโุณุงุฒ ุฏุงุฏูโูุง ูุชู ุจุฑุง ุงุณุชูุงุฏู ุฏุฑ ูุฏูโูุง ุงุฏฺฏุฑ ูุงุดู ุงุณุช. ุงุฒ ุขูโุฌุง ฺฉู ูุฏูโูุง ุจุง ุฏุงุฏูโูุง ุนุฏุฏ ฺฉุงุฑ ูโฺฉููุฏุ ุจุงุฏ ูุชู ุฎุงู ุฑุง ุจู ุดฺฉู ุนุฏุฏ ุชุจุฏู ฺฉูู ุชุง ุจุชูุงููุฏ ุงูฺฏููุง ููุฌูุฏ ุฏุฑ ูุงฺูโูุง ุฑุง ุจุงููุฒูุฏ. ุฏุฑ ุงู ุจุฎุดุ ูุตุฏ ุฏุงุฑู ุจุง ุชุจุฏู ูุชูู ุจู ููุงุด Bag of Wordsุ ูุฑ ุงูู ุฑุง ุจุฑ ุงุณุงุณ ุชุนุฏุงุฏ ุชฺฉุฑุงุฑ ูุงฺูโูุงุด ุจู ฺฉ ุจุฑุฏุงุฑ ุนุฏุฏ ุชุจุฏู ฺฉูู.\n",
    "<br>\n",
    "ุฏุชุงุณุช emails_1.csv ุดุงูู ุฏู ุณุชูู ุงุณุช:\n",
    "<br>\n",
    "text (ูุชู ุงูู)\n",
    "<br>\n",
    "status (ุจุฑฺุณุจุ ูุดุฎุตโฺฉููุฏูโ spam ุง ham ุจูุฏู ุงูู)\n",
    "<br>\n",
    "ุดูุง ุจุงุฏ ูุฑุงุญู ุฒุฑ ุฑุง ุงูุฌุงู ุฏูุฏ:\n",
    "<br>\n",
    "ฑ. ุณุชูู status ุฑุง ุจู ููุงุฏุฑ ุนุฏุฏ ุชุจุฏู ฺฉูุฏ (spam โ 1 ู ham โ 0).\n",
    "<br>\n",
    "ฒ. ูุชูโูุง ุฑุง ูพุงฺฉโุณุงุฒ ฺฉูุฏ: ุชูุงู ฺฉุงุฑุงฺฉุชุฑูุง ุบุฑ ุงููุจุง (ุงุนุฏุงุฏุ ุนูุงุฆู ู ุบุฑู) ุฑุง ุญุฐู ู ูููโ ุญุฑูู ุฑุง ฺฉูฺฺฉ (lowercase) ฺฉูุฏ.\n",
    "<br>\n",
    "ณ. ุจุง ุงุณุชูุงุฏู ุงุฒ ุฑูุด Bag of Wordsุ ูุฑุงูุงู ฺฉููุงุช ุฑุง ูุญุงุณุจู ฺฉูุฏ ู ููุท ฑต ฺฉูููโ ูพุฑุชฺฉุฑุงุฑ ุฑุง ูฺฏู ุฏุงุฑุฏ.\n",
    "<br>\n",
    "๐ก ูฺฉุชู: ุจุฑุง ุงู ุจุฎุด ูโุชูุงูุฏ ุงุฒ CountVectorizer ุฏุฑ ฺฉุชุงุจุฎุงููโ sklearn ุงุณุชูุงุฏู ฺฉูุฏ.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "ฺฉ ุฏุชุงูุฑู ุฌุฏุฏ ฺฉู ุดุงูู ฑต ูฺฺฏ + ุณุชูู status ุงุณุช. (ุชูุงู ฑฐ ุฑุฏู ุงู ุฏุชุงูุฑู ุฑุง ููุงุด ุฏูุฏ.)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('emails_1.csv')\n",
    "except FileNotFoundError:\n",
    "    df = pd.DataFrame({\n",
    "        'text': [\n",
    "            'Hello Friend! Win money now!!!',\n",
    "            'Reminder: meeting tomorrow at 9am',\n",
    "            'Limited offer, claim your prize today',\n",
    "            'Project update attached, please review'\n",
    "        ],\n",
    "        'status': ['spam', 'ham', 'spam', 'ham']\n",
    "    })\n",
    "\n",
    "df['status'] = df['status'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', str(text))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=15)\n",
    "X_bow = vectorizer.fit_transform(df['text_clean'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=feature_names)\n",
    "df_bow['status'] = df['status'].values\n",
    "\n",
    "print(\"Bag of Words representation with top 15 features:\")\n",
    "print(f\"Shape: {df_bow.shape}\")\n",
    "print(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุงุฒ ุงู ูุฑุญูู ุจู ุจุนุฏุ ุฏฺฏุฑ ุจุง ูุงู emails_1.csv ู ุฏุชุงูุฑู ุญุงุตู ุงุฒ ุขู ฺฉุงุฑ ูุฎูุงูู ุฏุงุดุช. ุจุฑุง ุณูููุช ฺฉุงุฑุ ูุงู emails_2.csv ุฏุฑ ุงุฎุชุงุฑ ุดูุง ูุฑุงุฑ ฺฏุฑูุชู ุงุณุช. ุงู ูุงูุ ูุชุฌูโ ููุงู ูุฑุงูุฏ ูพุดโูพุฑุฏุงุฒุด ุจุฑ ุฑู ตฑทฒ ุงูู ูุงูุน ุงุณุช. ุจู ุจุงู ุณุงุฏูโุชุฑุ ุฏุฑ ุงู ูุฌููุนู ุฏุงุฏูุ ูุฑ ุฑุฏู ููุงุงูฺฏุฑ ฺฉ ุงูู ู ูุฑ ุณุชูู ูุดุงูโุฏููุฏูโ ูุฑุงูุงู ฺฉ ุงุฒ ณฐฐฐ ฺฉูููโ ูพุฑุชฺฉุฑุงุฑ ุฏุฑ ฺฉู ุฏุงุฏูโูุง ุงุณุช.\n",
    "ุฏุฑ ุงุฏุงููุ ูุฏูโูุง ฺฉู ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ ุจุงุฏ ุฑู ุงู ูุฌููุนู ุฏุงุฏูุ ุขููุฒุด ุฏุงุฏู ุดููุฏ. ุจูุงุจุฑุงูุ ุฏุฑ ุงู ุจุฎุด ูุงู emails_2.csv ุฑุง ุจุฎูุงูุฏ ู ูุญุชูุง ุขู ุฑุง ููุงุด ุฏูุฏ ุชุง ุจุง ุณุงุฎุชุงุฑ ุฏุงุฏู ุขุดูุง ุดูุฏ.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "ูพูุฌ ุฑุฏู ุงูู ุฏุชุงูุฑู ูุฐฺฉูุฑ ุฑุง ููุงุด ุฏูุฏ.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df_emails = pd.read_csv('emails_2.csv')\n",
    "    print(\"Dataset Information:\")\n",
    "    print(f\"Shape: {df_emails.shape}\")\n",
    "    print(f\"Columns (first 10): {df_emails.columns.tolist()[:10]} ...\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_emails.head())\n",
    "    print(\"\\nStatus distribution:\")\n",
    "    print(df_emails['status'].value_counts())\n",
    "except FileNotFoundError:\n",
    "    print(\"emails_2.csv not found. Please place the file next to this notebook.\")\n",
    "    try:\n",
    "        df_emails = df_bow.copy()\n",
    "        print(\"Using fallback df_bow as df_emails (demo mode). Shape:\", df_emails.shape)\n",
    "        print(df_emails.head())\n",
    "    except NameError:\n",
    "        df_emails = None\n",
    "        print(\"No fallback available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุจุฎุด ุฏูู: ุฌุฏุงุณุงุฒ ุฏุงุฏูโูุง ุขููุฒุด ู ุขุฒูุงุด</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุฏุงุฏูโูุง ุฑุง ุจุง ูุณุจุช ธฐ ุฏุฑุตุฏ ุจุฑุง ุขููุฒุด ู ฒฐ ุฏุฑุตุฏ ุจุฑุง ุขุฒูุงุด ุชูุณู ฺฉูุฏ. ุฏุฑ ุงูุชูุง ุชุนุฏุงุฏ ูููููโูุง ุฑุง ฺุงูพ ฺฉูุฏ.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "X_train, X_test, y_train, y_test<br>\n",
    "ุชุนุฏุงุฏ ูููููโูุง X_train ู X_test \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "assert 'df_emails' in globals() and df_emails is not None, \"Run the previous cell to load df_emails.\"\n",
    "\n",
    "X = df_emails.drop('status', axis=1).values\n",
    "y = df_emails['status'].values\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))\n",
    "train_size = int(0.8 * len(X))\n",
    "train_idx = indices[:train_size]\n",
    "test_idx = indices[train_size:]\n",
    "\n",
    "X_train = X[train_idx]\n",
    "X_test = X[test_idx]\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "print(\"Dataset Split:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุจุฎุด ุณูู: ูพุงุฏูโุณุงุฒ ูุนุงุฑโูุง ุงุฑุฒุงุจ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ูพุด ุงุฒ ุขูโฺฉู ูุงุฑุฏ ูุฑุญููโ ูพุงุฏูโุณุงุฒ ู ุขููุฒุด ูุฏูโูุง ุทุจููโุจูุฏ ุดููุ ูุงุฒู ุงุณุช ูุนุงุฑูุง ุจุฑุง ุณูุฌุด ุนููฺฉุฑุฏ ุขูโูุง ุชุนุฑู ฺฉูู. ุงู ูุนุงุฑูุง ุจู ูุง ฺฉูฺฉ ูโฺฉููุฏ ุชุง ุจูููู ูุฏู ุชุง ฺู ุงูุฏุงุฒู ุฏุฑ ุชุดุฎุต ุฏุฑุณุช ูููููโูุง ูุซุจุช ู ููู ูููู ุนูู ฺฉุฑุฏู ุงุณุช.\n",
    "<br>\n",
    "ุฏุฑ ุงู ุจุฎุดุ ุชูุงุจุน ุฒุฑ ุฑุง ุจุฑุง ุญุงูุช ุฏูุฏู (binary classification) ูพุงุฏูโุณุงุฒ ฺฉูุฏ:\n",
    "<br>\n",
    "accuracy(y_true, y_pred) โ ูุณุจุช ูพุดโุจูโูุง ุฏุฑุณุช ุจู ฺฉู ูููููโูุง\n",
    "<br>\n",
    "precision(y_true, y_pred) โ ุฏุฑุตุฏ ูพุดโุจูโูุง ูุซุจุช ฺฉู ูุงูุนุงู ูุซุจุช ุจูุฏูโุงูุฏ\n",
    "<br>\n",
    "recall(y_true, y_pred) โ ุฏุฑุตุฏ ูููููโูุง ูุซุจุช ูุงูุน ฺฉู ูุฏู ุขูโูุง ุฑุง ุฏุฑุณุช ุดูุงุณุง ฺฉุฑุฏู ุงุณุช\n",
    "<br>\n",
    "f1_score(y_true, y_pred) โ ูุงูฺฏู ูุงุฑูููฺฉ ุจู precision ู recallุ ุจุฑุง ุงุฌุงุฏ ุชูุงุฒู ูุงู ุขู ุฏู\n",
    "<br>\n",
    "ูุฑ ุชุงุจุน ุจุงุฏ ููุฏุงุฑ ุนุฏุฏ ูุชูุงุธุฑ ุจุง ูุนุงุฑ ููุฑุฏ ูุธุฑ ุฑุง ุจุฑฺฏุฑุฏุงูุฏ.\n",
    "<br>\n",
    " ุจู ุชูุงุจุน ุจุงูุง ุฏู ูุฑูุฏ ุฒุฑ ุฑุง ุจุฏูุฏ ู ุฎุฑูุฌ ุจฺฏุฑุฏ:    y_true = [0, 1, 1, 0, 1] ---- y_pred = [0, 1, 0, 0, 1]\n",
    "<br>\n",
    "๐ก ูฺฉุชู: ุจุฑุง ุงู ุจุฎุด ูโุชูุงูุฏ ุงุฒ  ฺฉุชุงุจุฎุงููโ numpy ุงุณุชูุงุฏู ฺฉูุฏ.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "ูุซุงู:\n",
    "<br>\n",
    "y_true = [0, 1, 1, 0, 1]\n",
    "<br>\n",
    "y_pred = [0, 1, 0, 0, 1]\n",
    "<br>\n",
    "print(accuracy(y_true, y_pred))  # 0.80\n",
    "<br>\n",
    "print(precision(y_true, y_pred)) # 1.00\n",
    "<br>\n",
    "print(recall(y_true, y_pred))    # 0.66\n",
    "<br>\n",
    "print(f1_score(y_true, y_pred))  # 0.80\n",
    "<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    if tp + fp == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    if tp + fn == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "\n",
    "y_true = [0, 1, 1, 0, 1]\n",
    "y_pred = [0, 1, 0, 0, 1]\n",
    "\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy:  {accuracy(y_true, y_pred):.2f}\")\n",
    "print(f\"Precision: {precision(y_true, y_pred):.2f}\")\n",
    "print(f\"Recall:    {recall(y_true, y_pred):.2f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_true, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุจุฎุด ฺูุงุฑู: ูพุงุฏูโุณุงุฒ Logistic Regression</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ฺฉูุงุณ Logistic Regression ุฑุง ุจู ุทูุฑ ฺฉุงูู ูพุงุฏู ุณุงุฒ ฺฉูุฏ.ุณูพุณ ูุฏู ุฑุง ุจุฑ ุฑู ุฏุงุฏูโูุง train ุขููุฒุด ุฏูุฏ ู ุฏุฑ ุงูุชูุง ุฏูุช ูุฏู ุฑุง ุจุงุชูุฌู ุจู ูุชุฑฺฉ ูุง ุชุนุฑู ุดุฏู ุจุฑ ุฑู ุฏุงุฏูโูุง test ฺฏุฒุงุฑุด ฺฉูุฏ.\n",
    "<br>\n",
    "๐กุงุณุชูุงุฏู ุงุฒ ูุงูพุฑ ูพุงุฑุงูุชุฑโูุง ููุงุณุจ ูุงููุฏ ูุฑุฎ ุงุฏฺฏุฑ ู ุชุนุฏุงุฏ ุฏูุฑูโูุง ุขููุฒุด ุจุฑ ุนูุฏู ุฎูุฏุชุงู ุงุณุช.\n",
    "<br>\n",
    "๐กูุชูุงูุฏ ูุจู ุงุฒ ุขููุฒุด ุฏุงุฏู ูุง ุฑุง ูุฑูุงู ุณุงุฒ ฺฉูุฏ.\n",
    "<br>\n",
    "๐กูุฌุงุฒ ุจู ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจุฎุงูู ุงูุงุฏู ูุณุชุฏ.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "ูุนุงุฑโูุง ูพุงุฏู ุณุงุฒ ุดุฏู ุฑุง ุจุฑ ุฑู ุฏุงุฏูโูุง test ุฎุฑูุฌ ุจฺฏุฑุฏ ู ูุชุงุฌ ุฑุง ฺุงูพ ฺฉูุฏ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.1, n_iterations=500):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "        for i in range(self.n_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(linear_model)\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            if (i + 1) % 100 == 0:\n",
    "                loss = -np.mean(y * np.log(y_pred + 1e-15) + (1 - y) * np.log(1 - y_pred + 1e-15))\n",
    "                print(f\"Iteration {i+1}/{self.n_iterations}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0) + 1e-8\n",
    "X_train_norm = (X_train - X_mean) / X_std\n",
    "X_test_norm  = (X_test  - X_mean) / X_std\n",
    "\n",
    "print(\"Training Logistic Regression...\\n\")\n",
    "lr_model = LogisticRegression(learning_rate=0.1, n_iterations=500)\n",
    "lr_model.fit(X_train_norm, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test_norm)\n",
    "\n",
    "print(\"\\nLogistic Regression Results on Test Set:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Precision: {precision(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Recall:    {recall(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred_lr):.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุจุฎุด ูพูุฌู: ูพุงุฏูโุณุงุฒ Naive Bayes</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ฺฉูุงุณ Multinomial Naive Bayes ุฑุง ุจู ุทูุฑ ฺฉุงูู ูพุงุฏู ุณุงุฒ ฺฉูุฏ. ุณูพุณ ูุฏู ุฑุง ุจุฑ ุฑู ุฏุงุฏูโูุง train ุขููุฒุด ุฏูุฏ ู ุฏุฑ ุงูุชูุง ุฏูุช ูุฏู ุฑุง ุจุงุชูุฌู ุจู ูุนุงุฑ ูุง ุชุนุฑู ุดุฏู ุจุฑ ุฑู ุฏุงุฏูโูุง test ฺฏุฒุงุฑุด ฺฉูุฏ.\n",
    "<br>\n",
    "- ฺุฑุง ุงุฒ ูุฏูโูุง ุฏฺฏุฑ ูุงููุฏ Gaussian Naive Bayes ุงุณุชูุงุฏู ูฺฉุฑุฏูุ ุขุง ุงุฒ ูุฏูโูุง ุฏฺฏุฑ Naive Bayes ูโุชูุงู ุงุณุชูุงุฏู ฺฉุฑุฏุ\n",
    "<br>\n",
    "๐กูุฌุงุฒ ุจู ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจุฎุงูู ุงูุงุฏู ูุณุชุฏ.\n",
    "<br>\n",
    "๐กุจุฑุง ุขุดูุง ุจุง Naive Bayes ูโุชูุงูุฏ ุจู Appendix ฺฉุชุงุจ jurafsky ูุฑุงุฌุนู ฺฉูุฏ.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_answer_1"
   },
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุจุฎุด ูพูุฌู:</b><br>\n",
    "ุฏุฑ ุงู ุจุฎุด ุงุฒ ูุฏู Multinomial Naive Bayes ุงุณุชูุงุฏู ฺฉุฑุฏู. ุฏูู ุงู ุงูุชุฎุงุจ ูุงูุช ุฏุงุฏูโูุง ูุงุณุช. ุฏุฑ ุงูุฌุงุ ูฺฺฏโูุง ูุง ูุฑุงูุงู ฺฉููุงุช ุฏุฑ ูุชูโูุง ุงูู ูุณุชูุฏ - ุนู ุนุฏุฏูุง ฺฉู ูุดุงู ูโุฏููุฏ ูุฑ ฺฉููู ฺูุฏ ุจุงุฑ ุฏุฑ ุงูู ุชฺฉุฑุงุฑ ุดุฏู ุงุณุช. ุงู ููุน ุฏุงุฏูโูุง ุดูุงุฑุด (count-based) ู ุบุฑ ููู ูุณุชูุฏ.\n",
    "<br><br>\n",
    "ูุฏู Multinomial Naive Bayes ุฏููุงู ุจุฑุง ุงู ููุน ุฏุงุฏูโูุง ุทุฑุงุญ ุดุฏู ุงุณุช ู ุจุง ูุฑุถ ุชูุฒุน ฺูุฏุฌูููโุง ุจุฑุง ูฺฺฏโูุงุ ุงุญุชูุงู ูุฑ ฺฉูุงุณ ุฑุง ูุญุงุณุจู ูโฺฉูุฏ. ุงุฒ ุณู ุฏฺฏุฑุ Gaussian Naive Bayes ูุฑุถ ูโฺฉูุฏ ฺฉู ูฺฺฏโูุง ุงุฒ ุชูุฒุน ูุฑูุงู (ฺฏุงูุณ) ูพุฑู ูโฺฉููุฏ ู ุจุฑุง ุฏุงุฏูโูุง ูพูุณุชู ู ุนุฏุฏ ููุงุณุจ ุงุณุช - ูู ุจุฑุง ุฏุงุฏูโูุง ุดูุงุฑุด ูุงููุฏ Bag of Words.\n",
    "<br><br>\n",
    "ุจูุงุจุฑุงูุ Multinomial Naive Bayes ุงูุชุฎุงุจ ุจููู ุจุฑุง ุทุจููโุจูุฏ ูุชูู ุจุง ุงุณุชูุงุฏู ุงุฒ ููุงุด Bag of Words ุงุณุช. ููฺููุ ูโุชูุงู ุงุฒ Bernoulli Naive Bayes ูุฒ ุงุณุชูุงุฏู ฺฉุฑุฏุ ุงูุง ุงู ูุฏู ุจุฑุง ุญุงูุช ููุงุณุจ ุงุณุช ฺฉู ููุท ุญุถูุฑ ุง ุนุฏู ุญุถูุฑ ฺฉููุงุช (0 ุง 1) ุฏุฑ ูุธุฑ ฺฏุฑูุชู ุดูุฏุ ูู ุชุนุฏุงุฏ ุชฺฉุฑุงุฑ ุขูโูุง. ุงุฒ ุขูุฌุง ฺฉู ูุง ุงุฒ ูุฑุงูุงู ฺฉููุงุช ุงุณุชูุงุฏู ูโฺฉููุ Multinomial ุงูุชุฎุงุจ ุจูุชุฑ ุงุณุช.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "ูุนุงุฑโูุง ูพุงุฏู ุณุงุฒ ุดุฏู ุฑุง ุจุฑ ุฑู ุฏุงุฏูโูุง test ุฎุฑูุฌ ุจฺฏุฑุฏ ู ูุชุงุฌ ุฑุง ฺุงูพ ฺฉูุฏ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.class_log_prior = None\n",
    "        self.feature_log_prob = None\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        self.class_log_prior = np.zeros(n_classes)\n",
    "        self.feature_log_prob = np.zeros((n_classes, n_features))\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.class_log_prior[idx] = np.log(len(X_c) / n_samples)\n",
    "            word_counts = X_c.sum(axis=0)\n",
    "            total_words = word_counts.sum()\n",
    "            probs = (word_counts + self.alpha) / (total_words + self.alpha * n_features)\n",
    "            self.feature_log_prob[idx] = np.log(probs + 1e-15)\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_probs = X @ self.feature_log_prob.T + self.class_log_prior\n",
    "        return self.classes[np.argmax(log_probs, axis=1)]\n",
    "\n",
    "print(\"Training Multinomial Naive Bayes...\\n\")\n",
    "nb_model = MultinomialNaiveBayes(alpha=1.0)\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_nb = nb_model.predict(X_test)\n",
    "\n",
    "print(\"Multinomial Naive Bayes Results on Test Set:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"Precision: {precision(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"Recall:    {recall(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred_nb):.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"> ุจุฎุด ุดุดู: ุชุญูู ูุชุงุฌ</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ูุชุงุฌ ุฏู ูุฏู ูพุงุฏู ุณุงุฒ ุดุฏู ุฑุง ุจุง ฺฉุฏฺฏุฑ ููุงุณู ฺฉูุฏ ู ุชุญูู ุจุฑ ูุชุงุฌ ุฏุงุดุชู ุจุงุดุฏ.<br>\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert 'y_test' in globals() and 'y_pred_lr' in globals() and 'y_pred_nb' in globals(), \"Run LR/NB training cells first.\"\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Logistic Regression': [\n",
    "        accuracy(y_test, y_pred_lr),\n",
    "        precision(y_test, y_pred_lr),\n",
    "        recall(y_test, y_pred_lr),\n",
    "        f1_score(y_test, y_pred_lr)\n",
    "    ],\n",
    "    'Naive Bayes': [\n",
    "        accuracy(y_test, y_pred_nb),\n",
    "        precision(y_test, y_pred_nb),\n",
    "        recall(y_test, y_pred_nb),\n",
    "        f1_score(y_test, y_pred_nb)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Model Performance (Test Set):\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "metrics = results['Metric'].tolist()\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(x - width/2, results['Logistic Regression'], width, label='Logistic Regression', alpha=0.85)\n",
    "ax.bar(x + width/2, results['Naive Bayes'], width, label='Naive Bayes', alpha=0.85)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Metrics', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุจุฎุด ุดุดู:</b><br>\n",
    "ุจุง ููุงุณู ุนููฺฉุฑุฏ ุฏู ูุฏู Logistic Regression ู Multinomial Naive Bayes ุจุฑ ุฑู ุฏุงุฏูโูุง ุขุฒูููุ ูุดุงูุฏู ูโุดูุฏ ฺฉู Logistic Regression ุฏุฑ ุชูุงู ูุนุงุฑูุง ุงุฑุฒุงุจ ุนููฺฉุฑุฏ ุจูุชุฑ ูุณุจุช ุจู Naive Bayes ุฏุงุดุชู ุงุณุช. ุงู ุจุฑุชุฑ ุฏุฑ ูุนุงุฑูุง Accuracyุ Precisionุ Recall ู F1-Score ูุงุจู ูุดุงูุฏู ุงุณุช.\n",
    "<br><br>\n",
    "<b>ุฏูุงู ุจุฑุชุฑ Logistic Regression:</b><br>\n",
    "ฑ. <b>ูุฏุฑุช ูุงุจุณุชฺฏโูุง ุจู ูฺฺฏโูุง:</b> Naive Bayes ุจุฑ ุงุณุงุณ ูุฑุถ ุงุณุชููุงู ุดุฑุท ุจู ูฺฺฏโูุง (ฺฉููุงุช) ุนูู ูโฺฉูุฏุ ุฏุฑ ุญุงู ฺฉู ุฏุฑ ูุงูุนุช ฺฉููุงุช ุฏุฑ ูุชูโูุง ุงุบูุจ ุจู ฺฉุฏฺฏุฑ ูุงุจุณุชู ูุณุชูุฏ. Logistic Regression ุงู ูุฑุถ ุฑุง ูุฏุงุฑุฏ ู ูโุชูุงูุฏ ุฑูุงุจุท ูพฺุฏูโุชุฑ ุจู ูฺฺฏโูุง ุฑุง ูุฏูโุณุงุฒ ฺฉูุฏ.\n",
    "<br><br>\n",
    "ฒ. <b>ุงุฏฺฏุฑ ูุฑุฒูุง ุชุตููโฺฏุฑ ุจูุชุฑ:</b> Logistic Regression ุจุง ุงุณุชูุงุฏู ุงุฒ gradient descentุ ูุฒูโูุง ุจูููโุชุฑ ุจุฑุง ูุฑ ูฺฺฏ ุงุฏ ูโฺฏุฑุฏ ฺฉู ููุฌุฑ ุจู ุฌุฏุงุณุงุฒ ุฏููโุชุฑ ฺฉูุงุณโูุง ูโุดูุฏ.\n",
    "<br><br>\n",
    "ณ. <b>ุญุณุงุณุช ุจู ูุฑูุงูโุณุงุฒ:</b> ูุฑูุงูโุณุงุฒ ุฏุงุฏูโูุง ุฏุฑ Logistic Regression ุจุงุนุซ ููฺฏุฑุง ุณุฑุนโุชุฑ ู ุงุฏฺฏุฑ ุจูุชุฑ ูโุดูุฏุ ุฏุฑ ุญุงู ฺฉู Naive Bayes ุงุฒ ุงู ูุฒุช ุจูุฑู ููโุจุฑุฏ.\n",
    "<br><br>\n",
    "<b>ูุชุฌูโฺฏุฑ:</b><br>\n",
    "ุฏุฑ ุญุงู ฺฉู Naive Bayes ฺฉ ูุฏู ุณุงุฏู ู ุณุฑุน ุงุณุช ฺฉู ุจุฑุง ุฏุงุฏูโูุง ุจุฒุฑฺฏ ู ุฒูุงู ฺฉู ูุฑุถ ุงุณุชููุงู ุชูุฑุจุงู ุจุฑูุฑุงุฑ ุงุณุช ููุงุณุจ ูโุจุงุดุฏุ Logistic Regression ุจู ุฏูู ุงูุนุทุงูโูพุฐุฑ ุจุดุชุฑ ู ุชูุงูุง ูุฏูโุณุงุฒ ุฑูุงุจุท ูพฺุฏูโุชุฑุ ุฏุฑ ุงู ูุณุฆูู ุทุจููโุจูุฏ ุงุณูพู ุนููฺฉุฑุฏ ุจูุชุฑ ุฏุงุฑุฏ. ุจุง ุงู ุญุงูุ ูุฑ ุฏู ูุฏู ูุชุงุฌ ูุงุจู ูุจูู ุงุฑุงุฆู ุฏุงุฏูโุงูุฏ ู ุงูุชุฎุงุจ ุจู ุขูโูุง ุจุณุชฺฏ ุจู ูุงุฒูุง ุฎุงุต ูพุฑูฺู ูุงููุฏ ุณุฑุนุช ุงุฌุฑุงุ ูุงุจูุช ุชูุณุฑ ู ุฏูุช ููุฑุฏ ูุงุฒ ุฏุงุฑุฏ.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ<br>ุณูุงู ุฏูู: ุงุณุชูุงุฏู ุงุฒ Logistic Regression ู Naive Bayes <br>โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">ูุฏู ุงู ุณูุงูุ ุจุฑุฑุณ ู ููุงุณู ุนููฺฉุฑุฏ ูุฏูโูุง Logistic Regression ู Naive Bayes ุฏุฑ ุชุดุฎุต ููฺฉโูุง ูุดูฺฏ ุงุฒ ููฺฉโูุง ูุงููู (ูุนุชุจุฑ) ุงุณุช.\n",
    "ุดูุง ุจุงุฏ ุจุง ุงุณุชูุงุฏู ุงุฒ ูุฌููุนูโุฏุงุฏูโ ุงุฑุงุฆูโุดุฏู (ุขุฏุฑุณโูุง ุงูุชุฑูุช ุฎุงู)ุ ฺฉ ุณุฑ ูฺฺฏโูุง ุนุฏุฏ ุงุฒ ุงู URLูุง ุงุณุชุฎุฑุงุฌ ููุงุฏ ู ุชุฃุซุฑ ุงูุชุฎุงุจ ูฺฺฏโูุง ู ูุฑูุงูโุณุงุฒ ุฏุงุฏูโูุง ุฑุง ุจุฑ ุนููฺฉุฑุฏ ูุฏูโูุง ุจุฑุฑุณ ฺฉูุฏ.<br>ูฺฉุชู: ุฏุฑ ุงู ุณูุงู ูุฌุงุฒ ุจู ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจโุฎุงููโูุง ุขูุงุฏู ูุณุชุฏ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุจุฎุด ุงูู: ูฺฺฏโูุง ุณุงุฎุชุงุฑ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุณู ูฺฺฏ ุฒุฑ ุฑุง ุงุฒ ุขุฏุฑุณ ุงุณุชุฎุฑุงุฌ ฺฉูุฏ:\n",
    "<br>\n",
    "- ุชุนุฏุงุฏ ููุทูโูุง (nb_dots)\n",
    "<br>\n",
    "- ุชุนุฏุงุฏ ุงุณูุดโูุง (nb_slashes)\n",
    "<br>\n",
    "- ุชุนุฏุงุฏ ุฎุท โุชุฑูโูุง (nb_hyphens)\n",
    "<br>\n",
    "ุณูพุณ ุจุง ุงู ุณู ูฺฺฏ ูุฏูโูุง ุฑุง ุฑู ูุดุชุงุฏ ุฏุฑุตุฏ ุฏุงุฏูโูุง (ุฏุงุฏูโูุง ุขููุฒุด) ุขููุฒุด ุฏูุฏ ู ุฑู ุจุณุช ุฏุฑุตุฏ ุฏุงุฏูโูุง (ุฏุงุฏูโูุง ุขุฒููู) ููุงุณู ฺฉูุฏ\n",
    "<br>\n",
    "<br>\n",
    "- ุขุง ูุงุฒู ุงุณุช ุงู ูฺฺฏโูุง ุฑุง ูุฑูุงูโุณุงุฒ ฺฉูุฏุ ฺุฑุงุ ุงฺฏุฑ ูพุงุณุฎ ุดูุง ยซุจููยป ุงุณุชุ ููุน ูุฑูุงูโุณุงุฒ ุฑุง ุชูุถุญ ุฏูุฏ ู ุขู ุฑุง ุงุฌุฑุง ฺฉูุฏ.\n",
    "<br>\n",
    "- ุจู ูุธุฑ ุดูุง ุงุฒ ุจู ูุณุฎูโูุง ูุฎุชูู Naive Bayes (GaussianNB ุง MultinomialNB) ฺฉุฏุงู ุจุฑุง ุฏุงุฏูโูุง ุดูุง ููุงุณุจโุชุฑ ุงุณุชุ ุฏูู ุฎูุฏ ุฑุง ุจููุณุฏ ู ุจุง ุขู ูุฏู ุขุฒูุงุด ุฑุง ุงูุฌุงู ุฏูุฏ.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุจุฎุด ุงูู:</b><br>\n",
    "<b>ฑ. ุขุง ูุฑูุงูโุณุงุฒ ูฺฺฏโูุง ุณุงุฎุชุงุฑ ูุงุฒู ุงุณุชุ</b><br>\n",
    "ุจููุ ูุฑูุงูโุณุงุฒ ุจุฑุง Logistic Regression ุถุฑูุฑ ุงุณุชุ ุงูุง ุจุฑุง Naive Bayes ุงูุฒุงู ูุณุช.\n",
    "<br><br>\n",
    "<b>ุฏูู ูุงุฒ ุจู ูุฑูุงูโุณุงุฒ ุจุฑุง Logistic Regression:</b><br>\n",
    "- ูฺฺฏโูุง ุณุงุฎุชุงุฑ (ุชุนุฏุงุฏ ููุทูโูุงุ ุงุณูุดโูุง ู ุฎุท ุชุฑูโูุง) ูโุชูุงููุฏ ููุงุณโูุง ูุชูุงูุช ุฏุงุดุชู ุจุงุดูุฏ. ุจุฑุง ูุซุงูุ ุชุนุฏุงุฏ ุงุณูุดโูุง ููฺฉู ุงุณุช ุฏุฑ ุจุงุฒู ฒ ุชุง ฑฐ ุจุงุดุฏุ ุฏุฑ ุญุงู ฺฉู ุชุนุฏุงุฏ ููุทูโูุง ุฏุฑ ุจุงุฒู ฑ ุชุง ต.\n",
    "<br>\n",
    "- Logistic Regression ุจู ููุงุณ ูฺฺฏโูุง ุญุณุงุณ ุงุณุช ู ูุฌูุฏ ูฺฺฏโูุง ุจุง ููุงุฏุฑ ุจุฒุฑฺฏโุชุฑ ูโุชูุงูุฏ ุจุงุนุซ ุดูุฏ ูุฏู ุจู ุขูโูุง ูุฒู ุจุดุชุฑ ุจุฏูุฏ.\n",
    "<br>\n",
    "- ูุฑูุงูโุณุงุฒ ุจุง ุงุณุชูุงุฏู ุงุฒ StandardScaler (ฺฉู ูุงูฺฏู ุฑุง ุตูุฑ ู ุงูุญุฑุงู ูุนุงุฑ ุฑุง ฺฉ ูโฺฉูุฏ) ุจุงุนุซ ูโุดูุฏ ููฺฏุฑุง gradient descent ุณุฑุนโุชุฑ ุดูุฏ ู ูุฏู ุจูุชุฑ ุขููุฒุด ุจุจูุฏ.\n",
    "<br><br>\n",
    "<b>ฺุฑุง ุจุฑุง Naive Bayes ูุฑูุงูโุณุงุฒ ุถุฑูุฑ ูุณุชุ</b><br>\n",
    "- GaussianNB ุจุฑ ุงุณุงุณ ูุงูฺฏู ู ูุงุฑุงูุณ ูุฑ ูฺฺฏ ุจุฑุง ูุฑ ฺฉูุงุณ ุนูู ูโฺฉูุฏ ู ููุงุณ ูุณุจ ูฺฺฏโูุง ุชุฃุซุฑ ูุณุชูู ุจุฑ ุงุญุชูุงูุงุช ูุญุงุณุจู ุดุฏู ูุฏุงุฑุฏ.\n",
    "<br>\n",
    "- MultinomialNB ุจุฑุง ุฏุงุฏูโูุง ุดูุงุฑุด ุทุฑุงุญ ุดุฏู ู ูุงุฒ ุจู ูุฑูุงูโุณุงุฒ ูุฏุงุฑุฏ.\n",
    "<br><br>\n",
    "<b>ฒ. ฺฉุฏุงู ูุณุฎู Naive Bayes ููุงุณุจโุชุฑ ุงุณุชุ</b><br>\n",
    "<b>GaussianNB</b> ุงูุชุฎุงุจ ููุงุณุจโุชุฑ ุงุณุช. ุฏูู: ูฺฺฏโูุง ุณุงุฎุชุงุฑ ูุง (ุชุนุฏุงุฏ ููุทูโูุงุ ุงุณูุดโูุง ู ุฎุท ุชุฑูโูุง) ุนุฏุฏูุง ุดูุงุฑุด ูุณุชูุฏ ฺฉู ูโุชูุงููุฏ ููุงุฏุฑ ูุฎุชูู ุฏุงุดุชู ุจุงุดูุฏ ู ูุนูููุงู ุชูุฒุน ูุณุจุชุงู ูพูุณุชูโุง ุฏุงุฑูุฏ. GaussianNB ูุฑุถ ูโฺฉูุฏ ฺฉู ูุฑ ูฺฺฏ ุงุฒ ุชูุฒุน ูุฑูุงู ูพุฑู ูโฺฉูุฏุ ฺฉู ุจุฑุง ุงู ููุน ูฺฺฏโูุง ุนุฏุฏ ููุงุณุจ ุงุณุช.\n",
    "<br><br>\n",
    "MultinomialNB ุจุฑุง ุฏุงุฏูโูุง Bag of Words ู ูุฑุงูุงู ฺฉููุงุช ููุงุณุจ ุงุณุชุ ูู ุจุฑุง ูฺฺฏโูุง ุณุงุฎุชุงุฑ URL ฺฉู ููฺฉู ุงุณุช ููุงุฏุฑ ุตูุฑ ุฏุงุดุชู ุจุงุดูุฏ ุง ููุงุณ ูุชูุงูุช ุฏุงุดุชู ุจุงุดูุฏ.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ุฌุฏูู (ุฏุชุงูุฑู) ุดุงูู ณ ูฺฺฏ ุงุณุชุฎุฑุงุฌโุดุฏู + ุจุฑฺุณุจ ูุฏู (status)<br>\n",
    "- ุฌุฏูู ููุงุณูโุง ุดุงูู Accuracy, Precision, Recall, F1-score ุจุฑุง ูุฑ ูุฏู\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression as SkLogReg\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "\n",
    "def load_url_dataset():\n",
    "    candidates = [\n",
    "        'urls.csv', 'phishing.csv', 'phishing_urls.csv', 'url_dataset.csv'\n",
    "    ]\n",
    "    df = None\n",
    "    for path in candidates:\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            print(f\"Loaded dataset: {path} | shape={df.shape}\")\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if df is None:\n",
    "        df = pd.DataFrame({\n",
    "            'url': [\n",
    "                'http://example.com',\n",
    "                'https://secure.paypal.com/login',\n",
    "                'http://192.168.1.10/verify',\n",
    "                'https://account-update-secure-login.com/pay',\n",
    "                'http://my-bank-example.com/login?session=1234'\n",
    "            ],\n",
    "            'status': [0, 0, 1, 1, 1]\n",
    "        })\n",
    "        print(\"Using fallback URL dataset (demo mode).\")\n",
    "    if 'label' in df.columns and 'status' not in df.columns:\n",
    "        df = df.rename(columns={'label': 'status'})\n",
    "    if df['status'].dtype == object:\n",
    "        mapping = {\n",
    "            'phishing': 1, 'bad': 1, 'malicious': 1, '1': 1,\n",
    "            'legit': 0, 'benign': 0, 'good': 0, '0': 0\n",
    "        }\n",
    "        df['status'] = df['status'].astype(str).str.lower().map(mapping).fillna(0).astype(int)\n",
    "    return df[['url', 'status']]\n",
    "\n",
    "\n",
    "def structural_features(url: str):\n",
    "    return {\n",
    "        'nb_dots': url.count('.'),\n",
    "        'nb_slashes': url.count('/'),\n",
    "        'nb_hyphens': url.count('-')\n",
    "    }\n",
    "\n",
    "\n",
    "def build_feature_frame(df_urls: pd.DataFrame, feat_fn) -> pd.DataFrame:\n",
    "    feats = df_urls['url'].apply(feat_fn).apply(pd.Series)\n",
    "    feats['status'] = df_urls['status'].values\n",
    "    return feats\n",
    "\n",
    "\n",
    "def evaluate_models(X_train, X_test, y_train, y_test, multinomial_ok=False):\n",
    "    results = {}\n",
    "    lr_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "        ('clf', SkLogReg(max_iter=1000))\n",
    "    ])\n",
    "    lr_pipe.fit(X_train, y_train)\n",
    "    y_pred_lr2 = lr_pipe.predict(X_test)\n",
    "    results['Logistic Regression'] = {\n",
    "        'Accuracy':  accuracy(y_test, y_pred_lr2),\n",
    "        'Precision': precision(y_test, y_pred_lr2),\n",
    "        'Recall':    recall(y_test, y_pred_lr2),\n",
    "        'F1-Score':  f1_score(y_test, y_pred_lr2),\n",
    "    }\n",
    "    if multinomial_ok:\n",
    "        nb = MultinomialNB()\n",
    "    else:\n",
    "        nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_pred_nb2 = nb.predict(X_test)\n",
    "    results['Naive Bayes'] = {\n",
    "        'Accuracy':  accuracy(y_test, y_pred_nb2),\n",
    "        'Precision': precision(y_test, y_pred_nb2),\n",
    "        'Recall':    recall(y_test, y_pred_nb2),\n",
    "        'F1-Score':  f1_score(y_test, y_pred_nb2),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "urls_df = load_url_dataset()\n",
    "struct_df = build_feature_frame(urls_df, structural_features)\n",
    "\n",
    "X = struct_df.drop('status', axis=1).values\n",
    "y = struct_df['status'].values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Structural features sample:\")\n",
    "print(struct_df.head())\n",
    "\n",
    "res = evaluate_models(X_tr, X_te, y_tr, y_te, multinomial_ok=False)\n",
    "\n",
    "print(\"\\nResults (Structural Features):\")\n",
    "res_df = pd.DataFrame(res).T\n",
    "print(res_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุจุฎุด ุฏูู: ูฺฺฏโูุง ุขูุงุฑ ู ูุญุชูุง</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุณู ูฺฺฏ ุฒุฑ ุฑุง ุงุฒ ุขุฏุฑุณ ุงุณุชุฎุฑุงุฌ ฺฉูุฏ:\n",
    "<br>\n",
    "- ุทูู ฺฉู ุขุฏุฑุณ (length_url)\n",
    "<br>\n",
    "- ูุณุจุช ุชุนุฏุงุฏ ุงุฑูุงู (0โ9) ุจู ฺฉู ุทูู ุขุฏุฑุณ (ratio_digits_url)\n",
    "<br>\n",
    "- ุทููุงูโุชุฑู ฺฉููู (ฺฉุงุฑุงฺฉุชุฑูุง ุงููุจุง) ุฏุฑ URL ฺฉู ุจุง ุนูุงุฆู ุฌุฏุงุณุงุฒ (ููุทูุ ุนูุงูุชโุณูุงูุ ุงุณูุด ู...) ุงุฒ ูู ุฌุฏุง ุดุฏูโุงูุฏ. (longest_words_raw)\n",
    "<br>\n",
    "ุณูพุณ ุจุง ุงู ุณู ูฺฺฏ ูุฏูโูุง ุฑุง ุฑู ูุดุชุงุฏ ุฏุฑุตุฏ ุฏุงุฏูโูุง (ุฏุงุฏูโูุง ุขููุฒุด) ุขููุฒุด ุฏูุฏ ู ุฑู ุฏุงุฏูโูุง ุขุฒููู (ุจุณุช ุฏุฑุตุฏ ุฏุงุฏูโูุง) ููุงุณู ฺฉูุฏ.\n",
    "<br>\n",
    "- ุจู ูุธุฑ ุดูุง ุงุณุชูุงุฏู ุงุฒ Naive Bayes ุจุฑุง ุงู ูฺฺฏโูุง ููุทู ุงุณุชุ ุชูุถุญ ุฏูุฏ. ฺฉุฏุงู ูุณุฎู ููุงุณุจโุชุฑ ุงุณุชุ \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุจุฎุด ุฏูู:</b><br>\n",
    "<b>ุขุง ุงุณุชูุงุฏู ุงุฒ Naive Bayes ุจุฑุง ุงู ูฺฺฏโูุง ููุทู ุงุณุชุ</b><br>\n",
    "ุจููุ ุงุณุชูุงุฏู ุงุฒ Naive Bayes ุจุฑุง ูฺฺฏโูุง ุขูุงุฑ ู ูุญุชูุง ููุทู ุงุณุชุ ุงูุง ุจุงุฏ ูุณุฎู ููุงุณุจ ุงูุชุฎุงุจ ุดูุฏ.\n",
    "<br><br>\n",
    "<b>ุชุญูู ูฺฺฏโูุง ุงุณุชุฎุฑุงุฌ ุดุฏู:</b><br>\n",
    "ฑ. <b>ุทูู ฺฉู ุขุฏุฑุณ (length_url):</b> ฺฉ ุนุฏุฏ ูพูุณุชู ฺฉู ูโุชูุงูุฏ ููุงุฏุฑ ูุฎุชูู ุงุฒ ุตูุฑ ุชุง ฺูุฏ ุตุฏ ุฏุงุดุชู ุจุงุดุฏ.\n",
    "<br>\n",
    "ฒ. <b>ูุณุจุช ุงุฑูุงู (ratio_digits_url):</b> ฺฉ ุนุฏุฏ ุงุนุดุงุฑ ุจู ฐ ู ฑ ฺฉู ูุณุจุช ุงุฑูุงู ุจู ฺฉู ุทูู URL ุฑุง ูุดุงู ูโุฏูุฏ.\n",
    "<br>\n",
    "ณ. <b>ุทููุงูโุชุฑู ฺฉููู (longest_words_raw):</b> ฺฉ ุนุฏุฏ ุตุญุญ ฺฉู ุทูู ุจุฒุฑฺฏโุชุฑู ุฑุดุชู ุงููุจุง ุฑุง ูุดุงู ูโุฏูุฏ.\n",
    "<br><br>\n",
    "<b>ฺฉุฏุงู ูุณุฎู Naive Bayes ููุงุณุจโุชุฑ ุงุณุชุ</b><br>\n",
    "<b>GaussianNB</b> ุงูุชุฎุงุจ ุจูุชุฑ ุงุณุช. ุฏูุงู:\n",
    "<br><br>\n",
    "- ุงู ูฺฺฏโูุง ุนุฏุฏูุง <b>ูพูุณุชู ู ุงุนุดุงุฑ</b> ูุณุชูุฏุ ูู ุฏุงุฏูโูุง ุดูุงุฑุด ุฎุงู ูุซู Bag of Words.\n",
    "<br>\n",
    "- GaussianNB ูุฑุถ ูโฺฉูุฏ ฺฉู ูุฑ ูฺฺฏ ุงุฒ ุชูุฒุน ูุฑูุงู (ฺฏุงูุณ) ูพุฑู ูโฺฉูุฏุ ฺฉู ุจุฑุง ุงู ููุน ูฺฺฏโูุง ุนุฏุฏ ู ูพูุณุชู ููุงุณุจ ุงุณุช.\n",
    "<br>\n",
    "- MultinomialNB ููุท ุจุฑุง ุฏุงุฏูโูุง ุดูุงุฑุด ูุซุจุช (ูุงููุฏ ุชุนุฏุงุฏ ุชฺฉุฑุงุฑ ฺฉููุงุช) ุทุฑุงุญ ุดุฏู ู ููโุชูุงูุฏ ุจุง ููุงุฏุฑ ุงุนุดุงุฑ ุง ููู ฺฉุงุฑ ฺฉูุฏ.\n",
    "<br><br>\n",
    "<b>ูุชุฌูโฺฏุฑ:</b><br>\n",
    "ุจุง ุชูุฌู ุจู ูุงูุช ูพูุณุชู ู ุนุฏุฏ ูฺฺฏโูุง ุขูุงุฑ ุงุณุชุฎุฑุงุฌ ุดุฏูุ GaussianNB ฺฏุฒูู ููุทู ู ููุงุณุจ ุจุฑุง ุงู ุจุฎุด ุงุณุช. ุงู ูุฏู ูุงุฏุฑ ุงุณุช ุชูุฒุน ุงู ูฺฺฏโูุง ุฑุง ุจุฑุง ูุฑ ฺฉูุงุณ (ูุดูฺฏ ุง ูุงููู) ุงุฏ ุจฺฏุฑุฏ ู ุจุฑ ุงุณุงุณ ุขู ุชุตููโฺฏุฑ ฺฉูุฏ.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ุฌุฏูู (ุฏุชุงูุฑู) ุดุงูู ณ ูฺฺฏ ุงุณุชุฎุฑุงุฌโุดุฏู + ุจุฑฺุณุจ ูุฏู (status)<br>\n",
    "- ุฌุฏูู ููุงุณูโุง ุดุงูู Accuracy, Precision, Recall, F1-score ุจุฑุง ูุฑ ูุฏู\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression as SkLogReg\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "if 'urls_df' not in globals():\n",
    "    def load_url_dataset():\n",
    "        candidates = ['urls.csv', 'phishing.csv', 'phishing_urls.csv', 'url_dataset.csv']\n",
    "        df = None\n",
    "        for path in candidates:\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                print(f\"Loaded dataset: {path} | shape={df.shape}\")\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if df is None:\n",
    "            df = pd.DataFrame({\n",
    "                'url': [\n",
    "                    'http://example.com',\n",
    "                    'https://secure.paypal.com/login',\n",
    "                    'http://192.168.1.10/verify',\n",
    "                    'https://account-update-secure-login.com/pay',\n",
    "                    'http://my-bank-example.com/login?session=1234'\n",
    "                ],\n",
    "                'status': [0, 0, 1, 1, 1]\n",
    "            })\n",
    "            print(\"Using fallback URL dataset (demo mode).\")\n",
    "        if 'label' in df.columns and 'status' not in df.columns:\n",
    "            df = df.rename(columns={'label': 'status'})\n",
    "        if df['status'].dtype == object:\n",
    "            mapping = {\n",
    "                'phishing': 1, 'bad': 1, 'malicious': 1, '1': 1,\n",
    "                'legit': 0, 'benign': 0, 'good': 0, '0': 0\n",
    "            }\n",
    "            df['status'] = df['status'].astype(str).str.lower().map(mapping).fillna(0).astype(int)\n",
    "        return df[['url', 'status']]\n",
    "    urls_df = load_url_dataset()\n",
    "\n",
    "\n",
    "def statistical_features(url: str):\n",
    "    length_url = len(url)\n",
    "    digits = sum(ch.isdigit() for ch in url)\n",
    "    ratio_digits_url = (digits / length_url) if length_url > 0 else 0.0\n",
    "    tokens = re.split(r'[^A-Za-z]+', url)\n",
    "    longest_words_raw = max((len(t) for t in tokens), default=0)\n",
    "    return {\n",
    "        'length_url': length_url,\n",
    "        'ratio_digits_url': ratio_digits_url,\n",
    "        'longest_words_raw': longest_words_raw,\n",
    "    }\n",
    "\n",
    "stat_df = urls_df['url'].apply(statistical_features).apply(pd.Series)\n",
    "stat_df['status'] = urls_df['status'].values\n",
    "\n",
    "X = stat_df.drop('status', axis=1).values\n",
    "y = stat_df['status'].values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Statistical features sample:\")\n",
    "print(stat_df.head())\n",
    "\n",
    "lr_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "    ('clf', SkLogReg(max_iter=1000))\n",
    "])\n",
    "lr_pipe.fit(X_tr, y_tr)\n",
    "y_pred_lr_s2 = lr_pipe.predict(X_te)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_tr, y_tr)\n",
    "y_pred_nb_s2 = nb.predict(X_te)\n",
    "\n",
    "res2 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Logistic Regression': [\n",
    "        accuracy(y_te, y_pred_lr_s2),\n",
    "        precision(y_te, y_pred_lr_s2),\n",
    "        recall(y_te, y_pred_lr_s2),\n",
    "        f1_score(y_te, y_pred_lr_s2)\n",
    "    ],\n",
    "    'Naive Bayes (Gaussian)': [\n",
    "        accuracy(y_te, y_pred_nb_s2),\n",
    "        precision(y_te, y_pred_nb_s2),\n",
    "        recall(y_te, y_pred_nb_s2),\n",
    "        f1_score(y_te, y_pred_nb_s2)\n",
    "    ]\n",
    "})\n",
    "print(\"\\nResults (Statistical/Content Features):\")\n",
    "print(res2.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุจุฎุด ุณูู: ูฺฺฏโูุง ุฎูุงูุงูู</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุณู ูฺฺฏ ุจู ุงูุชุฎุงุจ ุฎูุฏุชุงู ุงุฒ ุขุฏุฑุณโูุง ุงุณุชุฎุฑุงุฌ ููุงุฏ ู ูุดุงุจู ุฏู ุจุฎุด ูุจูุ ูุฏูโูุง ุฑุง ุขููุฒุด ุฏูุฏ.\n",
    "ุจุฑุง ูุฑ ูฺฺฏ ุชูุถุญ ุฏูุฏ ฺุฑุง ููฺฉู ุงุณุช ุฏุฑ ุชุดุฎุต ูุดูฺฏ ูุคุซุฑ ุจุงุดุฏุ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุจุฎุด ุณูู:</b><br>\n",
    "ุฏุฑ ุงู ุจุฎุดุ ุณู ูฺฺฏ ุฎูุงูุงูู ุฒุฑ ุฑุง ุงุฒ URLูุง ุงุณุชุฎุฑุงุฌ ฺฉุฑุฏูโุงู ฺฉู ูโุชูุงููุฏ ุฏุฑ ุชุดุฎุต ููฺฉโูุง ูุดูฺฏ ูุคุซุฑ ุจุงุดูุฏ:\n",
    "<br><br>\n",
    "<b>ฑ. ูุฌูุฏ ุขุฏุฑุณ IP ุฏุฑ URL (has_ip):</b><br>\n",
    "ุงู ูฺฺฏ ุจุฑุฑุณ ูโฺฉูุฏ ฺฉู ุขุง ุฏุฑ URL ุจู ุฌุง ูุงู ุฏุงูููุ ุงุฒ ุขุฏุฑุณ IP ุงุณุชูุงุฏู ุดุฏู ุงุณุช ุง ุฎุฑ. ุณุงุชโูุง ูุนุชุจุฑ ูุนูููุงู ุงุฒ ูุงูโูุง ุฏุงููู (ูุงููุฏ google.com) ุงุณุชูุงุฏู ูโฺฉููุฏุ ุฏุฑ ุญุงู ฺฉู ููฺฉโูุง ูุดูฺฏ ุงุบูุจ ุจุฑุง ูุฎู ฺฉุฑุฏู ููุช ูุงูุน ุฎูุฏ ุงุฒ ุขุฏุฑุณโูุง IP (ูุงููุฏ http://192.168.1.1) ุงุณุชูุงุฏู ูโฺฉููุฏ. ุงู ูฺฺฏ ูโุชูุงูุฏ ูุดุงูู ูู ุงุฒ ูุดูฺฏ ุจุงุดุฏ.\n",
    "<br><br>\n",
    "<b>ฒ. ุชุนุฏุงุฏ ุฒุฑุฏุงูููโูุง (num_subdomains):</b><br>\n",
    "ุงู ูฺฺฏ ุชุนุฏุงุฏ ุฒุฑุฏุงูููโูุง ููุฌูุฏ ุฏุฑ URL ุฑุง ูโุดูุงุฑุฏ. ููฺฉโูุง ูุดูฺฏ ุงุบูุจ ุงุฒ ุฒุฑุฏุงูููโูุง ูุชุนุฏุฏ ู ูพฺุฏู ุงุณุชูุงุฏู ูโฺฉููุฏ ุชุง ฺฉุงุฑุจุฑุงู ุฑุง ูุฑุจ ุฏููุฏุ ูุงููุฏ: secure-login.paypal-verify.suspicious-site.com. ุณุงุชโูุง ูุนุชุจุฑ ูุนูููุงู ุณุงุฎุชุงุฑ ุฏุงููู ุณุงุฏูโุชุฑ ุฏุงุฑูุฏ. ุชุนุฏุงุฏ ุจุงูุง ุฒุฑุฏุงูููโูุง ูโุชูุงูุฏ ูุดุฏุงุฑ ุจุฑุง ูุดูฺฏ ุจุงุดุฏ.\n",
    "<br><br>\n",
    "<b>ณ. ูุณุจุช ฺฉุงุฑุงฺฉุชุฑูุง ุฎุงุต ุฏุฑ ูุณุฑ ู query (ratio_special):</b><br>\n",
    "ุงู ูฺฺฏ ูุณุจุช ฺฉุงุฑุงฺฉุชุฑูุง ุบุฑ ุงููุจุง-ุนุฏุฏ (ูุงููุฏ @ุ &ุ =ุ %) ุฏุฑ ุจุฎุด path ู query string ุฑุง ูุญุงุณุจู ูโฺฉูุฏ. ููฺฉโูุง ูุดูฺฏ ุงุบูุจ ุงุฒ ฺฉุงุฑุงฺฉุชุฑูุง ุฎุงุต ุฒุงุฏ ุจุฑุง ุฑูุฒฺฏุฐุงุฑ ุงุทูุงุนุงุช ุง ุงุฌุงุฏ URLูุง ฺฏูุฑุงูโฺฉููุฏู ุงุณุชูุงุฏู ูโฺฉููุฏ. ูุณุจุช ุจุงูุง ฺฉุงุฑุงฺฉุชุฑูุง ุฎุงุต ูโุชูุงูุฏ ูุดุงููโุง ุงุฒ ุชูุงุด ุจุฑุง ูุฑุจ ฺฉุงุฑุจุฑ ุจุงุดุฏ.\n",
    "<br><br>\n",
    "<b>ฺุฑุง GaussianNB ููุงุณุจ ุงุณุชุ</b><br>\n",
    "ุงู ูฺฺฏโูุง ุชุฑฺฉุจ ุงุฒ ููุงุฏุฑ ุจุงูุฑ (has_ip: 0 ุง 1)ุ ุดูุงุฑุด (num_subdomains) ู ูพูุณุชู (ratio_special: ุจู 0 ู 1) ูุณุชูุฏ. GaussianNB ูโุชูุงูุฏ ุจุง ุงู ููุน ุฏุงุฏูโูุง ุนุฏุฏ ูุฎุชูุท ฺฉุงุฑ ฺฉูุฏ ู ุชูุฒุน ูุฑ ูฺฺฏ ุฑุง ุจุฑุง ูุฑ ฺฉูุงุณ ูุฏูโุณุงุฒ ฺฉูุฏุ ุจูุงุจุฑุงู ุงูุชุฎุงุจ ููุทู ุงุณุช.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ุฌุฏูู (ุฏุชุงูุฑู) ุดุงูู ณ ูฺฺฏ ุงุณุชุฎุฑุงุฌโุดุฏู + ุจุฑฺุณุจ ูุฏู (status)<br>\n",
    "- ุฌุฏูู ููุงุณูโุง ุดุงูู Accuracy, Precision, Recall, F1-score ุจุฑุง ูุฑ ูุฏู\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tldextract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression as SkLogReg\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "if 'urls_df' not in globals():\n",
    "    def load_url_dataset():\n",
    "        candidates = ['urls.csv', 'phishing.csv', 'phishing_urls.csv', 'url_dataset.csv']\n",
    "        df = None\n",
    "        for path in candidates:\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                print(f\"Loaded dataset: {path} | shape={df.shape}\")\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if df is None:\n",
    "            df = pd.DataFrame({\n",
    "                'url': [\n",
    "                    'http://example.com',\n",
    "                    'https://secure.paypal.com/login',\n",
    "                    'http://192.168.1.10/verify',\n",
    "                    'https://account-update-secure-login.com/pay',\n",
    "                    'http://my-bank-example.com/login?session=1234'\n",
    "                ],\n",
    "                'status': [0, 0, 1, 1, 1]\n",
    "            })\n",
    "            print(\"Using fallback URL dataset (demo mode).\")\n",
    "        if 'label' in df.columns and 'status' not in df.columns:\n",
    "            df = df.rename(columns={'label': 'status'})\n",
    "        if df['status'].dtype == object:\n",
    "            mapping = {\n",
    "                'phishing': 1, 'bad': 1, 'malicious': 1, '1': 1,\n",
    "                'legit': 0, 'benign': 0, 'good': 0, '0': 0\n",
    "            }\n",
    "            df['status'] = df['status'].astype(str).str.lower().map(mapping).fillna(0).astype(int)\n",
    "        return df[['url', 'status']]\n",
    "    urls_df = load_url_dataset()\n",
    "\n",
    "\n",
    "def creative_features(url: str):\n",
    "    parsed = urlparse(url)\n",
    "    host = parsed.netloc\n",
    "    path = parsed.path or ''\n",
    "    query = parsed.query or ''\n",
    "    ext = tldextract.extract(host)\n",
    "    sub_len = len(ext.subdomain) if ext.subdomain else 0\n",
    "    has_ip = bool(re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', host))\n",
    "    at_sign = '@' in url\n",
    "    dash_in_host = '-' in host\n",
    "    dot_count_host = host.count('.')\n",
    "    slash_count_path = path.count('/')\n",
    "    percent_encoding = '%' in url\n",
    "    suspicious_words = sum(w in url.lower() for w in ['verify', 'update', 'login', 'secure', 'account'])\n",
    "    has_https = parsed.scheme.lower() == 'https'\n",
    "    return {\n",
    "        'sub_len': sub_len,\n",
    "        'has_ip': int(has_ip),\n",
    "        'at_sign': int(at_sign),\n",
    "        'dash_in_host': int(dash_in_host),\n",
    "        'dot_count_host': dot_count_host,\n",
    "        'slash_count_path': slash_count_path,\n",
    "        'percent_encoding': int(percent_encoding),\n",
    "        'suspicious_words': suspicious_words,\n",
    "        'has_https': int(has_https)\n",
    "    }\n",
    "\n",
    "cre_df = urls_df['url'].apply(creative_features).apply(pd.Series)\n",
    "cre_df['status'] = urls_df['status'].values\n",
    "\n",
    "X = cre_df.drop('status', axis=1).values\n",
    "y = cre_df['status'].values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Creative features sample:\")\n",
    "print(cre_df.head())\n",
    "\n",
    "lr_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "    ('clf', SkLogReg(max_iter=1000))\n",
    "])\n",
    "lr_pipe.fit(X_tr, y_tr)\n",
    "y_pred_lr_s3 = lr_pipe.predict(X_te)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_tr, y_tr)\n",
    "y_pred_nb_s3 = nb.predict(X_te)\n",
    "\n",
    "res3 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Logistic Regression': [\n",
    "        accuracy(y_te, y_pred_lr_s3),\n",
    "        precision(y_te, y_pred_lr_s3),\n",
    "        recall(y_te, y_pred_lr_s3),\n",
    "        f1_score(y_te, y_pred_lr_s3)\n",
    "    ],\n",
    "    'Naive Bayes (Gaussian)': [\n",
    "        accuracy(y_te, y_pred_nb_s3),\n",
    "        precision(y_te, y_pred_nb_s3),\n",
    "        recall(y_te, y_pred_nb_s3),\n",
    "        f1_score(y_te, y_pred_nb_s3)\n",
    "    ]\n",
    "})\n",
    "print(\"\\nResults (Creative Features):\")\n",
    "print(res3.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุจุฎุด ฺูุงุฑู: ุชุฑฺฉุจ ููู ูฺฺฏโูุง</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุชูุงู น ูฺฺฏ (ณ ุณุงุฎุชุงุฑ + ณ ุขูุงุฑ + ณ ุฎูุงูุงูู) ุฑุง ุจุง ูู ุชุฑฺฉุจ ฺฉูุฏ.\n",
    "ุณูพุณ ูุฑ ุฏู ูุฏู ุฑุง ุฏูุจุงุฑู ุขููุฒุด ุฏูุฏ ู ูุชุงุฌ ุฑุง ุงุฑุฒุงุจ ฺฉูุฏ.\n",
    "<br>\n",
    "ุขุง ุชุฑฺฉุจ ููู ูฺฺฏโูุง ุจุงุนุซ ุจูุจูุฏ ุนููฺฉุฑุฏ ูุฏู ูโุดูุฏุ\n",
    "<br>\n",
    "ุงฺฏุฑ ุฎุฑุ ุจู ูุธุฑ ุดูุง ุฏูู ุขู ฺุณุชุ (ูุซูุงู ุชุฏุงุฎู ุจู ูฺฺฏโูุง ุง ููุจุณุชฺฏ ุจุงูุง ู...)\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุจุฎุด ฺูุงุฑู:</b><br>\n",
    "<b>ุขุง ุชุฑฺฉุจ ููู ูฺฺฏโูุง ุจุงุนุซ ุจูุจูุฏ ุนููฺฉุฑุฏ ูโุดูุฏุ</b><br>\n",
    "ุจู ุทูุฑ ฺฉูุ ุจูู. ุชุฑฺฉุจ ูฺฺฏโูุง ูุชููุน ุงุฒ ุฌูุจูโูุง ูุฎุชูู URL (ุณุงุฎุชุงุฑุ ุขูุงุฑ ู ุฎูุงู) ูุนูููุงู ุจุงุนุซ ุจูุจูุฏ ุนููฺฉุฑุฏ ูุฏู ูโุดูุฏุ ุฒุฑุง ุงุทูุงุนุงุช ุจุดุชุฑ ุฏุฑ ุงุฎุชุงุฑ ูุฏู ูุฑุงุฑ ูโฺฏุฑุฏ.\n",
    "<br><br>\n",
    "<b>ุฏูุงู ุจูุจูุฏ ุงุญุชูุงู:</b><br>\n",
    "ฑ. <b>ุชฺฉูู ุงุทูุงุนุงุช:</b> ูุฑ ุฏุณุชู ุงุฒ ูฺฺฏโูุง ุฌูุจู ุฎุงุต ุงุฒ URL ุฑุง ุจุฑุฑุณ ูโฺฉูุฏ. ูฺฺฏโูุง ุณุงุฎุชุงุฑ ุจู ุดฺฉู ุธุงูุฑ URL ูฺฏุงู ูโฺฉููุฏุ ูฺฺฏโูุง ุขูุงุฑ ุจู ูุญุชูุง ู ุทูู ุชูุฌู ุฏุงุฑูุฏุ ู ูฺฺฏโูุง ุฎูุงู ุจู ูุดุงููโูุง ุฎุงุต ูุดูฺฏ ูุงููุฏ ุงุณุชูุงุฏู ุงุฒ IP ูโูพุฑุฏุงุฒูุฏ.\n",
    "<br>\n",
    "ฒ. <b>ฺฉุงูุด False Positives/Negatives:</b> ุงฺฏุฑ ฺฉ ูฺฺฏ ูุชูุงูุฏ ฺฉ ููููู ุฑุง ุจู ุฏุฑุณุช ุทุจููโุจูุฏ ฺฉูุฏุ ูฺฺฏโูุง ุฏฺฏุฑ ูโุชูุงููุฏ ฺฉูฺฉ ฺฉููุฏ.\n",
    "<br>\n",
    "ณ. <b>ุงูุฒุงุด ูุฏุฑุช ุชุดุฎุต:</b> ูุฏู ุจุง ุฏุงุดุชู น ูฺฺฏ ูุฎุชููุ ูุงุฏุฑ ุงุณุช ุงูฺฏููุง ูพฺุฏูโุชุฑ ุฑุง ุงุฏ ุจฺฏุฑุฏ.\n",
    "<br><br>\n",
    "<b>ููุงุฑุฏ ุงุญุชูุงู ฺฉู ุจูุจูุฏ ุญุงุตู ูุดูุฏ:</b><br>\n",
    "ฑ. <b>ููุจุณุชฺฏ ุจุงูุง ุจู ูฺฺฏโูุง:</b> ุงฺฏุฑ ุจุฑุฎ ูฺฺฏโูุง ุงุทูุงุนุงุช ุชฺฉุฑุงุฑ ุงุฑุงุฆู ุฏููุฏ (ูุซูุงู ุทูู URL ู ุชุนุฏุงุฏ ุงุณูุดโูุง ูุนูููุงู ุจุง ูู ูุฑุชุจุท ูุณุชูุฏ)ุ ุงูุฒูุฏู ุขูโูุง ููฺฉู ุงุณุช ุชุฃุซุฑ ฺูุฏุงู ูุฏุงุดุชู ุจุงุดุฏ.\n",
    "<br>\n",
    "ฒ. <b>Overfitting:</b> ุฏุฑ ุตูุฑุช ฺฉู ุชุนุฏุงุฏ ูููููโูุง ฺฉู ุจุงุดุฏุ ุงูุฒูุฏู ูฺฺฏโูุง ุฒุงุฏ ูโุชูุงูุฏ ุจุงุนุซ ุจุดโุจุฑุงุฒุด ุดูุฏ.\n",
    "<br>\n",
    "ณ. <b>ููุฒ:</b> ุงฺฏุฑ ุจุฑุฎ ูฺฺฏโูุง ุงุทูุงุนุงุช ุจโุฑุจุท ุง ููุฒ ุจุดุชุฑ ุฏุงุดุชู ุจุงุดูุฏุ ููฺฉู ุงุณุช ุนููฺฉุฑุฏ ุฑุง ฺฉุงูุด ุฏููุฏ.\n",
    "<br><br>\n",
    "<b>ูุชุฌูโฺฏุฑ:</b><br>\n",
    "ุฏุฑ ุงู ูุณุฆููุ ุงูุชุธุงุฑ ูโุฑูุฏ ฺฉู ุชุฑฺฉุจ น ูฺฺฏ ุนููฺฉุฑุฏ ุจูุชุฑ ูุณุจุช ุจู ุงุณุชูุงุฏู ุงุฒ ณ ูฺฺฏ ุชููุง ุฏุงุดุชู ุจุงุดุฏุ ุจู ุดุฑุท ฺฉู ุฏุงุฏู ฺฉุงู ุฏุงุดุชู ุจุงุดู ู ูฺฺฏโูุง ุงุทูุงุนุงุช ูฺฉูู ุงุฑุงุฆู ุฏููุฏ. Logistic Regression ูุนูููุงู ุจูุชุฑ ุงุฒ Naive Bayes ูโุชูุงูุฏ ุจุง ูฺฺฏโูุง ูุงุจุณุชู ฺฉูุงุฑ ุจุงุฏ.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tldextract\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression as SkLogReg\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "if 'urls_df' not in globals():\n",
    "    def load_url_dataset():\n",
    "        candidates = ['urls.csv', 'phishing.csv', 'phishing_urls.csv', 'url_dataset.csv']\n",
    "        df = None\n",
    "        for path in candidates:\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                print(f\"Loaded dataset: {path} | shape={df.shape}\")\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if df is None:\n",
    "            df = pd.DataFrame({\n",
    "                'url': [\n",
    "                    'http://example.com',\n",
    "                    'https://secure.paypal.com/login',\n",
    "                    'http://192.168.1.10/verify',\n",
    "                    'https://account-update-secure-login.com/pay',\n",
    "                    'http://my-bank-example.com/login?session=1234'\n",
    "                ],\n",
    "                'status': [0, 0, 1, 1, 1]\n",
    "            })\n",
    "            print(\"Using fallback URL dataset (demo mode).\")\n",
    "        if 'label' in df.columns and 'status' not in df.columns:\n",
    "            df = df.rename(columns={'label': 'status'})\n",
    "        if df['status'].dtype == object:\n",
    "            mapping = {\n",
    "                'phishing': 1, 'bad': 1, 'malicious': 1, '1': 1,\n",
    "                'legit': 0, 'benign': 0, 'good': 0, '0': 0\n",
    "            }\n",
    "            df['status'] = df['status'].astype(str).str.lower().map(mapping).fillna(0).astype(int)\n",
    "        return df[['url', 'status']]\n",
    "    urls_df = load_url_dataset()\n",
    "\n",
    "\n",
    "def structural_features(url: str):\n",
    "    p = urlparse(url)\n",
    "    host = p.netloc\n",
    "    path = p.path or ''\n",
    "    query = p.query or ''\n",
    "    host_len = len(host)\n",
    "    path_len = len(path)\n",
    "    url_len = len(url)\n",
    "    num_params = (query.count('&') + 1) if query else 0\n",
    "    has_fragment = int('#' in url)\n",
    "    digits_in_host = sum(ch.isdigit() for ch in host)\n",
    "    return {\n",
    "        'host_len': host_len,\n",
    "        'path_len': path_len,\n",
    "        'url_len': url_len,\n",
    "        'num_params': num_params,\n",
    "        'has_fragment': has_fragment,\n",
    "        'digits_in_host': digits_in_host\n",
    "    }\n",
    "\n",
    "\n",
    "def statistical_features(url: str):\n",
    "    length_url = len(url)\n",
    "    digits = sum(ch.isdigit() for ch in url)\n",
    "    ratio_digits_url = (digits / length_url) if length_url > 0 else 0.0\n",
    "    tokens = re.split(r'[^A-Za-z]+', url)\n",
    "    longest_words_raw = max((len(t) for t in tokens), default=0)\n",
    "    return {\n",
    "        'length_url': length_url,\n",
    "        'ratio_digits_url': ratio_digits_url,\n",
    "        'longest_words_raw': longest_words_raw,\n",
    "    }\n",
    "\n",
    "\n",
    "def creative_features(url: str):\n",
    "    parsed = urlparse(url)\n",
    "    host = parsed.netloc\n",
    "    path = parsed.path or ''\n",
    "    ext = tldextract.extract(host)\n",
    "    sub_len = len(ext.subdomain) if ext.subdomain else 0\n",
    "    has_ip = bool(re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', host))\n",
    "    at_sign = '@' in url\n",
    "    dash_in_host = '-' in host\n",
    "    dot_count_host = host.count('.')\n",
    "    slash_count_path = path.count('/')\n",
    "    percent_encoding = '%' in url\n",
    "    suspicious_words = sum(w in url.lower() for w in ['verify', 'update', 'login', 'secure', 'account'])\n",
    "    has_https = parsed.scheme.lower() == 'https'\n",
    "    return {\n",
    "        'sub_len': sub_len,\n",
    "        'has_ip': int(has_ip),\n",
    "        'at_sign': int(at_sign),\n",
    "        'dash_in_host': int(dash_in_host),\n",
    "        'dot_count_host': dot_count_host,\n",
    "        'slash_count_path': slash_count_path,\n",
    "        'percent_encoding': int(percent_encoding),\n",
    "        'suspicious_words': suspicious_words,\n",
    "        'has_https': int(has_https)\n",
    "    }\n",
    "\n",
    "struct_df = urls_df['url'].apply(structural_features).apply(pd.Series)\n",
    "stat_df = urls_df['url'].apply(statistical_features).apply(pd.Series)\n",
    "cre_df = urls_df['url'].apply(creative_features).apply(pd.Series)\n",
    "\n",
    "all_df = pd.concat([struct_df, stat_df, cre_df], axis=1)\n",
    "all_df['status'] = urls_df['status'].values\n",
    "\n",
    "X = all_df.drop('status', axis=1).values\n",
    "y = all_df['status'].values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Combined features sample:\")\n",
    "print(all_df.head())\n",
    "\n",
    "lr_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "    ('clf', SkLogReg(max_iter=1000))\n",
    "])\n",
    "lr_pipe.fit(X_tr, y_tr)\n",
    "y_pred_lr_all = lr_pipe.predict(X_te)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_tr, y_tr)\n",
    "y_pred_nb_all = nb.predict(X_te)\n",
    "\n",
    "res_all = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Logistic Regression': [\n",
    "        accuracy(y_te, y_pred_lr_all),\n",
    "        precision(y_te, y_pred_lr_all),\n",
    "        recall(y_te, y_pred_lr_all),\n",
    "        f1_score(y_te, y_pred_lr_all)\n",
    "    ],\n",
    "    'Naive Bayes (Gaussian)': [\n",
    "        accuracy(y_te, y_pred_nb_all),\n",
    "        precision(y_te, y_pred_nb_all),\n",
    "        recall(y_te, y_pred_nb_all),\n",
    "        f1_score(y_te, y_pred_nb_all)\n",
    "    ]\n",
    "})\n",
    "print(\"\\nResults (All Features Combined):\")\n",
    "print(res_all.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุจุฎุด ูพูุฌู: ุฌูุนโุจูุฏ ููุง</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ูุชุฌูโฺฏุฑ ููุง ุฎูุฏุชุงู ุงุฒ ุงู ุณูุงู ุฑุง ุงุฑุงุฆู ุฏูุฏ. ุจุฑุง ูุซุงู ุจุงุฏ \"ุญุฏุงูู\" ุจู ุณูุงูโูุง ุฒุฑ ูพุงุณุฎ ุฏูุฏ:\n",
    "<br>\n",
    "- ฺฉุฏุงู ูุฏู ุจุฑุง ุงู ููุน ุฏุงุฏู ุจูุชุฑ ุนูู ฺฉุฑุฏู ุงุณุชุ ฺุฑุงุ\n",
    "<br>\n",
    "- ฺฉุฏุงู ููุน ูฺฺฏ ุจุดุชุฑู ุชุฃุซุฑ ุฑุง ุฏุงุดุชู ุงุณุชุ ฺฉุฏุงู ููุน ฺฉูุชุฑูุ ฺุฑุงุ\n",
    "<br>\n",
    "- ูุฑูุงูโุณุงุฒ ูุงุฒ ุจูุฏู ุงุณุชุ ุงฺฏุฑ ุจููุ ุจุฑุง ูุฑ ุฏู ูุฏูุ ฺุฑุงุ\n",
    "<br>\n",
    "- ุบุฑ ุงุฒ ุงุณุชุฎุฑุงุญ ูฺฺฏ ุงุฒ ุฎูุฏ ุขุฏุฑุณ ุงูุชุฑูุชุ ฺู ุฑูฺฉุฑุฏ ุฏฺฏุฑ ูโุชูุงู ุงุชุฎุงุฐ ูููุฏุ\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุจุฎุด ูพูุฌู:</b><br>\n",
    "<b>ฑ. ฺฉุฏุงู ูุฏู ุจูุชุฑ ุนูู ฺฉุฑุฏู ุงุณุชุ</b><br>\n",
    "Logistic Regression ุฏุฑ ุชูุงู ุจุฎุดโูุง ุนููฺฉุฑุฏ ุจูุชุฑ ูุณุจุช ุจู Naive Bayes ุฏุงุดุชู ุงุณุช. ุงู ุจุฑุชุฑ ุจู ุฏูุงู ุฒุฑ ุงุณุช:\n",
    "<br>\n",
    "- Logistic Regression ูุงุฏุฑ ุงุณุช ูุงุจุณุชฺฏโูุง ุจู ูฺฺฏโูุง ุฑุง ูุฏูโุณุงุฒ ฺฉูุฏุ ุฏุฑ ุญุงู ฺฉู Naive Bayes ูุฑุถ ุงุณุชููุงู ุฏุงุฑุฏ ฺฉู ุฏุฑ ุนูู ููุดู ุจุฑูุฑุงุฑ ูุณุช.\n",
    "<br>\n",
    "- Logistic Regression ุจุง ุงุณุชูุงุฏู ุงุฒ gradient descentุ ูุฒูโูุง ุจูููโุชุฑ ุงุฏ ูโฺฏุฑุฏ ฺฉู ููุฌุฑ ุจู ุชุตููโฺฏุฑ ุฏููโุชุฑ ูโุดูุฏ.\n",
    "<br>\n",
    "- ุงุณุชูุงุฏู ุงุฒ ูุฑูุงูโุณุงุฒ ุฏุฑ Logistic Regression ุจุงุนุซ ุจูุจูุฏ ููฺฏุฑุง ู ุงุฏฺฏุฑ ุจูุชุฑ ูโุดูุฏ.\n",
    "<br><br>\n",
    "<b>ฒ. ฺฉุฏุงู ููุน ูฺฺฏ ุจุดุชุฑู ู ฺฉูุชุฑู ุชุฃุซุฑ ุฑุง ุฏุงุดุชุ</b><br>\n",
    "<b>ุจุดุชุฑู ุชุฃุซุฑ:</b> ูฺฺฏโูุง ุฎูุงูุงูู (ูุงููุฏ ูุฌูุฏ IPุ ุชุนุฏุงุฏ ุฒุฑุฏุงูููโูุง) ุงุญุชูุงูุงู ุจุดุชุฑู ุชุฃุซุฑ ุฑุง ุฏุงุฑูุฏุ ุฒุฑุง ูุณุชููุงู ุจู ูุดุงููโูุง ุฑุงุฌ ูุดูฺฏ ุงุดุงุฑู ูโฺฉููุฏ. ููฺูู ูฺฺฏโูุง ุขูุงุฑ ูุงููุฏ ุทูู URL ู ูุณุจุช ุงุฑูุงู ูุฒ ูุคุซุฑ ูุณุชูุฏ.\n",
    "<br><br>\n",
    "<b>ฺฉูุชุฑู ุชุฃุซุฑ:</b> ุจุฑุฎ ูฺฺฏโูุง ุณุงุฎุชุงุฑ ูุงููุฏ ุชุนุฏุงุฏ ุฎุท ุชุฑูโูุง ููฺฉู ุงุณุช ุชุฃุซุฑ ฺฉูุชุฑ ุฏุงุดุชู ุจุงุดูุฏุ ุฒุฑุง ูู ุฏุฑ ุณุงุชโูุง ูุนุชุจุฑ ู ูู ุฏุฑ ููฺฉโูุง ูุดูฺฏ ุฏุฏู ูโุดููุฏ.\n",
    "<br><br>\n",
    "<b>ณ. ุขุง ูุฑูุงูโุณุงุฒ ูุงุฒู ุจูุฏุ</b><br>\n",
    "ุจููุ ุจุฑุง Logistic Regression ูุฑูุงูโุณุงุฒ ุถุฑูุฑ ุงุณุช:\n",
    "<br>\n",
    "- Logistic Regression ุจู ููุงุณ ูฺฺฏโูุง ุญุณุงุณ ุงุณุช ู ูุฑูุงูโุณุงุฒ ุจุงุนุซ ููฺฏุฑุง ุณุฑุนโุชุฑ ู ุงุฏฺฏุฑ ุจูุชุฑ ูโุดูุฏ.\n",
    "<br>\n",
    "- ุจุฑุง Naive Bayes (ุจู ุฎุตูุต GaussianNB) ูุฑูุงูโุณุงุฒ ุงูุฒุงู ูุณุชุ ุงูุง ูโุชูุงูุฏ ุฏุฑ ุจุฑุฎ ููุงุฑุฏ ููุฏ ุจุงุดุฏ.\n",
    "<br><br>\n",
    "<b>ด. ุฑูฺฉุฑุฏูุง ุฏฺฏุฑ ุบุฑ ุงุฒ ุงุณุชุฎุฑุงุฌ ูฺฺฏ ุงุฒ URL:</b><br>\n",
    "- <b>ุชุญูู ูุญุชูุง ุตูุญู:</b> ุจุฑุฑุณ HTMLุ ูุชูุ ูุฑูโูุง ู ุงุณฺฉุฑูพุชโูุง ุฏุงุฎู ุตูุญู ุจุฑุง ุดูุงุณุง ูุดุงููโูุง ูุดูฺฏ (ูุงููุฏ ุฏุฑุฎูุงุณุช ุงุทูุงุนุงุช ุญุณุงุณ).\n",
    "<br>\n",
    "- <b>ุจุฑุฑุณ ุงุทูุงุนุงุช WHOIS:</b> ุชุงุฑุฎ ุซุจุช ุฏุงูููุ ุณู ุฏุงูููุ ู ุตุงุญุจ ุฏุงููู - ุณุงุชโูุง ูุดูฺฏ ูุนูููุงู ุฏุงูููโูุง ุชุงุฒู ุซุจุชโุดุฏู ุฏุงุฑูุฏ.\n",
    "<br>\n",
    "- <b>ุชุญูู ฺฏูุงู SSL:</b> ุจุฑุฑุณ ุงุนุชุจุงุฑ ฺฏูุงู HTTPSุ ููุน ฺฏูุงูุ ู ูุฑุฌุน ุตุงุฏุฑฺฉููุฏู.\n",
    "<br>\n",
    "- <b>ุงุณุชูุงุฏู ุงุฒ ูุณุช ุณุงู (Blacklist):</b> ููุงุณู URL ุจุง ูพุงฺฏุงูโุฏุงุฏูโูุง ุดูุงุฎุชูโุดุฏู ููฺฉโูุง ูุดูฺฏ.\n",
    "<br>\n",
    "- <b>ุชุญูู ุฑูุชุงุฑ ฺฉุงุฑุจุฑ:</b> ุจุฑุฑุณ ุงูฺฏููุง ฺฉูฺฉุ ุฒูุงู ูุงูุฏู ุฏุฑ ุตูุญูุ ู ุชุนุงููุงุช ฺฉุงุฑุจุฑ.\n",
    "<br>\n",
    "- <b>ุงุฏฺฏุฑ ุนูู:</b> ุงุณุชูุงุฏู ุงุฒ ุดุจฺฉูโูุง ุนุตุจ ู ูุฏูโูุง ูพฺุฏูโุชุฑ ุจุฑุง ุงุฏฺฏุฑ ุฎูุฏฺฉุงุฑ ูฺฺฏโูุง ุจุฏูู ูุงุฒ ุจู ุงุณุชุฎุฑุงุฌ ุฏุณุช.\n",
    "<br>\n",
    "- <b>ุชุญูู ุดุจุงูุช ุจุตุฑ:</b> ููุงุณู ุธุงูุฑ ุตูุญู ุจุง ุณุงุชโูุง ูุนุชุจุฑ ุจุฑุง ุชุดุฎุต ุฌุนู ููุช.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_title"
   },
   "source": [
    "# <h1 style=\"text-align: right;\">**ูฺฉุงุช ููู ู ููุงูู ุชุญูู**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_body"
   },
   "source": [
    "\n",
    "\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ูุงู ุงุฑุณุงู ุดูุง ุจุงุฏ ุจุง ูุฑูุช ุฒุฑ ูุงูฺฏุฐุงุฑ ุดูุฏ: <code>NLP_CA{n}_{LASTNAME}_{STUDENTID}.ipynb</code></h4>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ูุญูู ุงูุฌุงู ุชูุฑู:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "  <li>ุณูููโูุง ฺฉุฏ ุจุง ุจุฑฺุณุจ <code>WRITE YOUR CODE HERE</code> ุฑุง ุชฺฉูู ฺฉูุฏ.</li>\n",
    "  <li>ุจุฑุง ูพุงุณุฎโูุง ูุชูุ ูุชู <code>{{ูพุงุณุฎ_ุฎูุฏ_ุฑุง_ุงูุฌุง_ุจููุณุฏ}}</code> ุฑุง ุจุง ูพุงุณุฎ ุฎูุฏ ุฌุงฺฏุฒู ฺฉูุฏ.</li>\n",
    "</ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ุตุฏุงูุช ุนูู:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>ูุง ููุชโุจูฺฉโูุง ุชุนุฏุงุฏ ูุดุฎุต ุงุฒ ุฏุงูุดุฌูุงู ฺฉู ุจู ุตูุฑุช ุชุตุงุฏู ุงูุชุฎุงุจ ูโุดููุฏุ ุจุฑุฑุณ ุฎูุงูู ฺฉุฑุฏ. ุงู ุจุฑุฑุณโูุง ุงุทููุงู ุญุงุตู ูโฺฉููุฏ ฺฉู ฺฉุฏ ฺฉู ููุดุชุฏ ูุงูุนุงู ูพุงุณุฎโูุง ููุฌูุฏ ุฏุฑ ููุชโุจูฺฉ ุดูุง ุฑุง ุชููุฏ ูโฺฉูุฏ. ุงฺฏุฑ ูพุงุณุฎโูุง ุตุญุญ ุฑุง ุฏุฑ ููุชโุจูฺฉ ุฎูุฏ ุจุฏูู ฺฉุฏ ฺฉู ูุงูุนุงู ุขู ูพุงุณุฎโูุง ุฑุง ุชููุฏ ฺฉูุฏ ุชุญูู ุฏูุฏุ ุงู ฺฉ ููุฑุฏ ุฌุฏ ุงุฒ ุนุฏู ุตุฏุงูุช ุนูู ูุญุณูุจ ูโุดูุฏ.</li> <li>ูุง ููฺูู ุจุฑุฑุณโูุง ุฎูุฏฺฉุงุฑ ุฑุง ุจุฑุง ุชุดุฎุต ุณุฑูุช ุนูู ุฏุฑ ููุชโุจูฺฉโูุง ฺฉููุจ ุงูุฌุงู ุฎูุงูู ุฏุงุฏ. ฺฉูพ ฺฉุฑุฏู ฺฉุฏ ุงุฒ ุฏฺฏุฑุงู ูุฒ ฺฉ ููุฑุฏ ุฌุฏ ุงุฒ ุนุฏู ุตุฏุงูุช ุนูู ูุญุณูุจ ูโุดูุฏ.</li> </ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ุชูุถุญุงุช ุชฺฉูู:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "ุฎูุงูุง ู ุฏูุช ุจุฑุฑุณโูุง ุฏุฑ ฺฏุฒุงุฑุด ููุง ุงุฒ ุงููุช ูฺูโุง ุจุฑุฎูุฑุฏุงุฑ ุงุณุช. ุจู ุชูุฑูโูุง ฺฉู ุจู ุตูุฑุช ฺฉุงุบุฐ ุชุญูู ุฏุงุฏู ุดููุฏ ุง ุจู ุตูุฑุช ุนฺฉุณ ุฏุฑ ุณุงุช ุจุงุฑฺฏุฐุงุฑ ุดููุฏุ ุชุฑุชุจ ุงุซุฑ ุฏุงุฏู ูุฎูุงูุฏ ุดุฏ.</li>\n",
    "<li>\n",
    " ูููโ ฺฉุฏูุง ูพูุณุช ฺฏุฒุงุฑุด ุจุงุณุช ูุงุจูุช ุงุฌุฑุง ูุฌุฏุฏ ุฏุงุดุชู ุจุงุดูุฏ. ุฏุฑ ุตูุฑุช ฺฉู ุจุฑุง ุงุฌุฑุง ูุฌุฏุฏ ุขูโูุง ูุงุฒ ุจู ุชูุธูุงุช ุฎุงุต ูโุจุงุดุฏุ ุจุงุณุช ุชูุธูุงุช ููุฑุฏ ูุงุฒ ุฑุง ูุฒ ุฏุฑ ฺฏุฒุงุฑุด ุฎูุฏ ุฐฺฉุฑ ฺฉูุฏ.  ุฏูุช ฺฉูุฏ ฺฉู  ุชูุงู ฺฉุฏูุง ุจุงุฏ ุชูุณุท ุดูุง ุงุฌุฑุง ุดุฏู ุจุงุดูุฏ ู ูุชุงุฌ ุงุฌุฑุง ุฏุฑ ูุงู ฺฉุฏูุง ุงุฑุณุงู ูุดุฎุต ุจุงุดุฏ. ุจู ฺฉุฏูุง ฺฉู ูุชุงุฌ ุงุฌุฑุง ุขูโูุง ุฏุฑ ูุงู ุงุฑุณุงู ูุดุฎุต ูุจุงุดุฏ ููุฑูโุง ุชุนูู ููโฺฏุฑุฏ.\n",
    "</li>\n",
    "<li>ุชูุฌู ฺฉูุฏ ุงู ุชูุฑู ุจุงุฏ ุจู ุตูุฑุช ุชฺฉโููุฑู ุงูุฌุงู ุดูุฏ ู ูพุงุณุฎโูุง ุงุฑุงุฆู ุดุฏู ุจุงุฏ ูุชุฌู ูุนุงูุช ูุฑุฏ ููุณูุฏู ุจุงุดุฏ (ูููฺฉุฑ ู ุจู ุงุชูุงู ูู ููุดุชู ุชูุฑู ูุฒ ููููุน ุงุณุช). ุฏุฑ ุตูุฑุช ูุดุงูุฏู\n",
    " ุชุดุงุจู ุจู ููู ุงูุฑุงุฏ ูุดุงุฑฺฉุชโฺฉููุฏูุ ููุฑู ุชูุฑู ุตูุฑ ู ุจู ุงุณุชุงุฏ ฺฏุฒุงุฑุด ูโฺฏุฑุฏุฏ.\n",
    " </li>\n",
    "\n",
    " <li>\n",
    "ูุทูุงู ุชูุงู ูพุงุณุฎโูุง ูุชู ุฎูุฏ ุฑุง ุจุง <b>ูููุช ูุฒุฑ (Vazir)</b> ู ุจูโุตูุฑุช <b>ุฑุงุณุชโฺู</b> ุจููุณุฏ.  \n",
    "ุงุฒ ุงุณุชูุงุฏู ุงุฒ ูููุชโูุง ูพุดโูุฑุถ ุฎูุฏุฏุงุฑ ฺฉูุฏ ุชุง ุธุงูุฑ ููุชโุจูฺฉ ุดูุง ฺฉโุฏุณุช ู ุฎูุงูุง ุจุงุดุฏ.  \n",
    "ุฏุฑ ุจุฎุดโูุง ุชุดุฑุญุ ุณุน ฺฉูุฏ ูพุงุณุฎโูุง ุฑุง ฺฉุงููุ ููุณุฌู ู ุจุง ุฑุนุงุช ูฺฏุงุฑุด ูุงุฑุณ ุจููุณุฏ.  \n",
    "ููฺููุ ุจู ฺูุด ุชูุฒ ุณูููโูุง ู ุงุฌุฑุง ุฏุฑุณุช ฺฉุฏูุง ุชูุฌู ฺฉูุฏ ุชุง ุชูุฑู ุดูุง ุจุง ูุฑูุช ุฎูุงุณุชูโุดุฏู ู ุงุณุชุงูุฏุงุฑุฏ ุงุฑุงุฆู ุดูุฏ.\n",
    "</li>\n",
    " <li>ุจุฑุง ูุทุงูุนู ุจุดุชุฑ ุฏุฑุจุงุฑูโ ูุฑูุช Markdown ูโุชูุงูุฏ ุงุฒ <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">ุงู ููฺฉ</a> ูุทุงูุนู ฺฉูุฏ.\n",
    " </li>\n",
    " </ul>\n",
    "    \n",
    "\n",
    " </div>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "section2_title",
    "section3_title",
    "section4_title",
    "eval_title",
    "policies_title"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
