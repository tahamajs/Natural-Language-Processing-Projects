{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cover_header",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"text-align: center; padding: 20px; font-family: Vazir;\">\n",
    "<h1 align=\"center\" style=\"font-size: 28px; color:rgb(64, 244, 202); width: 100%;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>ØªÙ…Ø±ÛŒÙ† Û²<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1>\n",
    "<h2 style=\"color:rgb(90, 255, 184); font-size: 20px;\">Logistic Regression and Naive Bayes</h2>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">ÙØ±Ù‡Ø§Ø¯ Ù†ØµØ±ÛŒ - Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø²Ù…Ø§Ù†ÛŒ</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">farhadnasri999@gmail.com - shigzv@gmail.com</p>\n",
    "\n",
    "<div dir=\"rtl\" style=\"border: 2px dashed rgb(90, 255, 184); border-radius: 8px; padding: 20px; margin: 20px auto; max-width: 500px; text-align: right;\">\n",
    "<p style=\"color: rgb(64, 244, 202); font-size: 18px; margin-bottom: 15px;\">ğŸ“ Ù…Ø´Ø®ØµØ§Øª Ø¯Ø§Ù†Ø´Ø¬Ùˆ:</p>\n",
    "<p style=\"color: #666; margin: 5px;\">Ù†Ø§Ù… Ùˆ Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ: Ø·Ø§Ù‡Ø§ Ù…Ø¬Ù„Ø³ÛŒ</p>\n",
    "<p style=\"color: #666; margin: 5px;\">Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒÛŒ: 810101504</p>\n",
    "<p style=\"color: #666; margin: 5px;\">ØªØ§Ø±ÛŒØ® Ø§Ø±Ø³Ø§Ù„: ----</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; padding: 25px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<div style=\"padding-right:100px\">\n",
    "ğŸ“‹ <b>Ø³Ø§Ø®ØªØ§Ø± ØªÙ…Ø±ÛŒÙ†:</b>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ - <span dir=\"ltr\">Logistic Regression and Naive Bayes (from scratch)</span> (60)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Bag of Words</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Logistic Regression</li>\n",
    "<li>Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Naive Bayes</li>\n",
    "<li>Ø¨Ø®Ø´ Ø´Ø´Ù…: ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬</li>\n",
    "</ul>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… - <span dir=\"ltr\">Logistic Regression and Naive Bayes (e.g. \n",
    "sklearn)</span> (40)</b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒÛŒ</li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ù„Ø®ÙˆØ§Ù‡</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "<div dir='rtl' style=\"line-height: 1.8; font-family: Vazir; font-size: 16px; margin-top: 20px; background-color: #e8eaf6; padding: 15px; border-radius: 8px; color:black\">\n",
    "ğŸ’¡ <b>Ù†Ú©Ø§Øª Ù…Ù‡Ù…:</b>\n",
    "<br>\n",
    "Ø¯Ø± Ø³ÙˆØ§Ù„ Ø¯Ùˆ Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ Ù‡Ø³ØªÛŒØ¯ ÙˆÙ„ÛŒ Ø¯Ø± Ø³ÙˆØ§Ù„ ÛŒÚ©ØŒ Ù‡Ù…Ø§Ù†â€ŒØ·ÙˆØ± Ú©Ù‡ Ø¯Ø± Ù…ØªÙ† Ø³ÙˆØ§Ù„ Ù‡Ù… Ø°Ú©Ø± Ø´Ø¯Ù‡ØŒ Ø¨Ø§ÛŒØ¯ Ø§Ø² Ù¾Ø§ÛŒÙ‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_title"
   },
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Logistic Regression Ùˆ Naive Bayes <br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø§ÛŒÙ† Ø¯Ùˆ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙˆÙ† Ø±Ø§ Ø§Ø² ØµÙØ± (Ø¨Ø¯ÙˆÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡) Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯. Ù‡Ù…â€ŒÚ†Ù†ÛŒÙ† Ù†ÛŒØ§Ø² Ø§Ø³ØªØŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ù†ÛŒØ² Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Bag of Words</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ú¯Ø§Ù… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ØŒ Ù‡Ø¯Ù Ù…Ø§ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ† Ø§Ø³Øª. Ø§Ø² Ø¢Ù†â€ŒØ¬Ø§ Ú©Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ø¨Ø§ÛŒØ¯ Ù…ØªÙ† Ø®Ø§Ù… Ø±Ø§ Ø¨Ù‡ Ø´Ú©Ù„ÛŒ Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ… ØªØ§ Ø¨ØªÙˆØ§Ù†Ù†Ø¯ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨ÛŒØ§Ù…ÙˆØ²Ù†Ø¯. Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ Ù‚ØµØ¯ Ø¯Ø§Ø±ÛŒÙ… Ø¨Ø§ ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙˆÙ† Ø¨Ù‡ Ù†Ù…Ø§ÛŒØ´ Bag of WordsØŒ Ù‡Ø± Ø§ÛŒÙ…ÛŒÙ„ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø± ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒØ´ Ø¨Ù‡ ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ….\n",
    "<br>\n",
    "Ø¯ÛŒØªØ§Ø³Øª emails_1.csv Ø´Ø§Ù…Ù„ Ø¯Ùˆ Ø³ØªÙˆÙ† Ø§Ø³Øª:\n",
    "<br>\n",
    "text (Ù…ØªÙ† Ø§ÛŒÙ…ÛŒÙ„)\n",
    "<br>\n",
    "status (Ø¨Ø±Ú†Ø³Ø¨ØŒ Ù…Ø´Ø®Øµâ€ŒÚ©Ù†Ù†Ø¯Ù‡â€ŒÛŒ spam ÛŒØ§ ham Ø¨ÙˆØ¯Ù† Ø§ÛŒÙ…ÛŒÙ„)\n",
    "<br>\n",
    "Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ù…Ø±Ø§Ø­Ù„ Ø²ÛŒØ± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯:\n",
    "<br>\n",
    "Û±. Ø³ØªÙˆÙ† status Ø±Ø§ Ø¨Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒØ¯ (spam â†’ 1 Ùˆ ham â†’ 0).\n",
    "<br>\n",
    "Û². Ù…ØªÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ù¾Ø§Ú©â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯: ØªÙ…Ø§Ù… Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ± Ø§Ù„ÙØ¨Ø§ÛŒÛŒ (Ø§Ø¹Ø¯Ø§Ø¯ØŒ Ø¹Ù„Ø§Ø¦Ù… Ùˆ ØºÛŒØ±Ù‡) Ø±Ø§ Ø­Ø°Ù Ùˆ Ù‡Ù…Ù‡â€ŒÛŒ Ø­Ø±ÙˆÙ Ø±Ø§ Ú©ÙˆÚ†Ú© (lowercase) Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Û³. Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ Bag of WordsØŒ ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ÙÙ‚Ø· Û±Ûµ Ú©Ù„Ù…Ù‡â€ŒÛŒ Ù¾Ø±ØªÚ©Ø±Ø§Ø± Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡ Ù†Ú©ØªÙ‡: Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² CountVectorizer Ø¯Ø± Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÛŒ sklearn Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "ÛŒÚ© Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¬Ø¯ÛŒØ¯ Ú©Ù‡ Ø´Ø§Ù…Ù„ Û±Ûµ ÙˆÛŒÚ˜Ú¯ÛŒ + Ø³ØªÙˆÙ† status Ø§Ø³Øª. (ØªÙ…Ø§Ù… Û±Û° Ø±Ø¯ÛŒÙ Ø§ÛŒÙ† Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯.)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv('emails_1.csv')\n",
    "except FileNotFoundError:\n",
    "    df = pd.DataFrame({\n",
    "        'text': [\n",
    "            'Hello Friend! Win money now!!!',\n",
    "            'Reminder: meeting tomorrow at 9am',\n",
    "            'Limited offer, claim your prize today',\n",
    "            'Project update attached, please review'\n",
    "        ],\n",
    "        'status': ['spam', 'ham', 'spam', 'ham']\n",
    "    })\n",
    "\n",
    "df['status'] = df['status'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', str(text))\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text.lower()\n",
    "\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=15)\n",
    "X_bow = vectorizer.fit_transform(df['text_clean'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "df_bow = pd.DataFrame(X_bow.toarray(), columns=feature_names)\n",
    "df_bow['status'] = df['status'].values\n",
    "\n",
    "print(\"Bag of Words representation with top 15 features:\")\n",
    "print(f\"Shape: {df_bow.shape}\")\n",
    "print(df_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø§Ø² Ø§ÛŒÙ† Ù…Ø±Ø­Ù„Ù‡ Ø¨Ù‡ Ø¨Ø¹Ø¯ØŒ Ø¯ÛŒÚ¯Ø± Ø¨Ø§ ÙØ§ÛŒÙ„ emails_1.csv Ùˆ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø­Ø§ØµÙ„ Ø§Ø² Ø¢Ù† Ú©Ø§Ø±ÛŒ Ù†Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø´Øª. Ø¨Ø±Ø§ÛŒ Ø³Ù‡ÙˆÙ„Øª Ú©Ø§Ø±ØŒ ÙØ§ÛŒÙ„ emails_2.csv Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ø´Ù…Ø§ Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡ Ø§Ø³Øª. Ø§ÛŒÙ† ÙØ§ÛŒÙ„ØŒ Ù†ØªÛŒØ¬Ù‡â€ŒÛŒ Ù‡Ù…Ø§Ù† ÙØ±Ø§ÛŒÙ†Ø¯ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø± Ø±ÙˆÛŒ ÛµÛ±Û·Û² Ø§ÛŒÙ…ÛŒÙ„ ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³Øª. Ø¨Ù‡ Ø¨ÛŒØ§Ù† Ø³Ø§Ø¯Ù‡â€ŒØªØ±ØŒ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ Ù‡Ø± Ø±Ø¯ÛŒÙ Ù†Ù…Ø§ÛŒØ§Ù†Ú¯Ø± ÛŒÚ© Ø§ÛŒÙ…ÛŒÙ„ Ùˆ Ù‡Ø± Ø³ØªÙˆÙ† Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡â€ŒÛŒ ÙØ±Ø§ÙˆØ§Ù†ÛŒ ÛŒÚ©ÛŒ Ø§Ø² Û³Û°Û°Û° Ú©Ù„Ù…Ù‡â€ŒÛŒ Ù¾Ø±ØªÚ©Ø±Ø§Ø± Ø¯Ø± Ú©Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø§Ø³Øª.\n",
    "Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ Ø¨Ø§ÛŒØ¯ Ø±ÙˆÛŒ Ø§ÛŒÙ† Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ØŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯. Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ ÙØ§ÛŒÙ„ emails_2.csv Ø±Ø§ Ø¨Ø®ÙˆØ§Ù†ÛŒØ¯ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù† Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯ ØªØ§ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡ Ø¢Ø´Ù†Ø§ Ø´ÙˆÛŒØ¯.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù¾Ù†Ø¬ Ø±Ø¯ÛŒÙ Ø§ÙˆÙ„ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ù…Ø°Ú©ÙˆØ± Ø±Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    df_emails = pd.read_csv('emails_2.csv')\n",
    "    print(\"Dataset Information:\")\n",
    "    print(f\"Shape: {df_emails.shape}\")\n",
    "    print(f\"Columns (first 10): {df_emails.columns.tolist()[:10]} ...\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df_emails.head())\n",
    "    print(\"\\nStatus distribution:\")\n",
    "    print(df_emails['status'].value_counts())\n",
    "except FileNotFoundError:\n",
    "    print(\"emails_2.csv not found. Please place the file next to this notebook.\")\n",
    "    try:\n",
    "        df_emails = df_bow.copy()\n",
    "        print(\"Using fallback df_bow as df_emails (demo mode). Shape:\", df_emails.shape)\n",
    "        print(df_emails.head())\n",
    "    except NameError:\n",
    "        df_emails = None\n",
    "        print(\"No fallback available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ù†Ø³Ø¨Øª Û¸Û° Ø¯Ø±ØµØ¯ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Û²Û° Ø¯Ø±ØµØ¯ Ø¨Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†ÛŒØ¯. Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "X_train, X_test, y_train, y_test<br>\n",
    "ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ X_train Ùˆ X_test \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "assert 'df_emails' in globals() and df_emails is not None, \"Run the previous cell to load df_emails.\"\n",
    "\n",
    "X = df_emails.drop('status', axis=1).values\n",
    "y = df_emails['status'].values\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(len(X))\n",
    "train_size = int(0.8 * len(X))\n",
    "train_idx = indices[:train_size]\n",
    "test_idx = indices[train_size:]\n",
    "\n",
    "X_train = X[train_idx]\n",
    "X_test = X[test_idx]\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "print(\"Dataset Split:\")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape:  {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù¾ÛŒØ´ Ø§Ø² Ø¢Ù†â€ŒÚ©Ù‡ ÙˆØ§Ø±Ø¯ Ù…Ø±Ø­Ù„Ù‡â€ŒÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´ÙˆÛŒÙ…ØŒ Ù„Ø§Ø²Ù… Ø§Ø³Øª Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ù†Ø¬Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ ØªØ¹Ø±ÛŒÙ Ú©Ù†ÛŒÙ…. Ø§ÛŒÙ† Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ø¨Ù‡ Ù…Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ ØªØ§ Ø¨ÙÙ‡Ù…ÛŒÙ… Ù…Ø¯Ù„ ØªØ§ Ú†Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø¯Ø±Ø³Øª Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ùˆ Ù…Ù†ÙÛŒ Ù…ÙˆÙÙ‚ Ø¹Ù…Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.\n",
    "<br>\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ ØªÙˆØ§Ø¨Ø¹ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª Ø¯ÙˆØ¯ÙˆÛŒÛŒ (binary classification) Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "accuracy(y_true, y_pred) â€” Ù†Ø³Ø¨Øª Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯Ø±Ø³Øª Ø¨Ù‡ Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§\n",
    "<br>\n",
    "precision(y_true, y_pred) â€” Ø¯Ø±ØµØ¯ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ù…Ø«Ø¨Øª Ø¨ÙˆØ¯Ù‡â€ŒØ§Ù†Ø¯\n",
    "<br>\n",
    "recall(y_true, y_pred) â€” Ø¯Ø±ØµØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª ÙˆØ§Ù‚Ø¹ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø±Ø³Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª\n",
    "<br>\n",
    "f1_score(y_true, y_pred) â€” Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‡Ø§Ø±Ù…ÙˆÙ†ÛŒÚ© Ø¨ÛŒÙ† precision Ùˆ recallØŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ ØªÙˆØ§Ø²Ù† Ù…ÛŒØ§Ù† Ø¢Ù† Ø¯Ùˆ\n",
    "<br>\n",
    "Ù‡Ø± ØªØ§Ø¨Ø¹ Ø¨Ø§ÛŒØ¯ Ù…Ù‚Ø¯Ø§Ø± Ø¹Ø¯Ø¯ÛŒ Ù…ØªÙ†Ø§Ø¸Ø± Ø¨Ø§ Ù…Ø¹ÛŒØ§Ø± Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯.\n",
    "<br>\n",
    " Ø¨Ù‡ ØªÙˆØ§Ø¨Ø¹ Ø¨Ø§Ù„Ø§ Ø¯Ùˆ ÙˆØ±ÙˆØ¯ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø¯Ù‡ÛŒØ¯ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯:    y_true = [0, 1, 1, 0, 1] ---- y_pred = [0, 1, 0, 0, 1]\n",
    "<br>\n",
    "ğŸ’¡ Ù†Ú©ØªÙ‡: Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø²  Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÛŒ numpy Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø«Ø§Ù„:\n",
    "<br>\n",
    "y_true = [0, 1, 1, 0, 1]\n",
    "<br>\n",
    "y_pred = [0, 1, 0, 0, 1]\n",
    "<br>\n",
    "print(accuracy(y_true, y_pred))  # 0.80\n",
    "<br>\n",
    "print(precision(y_true, y_pred)) # 1.00\n",
    "<br>\n",
    "print(recall(y_true, y_pred))    # 0.66\n",
    "<br>\n",
    "print(f1_score(y_true, y_pred))  # 0.80\n",
    "<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    return np.sum(y_true == y_pred) / len(y_true)\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    if tp + fp == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "    if tp + fn == 0:\n",
    "        return 0.0\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    prec = precision(y_true, y_pred)\n",
    "    rec = recall(y_true, y_pred)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "\n",
    "y_true = [0, 1, 1, 0, 1]\n",
    "y_pred = [0, 1, 0, 0, 1]\n",
    "\n",
    "print(\"Test Results:\")\n",
    "print(f\"Accuracy:  {accuracy(y_true, y_pred):.2f}\")\n",
    "print(f\"Precision: {precision(y_true, y_pred):.2f}\")\n",
    "print(f\"Recall:    {recall(y_true, y_pred):.2f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_true, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Logistic Regression</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ú©Ù„Ø§Ø³ Logistic Regression Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.Ø³Ù¾Ø³ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ train Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…ØªØ±ÛŒÚ© Ù‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ú¯Ø²Ø§Ø±Ø´ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù‡Ø§ÛŒÙ¾Ø± Ù¾Ø§Ø±Ø§Ù…ØªØ±â€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ù…Ø§Ù†Ù†Ø¯ Ù†Ø±Ø® ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ùˆ ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø± Ø¹Ù‡Ø¯Ù‡ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø³Øª.\n",
    "<br>\n",
    "ğŸ’¡Ù…ÛŒØªÙˆØ§Ù†ÛŒØ¯ Ù‚Ø¨Ù„ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ Ø±Ø§ Ù†Ø±Ù…Ø§Ù„ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø§Ù…Ø§Ø¯Ù‡ Ù†ÛŒØ³ØªÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def to_numeric_array(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        num = X.select_dtypes(include=[np.number])\n",
    "        if num.shape[1] != X.shape[1]:\n",
    "            coerced = X.drop(columns=num.columns).apply(pd.to_numeric, errors='coerce')\n",
    "            num = pd.concat([num, coerced], axis=1)\n",
    "        return np.nan_to_num(num.to_numpy(), nan=0.0)\n",
    "    if isinstance(X, np.ndarray):\n",
    "        if not np.issubdtype(X.dtype, np.number):\n",
    "            try:\n",
    "                return X.astype(float)\n",
    "            except Exception:\n",
    "                return np.nan_to_num(pd.DataFrame(X).apply(pd.to_numeric, errors='coerce').to_numpy(), nan=0.0)\n",
    "        return X\n",
    "    return to_numeric_array(np.array(X))\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.1, n_iterations=500):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    def sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0.0\n",
    "        for i in range(self.n_iterations):\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = self.sigmoid(linear_model)\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            if (i + 1) % 100 == 0:\n",
    "                loss = -np.mean(y * np.log(y_pred + 1e-15) + (1 - y) * np.log(1 - y_pred + 1e-15))\n",
    "                print(f\"Iteration {i+1}/{self.n_iterations}, Loss: {loss:.4f}\")\n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.weights) + self.bias)\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X) >= 0.5).astype(int)\n",
    "\n",
    "X_train_arr = to_numeric_array(X_train)\n",
    "X_test_arr = to_numeric_array(X_test)\n",
    "X_mean = X_train_arr.mean(axis=0)\n",
    "X_std = X_train_arr.std(axis=0) + 1e-8\n",
    "X_train_norm = (X_train_arr - X_mean) / X_std\n",
    "X_test_norm  = (X_test_arr  - X_mean) / X_std\n",
    "\n",
    "print(\"Training Logistic Regression...\\n\")\n",
    "lr_model = LogisticRegression(learning_rate=0.1, n_iterations=500)\n",
    "lr_model.fit(X_train_norm, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test_norm)\n",
    "\n",
    "print(\"\\nLogistic Regression Results on Test Set:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Precision: {precision(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Recall:    {recall(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred_lr):.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Naive Bayes</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ú©Ù„Ø§Ø³ Multinomial Naive Bayes Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ú©Ø§Ù…Ù„ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯. Ø³Ù¾Ø³ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ train Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø± Ù‡Ø§ÛŒ ØªØ¹Ø±ÛŒÙ Ø´Ø¯Ù‡ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ú¯Ø²Ø§Ø±Ø´ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "- Ú†Ø±Ø§ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ù…Ø§Ù†Ù†Ø¯ Gaussian Naive Bayes Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ø±Ø¯ÛŒÙ…ØŸ Ø¢ÛŒØ§ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Naive Bayes Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ØŸ\n",
    "<br>\n",
    "ğŸ’¡Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø§Ù…Ø§Ø¯Ù‡ Ù†ÛŒØ³ØªÛŒØ¯.\n",
    "<br>\n",
    "ğŸ’¡Ø¨Ø±Ø§ÛŒ Ø¢Ø´Ù†Ø§ÛŒÛŒ Ø¨Ø§ Naive Bayes Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¨Ù‡ Appendix Ú©ØªØ§Ø¨ jurafsky Ù…Ø±Ø§Ø¬Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_answer_1"
   },
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…:</b><br>\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø§Ø² Ù…Ø¯Ù„ Multinomial Naive Bayes Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ÛŒÙ…. Ø¯Ù„ÛŒÙ„ Ø§ÛŒÙ† Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø§Ù‡ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ø³Øª. Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ØŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø§ ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ…ÛŒÙ„ Ù‡Ø³ØªÙ†Ø¯ - ÛŒØ¹Ù†ÛŒ Ø¹Ø¯Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯ Ù‡Ø± Ú©Ù„Ù…Ù‡ Ú†Ù†Ø¯ Ø¨Ø§Ø± Ø¯Ø± Ø§ÛŒÙ…ÛŒÙ„ ØªÚ©Ø±Ø§Ø± Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø§ÛŒÙ† Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø´Ù…Ø§Ø±Ø´ÛŒ (count-based) Ùˆ ØºÛŒØ± Ù…Ù†ÙÛŒ Ù‡Ø³ØªÙ†Ø¯.\n",
    "<br><br>\n",
    "Ù…Ø¯Ù„ Multinomial Naive Bayes Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª Ùˆ Ø¨Ø§ ÙØ±Ø¶ ØªÙˆØ²ÛŒØ¹ Ú†Ù†Ø¯Ø¬Ù…Ù„Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ØŒ Ø§Ø­ØªÙ…Ø§Ù„ Ù‡Ø± Ú©Ù„Ø§Ø³ Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ø² Ø³ÙˆÛŒ Ø¯ÛŒÚ¯Ø±ØŒ Gaussian Naive Bayes ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø² ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ (Ú¯Ø§ÙˆØ³ÛŒ) Ù¾ÛŒØ±ÙˆÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÙˆØ³ØªÙ‡ Ùˆ Ø¹Ø¯Ø¯ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª - Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§Ø±Ø´ÛŒ Ù…Ø§Ù†Ù†Ø¯ Bag of Words.\n",
    "<br><br>\n",
    "Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ†ØŒ Multinomial Naive Bayes Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…ØªÙˆÙ† Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ù…Ø§ÛŒØ´ Bag of Words Ø§Ø³Øª. Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø² Bernoulli Naive Bayes Ù†ÛŒØ² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ØŒ Ø§Ù…Ø§ Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„ØªÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª Ú©Ù‡ ÙÙ‚Ø· Ø­Ø¶ÙˆØ± ÛŒØ§ Ø¹Ø¯Ù… Ø­Ø¶ÙˆØ± Ú©Ù„Ù…Ø§Øª (0 ÛŒØ§ 1) Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø´ÙˆØ¯ØŒ Ù†Ù‡ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø± Ø¢Ù†â€ŒÙ‡Ø§. Ø§Ø² Ø¢Ù†Ø¬Ø§ Ú©Ù‡ Ù…Ø§ Ø§Ø² ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ Multinomial Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒ Ø§Ø³Øª.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø¹ÛŒØ§Ø±â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ test Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ú¯ÛŒØ±ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ú†Ø§Ù¾ Ú©Ù†ÛŒØ¯\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "        self.class_log_prior = None\n",
    "        self.feature_log_prob = None\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.classes = np.unique(y)\n",
    "        n_classes = len(self.classes)\n",
    "        self.class_log_prior = np.zeros(n_classes)\n",
    "        self.feature_log_prob = np.zeros((n_classes, n_features))\n",
    "        for idx, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.class_log_prior[idx] = np.log(len(X_c) / n_samples)\n",
    "            word_counts = X_c.sum(axis=0)\n",
    "            total_words = word_counts.sum()\n",
    "            probs = (word_counts + self.alpha) / (total_words + self.alpha * n_features)\n",
    "            self.feature_log_prob[idx] = np.log(probs + 1e-15)\n",
    "\n",
    "    def predict(self, X):\n",
    "        log_probs = X @ self.feature_log_prob.T + self.class_log_prior\n",
    "        return self.classes[np.argmax(log_probs, axis=1)]\n",
    "\n",
    "def to_numeric_counts(X):\n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        num = X.select_dtypes(include=[np.number])\n",
    "        if num.shape[1] != X.shape[1]:\n",
    "            coerced = X.drop(columns=num.columns).apply(pd.to_numeric, errors='coerce')\n",
    "            num = pd.concat([num, coerced], axis=1)\n",
    "        return np.nan_to_num(num.to_numpy(), nan=0.0)\n",
    "    if isinstance(X, np.ndarray):\n",
    "        if not np.issubdtype(X.dtype, np.number):\n",
    "            try:\n",
    "                return X.astype(float)\n",
    "            except Exception:\n",
    "                return np.nan_to_num(pd.DataFrame(X).apply(pd.to_numeric, errors='coerce').to_numpy(), nan=0.0)\n",
    "        return X\n",
    "    return to_numeric_counts(np.array(X))\n",
    "\n",
    "print(\"Training Multinomial Naive Bayes...\\n\")\n",
    "nb_model = MultinomialNaiveBayes(alpha=1.0)\n",
    "X_train_nb = to_numeric_counts(X_train)\n",
    "X_test_nb = to_numeric_counts(X_test)\n",
    "nb_model.fit(X_train_nb, y_train)\n",
    "\n",
    "y_pred_nb = nb_model.predict(X_test_nb)\n",
    "\n",
    "print(\"Multinomial Naive Bayes Results on Test Set:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy:  {accuracy(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"Precision: {precision(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"Recall:    {recall(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"F1-Score:  {f1_score(y_test, y_pred_nb):.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"> Ø¨Ø®Ø´ Ø´Ø´Ù…: ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù†ØªØ§ÛŒØ¬ Ø¯Ùˆ Ù…Ø¯Ù„ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯ Ùˆ ØªØ­Ù„ÛŒÙ„ÛŒ Ø¨Ø± Ù†ØªØ§ÛŒØ¬ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯.<br>\n",
    "    \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert 'y_test' in globals() and 'y_pred_lr' in globals() and 'y_pred_nb' in globals(), \"Run LR/NB training cells first.\"\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Logistic Regression': [\n",
    "        accuracy(y_test, y_pred_lr),\n",
    "        precision(y_test, y_pred_lr),\n",
    "        recall(y_test, y_pred_lr),\n",
    "        f1_score(y_test, y_pred_lr)\n",
    "    ],\n",
    "    'Naive Bayes': [\n",
    "        accuracy(y_test, y_pred_nb),\n",
    "        precision(y_test, y_pred_nb),\n",
    "        recall(y_test, y_pred_nb),\n",
    "        f1_score(y_test, y_pred_nb)\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Model Performance (Test Set):\")\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "metrics = results['Metric'].tolist()\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(x - width/2, results['Logistic Regression'], width, label='Logistic Regression', alpha=0.85)\n",
    "ax.bar(x + width/2, results['Naive Bayes'], width, label='Naive Bayes', alpha=0.85)\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Metrics', fontsize=12)\n",
    "ax.set_ylabel('Score', fontsize=12)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, calibration_curve\n",
    "import numpy as np\n",
    "\n",
    "# Extended Email Model Visualizations\n",
    "try:\n",
    "    # Normalized confusion matrices\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    cm_lr = confusion_matrix(y_test, y_pred_lr, normalize='true')\n",
    "    cm_nb = confusion_matrix(y_test, y_pred_nb, normalize='true')\n",
    "    sns.heatmap(cm_lr, annot=True, fmt='.2f', cmap='Blues', ax=axes[0])\n",
    "    axes[0].set_title('Normalized Confusion - LR')\n",
    "    sns.heatmap(cm_nb, annot=True, fmt='.2f', cmap='Greens', ax=axes[1])\n",
    "    axes[1].set_title('Normalized Confusion - NB')\n",
    "    plt.tight_layout(); plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Probability histogram (LR)\n",
    "    y_score_lr = lr_model.predict_proba(X_test_norm)\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.hist(y_score_lr, bins=30, color='#1f77b4', alpha=0.75)\n",
    "    plt.title('LR Predicted Probability Distribution')\n",
    "    plt.xlabel('Predicted Probability of Spam'); plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Calibration curve (LR)\n",
    "    y_score_lr = lr_model.predict_proba(X_test_norm)\n",
    "    frac_pos, mean_pred = calibration_curve(y_test, y_score_lr, n_bins=10)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.plot(mean_pred, frac_pos, marker='o')\n",
    "    plt.plot([0,1],[0,1],'--',color='gray')\n",
    "    plt.xlabel('Mean Predicted'); plt.ylabel('Fraction Positive')\n",
    "    plt.title('Calibration Curve (LR)')\n",
    "    plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Cumulative gains chart (LR)\n",
    "    y_score_lr = lr_model.predict_proba(X_test_norm)\n",
    "    order = np.argsort(-y_score_lr)\n",
    "    y_sorted = y_test[order]\n",
    "    cum_positive = np.cumsum(y_sorted)\n",
    "    total_positive = y_sorted.sum()\n",
    "    perc_samples = np.arange(1, len(y_sorted)+1) / len(y_sorted)\n",
    "    gains = cum_positive / total_positive if total_positive > 0 else cum_positive\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(perc_samples, gains, label='LR Gains')\n",
    "    plt.plot([0,1],[0,1],'--',color='gray', label='Baseline')\n",
    "    plt.xlabel('Proportion of Samples (sorted by score)')\n",
    "    plt.ylabel('Proportion of Positives Captured')\n",
    "    plt.title('Cumulative Gains Curve (LR)')\n",
    "    plt.legend(); plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Class distribution bar (train vs test)\n",
    "    train_counts = np.bincount(y_train)\n",
    "    test_counts = np.bincount(y_test)\n",
    "    labels = ['Ham','Spam']\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.35\n",
    "    fig, ax = plt.subplots(figsize=(6,4))\n",
    "    ax.bar(x - width/2, train_counts, width, label='Train', color='#2ca02c')\n",
    "    ax.bar(x + width/2, test_counts, width, label='Test', color='#ff7f0e')\n",
    "    ax.set_xticks(x); ax.set_xticklabels(labels)\n",
    "    ax.set_title('Class Distribution (Train vs Test)')\n",
    "    ax.legend(); plt.tight_layout(); plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Weight magnitude distribution (LR)\n",
    "    if hasattr(lr_model, 'weights'):\n",
    "        w = lr_model.weights\n",
    "        plt.figure(figsize=(7,4))\n",
    "        plt.hist(w, bins=40, color='#9467bd', alpha=0.8)\n",
    "        plt.title('LR Weight Distribution')\n",
    "        plt.xlabel('Weight Value'); plt.ylabel('Count')\n",
    "        plt.show()\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø´Ø´Ù…:</b><br>\n",
    "Ø¨Ø§ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ùˆ Ù…Ø¯Ù„ Logistic Regression Ùˆ Multinomial Naive Bayes Ø¨Ø± Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ†ØŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Logistic Regression Ø¯Ø± ØªÙ…Ø§Ù… Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Naive Bayes Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª. Ø§ÛŒÙ† Ø¨Ø±ØªØ±ÛŒ Ø¯Ø± Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ AccuracyØŒ PrecisionØŒ Recall Ùˆ F1-Score Ù‚Ø§Ø¨Ù„ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø§Ø³Øª.\n",
    "<br><br>\n",
    "<b>Ø¯Ù„Ø§ÛŒÙ„ Ø¨Ø±ØªØ±ÛŒ Logistic Regression:</b><br>\n",
    "Û±. <b>Ù…Ø¯ÛŒØ±ÛŒØª ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:</b> Naive Bayes Ø¨Ø± Ø§Ø³Ø§Ø³ ÙØ±Ø¶ Ø§Ø³ØªÙ‚Ù„Ø§Ù„ Ø´Ø±Ø·ÛŒ Ø¨ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ (Ú©Ù„Ù…Ø§Øª) Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ø¯Ø± ÙˆØ§Ù‚Ø¹ÛŒØª Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ù…ØªÙ†â€ŒÙ‡Ø§ Ø§ØºÙ„Ø¨ Ø¨Ù‡ ÛŒÚ©Ø¯ÛŒÚ¯Ø± ÙˆØ§Ø¨Ø³ØªÙ‡ Ù‡Ø³ØªÙ†Ø¯. Logistic Regression Ø§ÛŒÙ† ÙØ±Ø¶ Ø±Ø§ Ù†Ø¯Ø§Ø±Ø¯ Ùˆ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø±ÙˆØ§Ø¨Ø· Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ø¨ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†Ø¯.\n",
    "<br><br>\n",
    "Û². <b>ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø±Ø²Ù‡Ø§ÛŒ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ù‡ØªØ±:</b> Logistic Regression Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² gradient descentØŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØªØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ú©Ù‡ Ù…Ù†Ø¬Ø± Ø¨Ù‡ Ø¬Ø¯Ø§Ø³Ø§Ø²ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
    "<br><br>\n",
    "Û³. <b>Ø­Ø³Ø§Ø³ÛŒØª Ø¨Ù‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ:</b> Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Logistic Regression Ø¨Ø§Ø¹Ø« Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ù‡ØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Naive Bayes Ø§Ø² Ø§ÛŒÙ† Ù…Ø²ÛŒØª Ø¨Ù‡Ø±Ù‡ Ù†Ù…ÛŒâ€ŒØ¨Ø±Ø¯.\n",
    "<br><br>\n",
    "<b>Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ:</b><br>\n",
    "Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Naive Bayes ÛŒÚ© Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡ Ùˆ Ø³Ø±ÛŒØ¹ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ùˆ Ø²Ù…Ø§Ù†ÛŒ Ú©Ù‡ ÙØ±Ø¶ Ø§Ø³ØªÙ‚Ù„Ø§Ù„ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø¨Ø±Ù‚Ø±Ø§Ø± Ø§Ø³Øª Ù…Ù†Ø§Ø³Ø¨ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ Logistic Regression Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¨ÛŒØ´ØªØ± Ùˆ ØªÙˆØ§Ù†Ø§ÛŒÛŒ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±ÙˆØ§Ø¨Ø· Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ±ØŒ Ø¯Ø± Ø§ÛŒÙ† Ù…Ø³Ø¦Ù„Ù‡ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø³Ù¾Ù… Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ø¯Ø§Ø±Ø¯. Ø¨Ø§ Ø§ÛŒÙ† Ø­Ø§Ù„ØŒ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ Ù†ØªØ§ÛŒØ¬ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨ÛŒÙ† Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ù‡ Ù†ÛŒØ§Ø²Ù‡Ø§ÛŒ Ø®Ø§Øµ Ù¾Ø±ÙˆÚ˜Ù‡ Ù…Ø§Ù†Ù†Ø¯ Ø³Ø±Ø¹Øª Ø§Ø¬Ø±Ø§ØŒ Ù‚Ø§Ø¨Ù„ÛŒØª ØªÙØ³ÛŒØ± Ùˆ Ø¯Ù‚Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ…: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Logistic Regression Ùˆ Naive Bayes <br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">Ù‡Ø¯Ù Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Logistic Regression Ùˆ Naive Bayes Ø¯Ø± ØªØ´Ø®ÛŒØµ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø§Ø² Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ù†ÙˆÙ†ÛŒ (Ù…Ø¹ØªØ¨Ø±) Ø§Ø³Øª.\n",
    "Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÛŒ Ø§Ø±Ø§Ø¦Ù‡â€ŒØ´Ø¯Ù‡ (Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒ Ø®Ø§Ù…)ØŒ ÛŒÚ© Ø³Ø±ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø§Ø² Ø§ÛŒÙ† URLÙ‡Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø§ÛŒÛŒØ¯ Ùˆ ØªØ£Ø«ÛŒØ± Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.<br>Ù†Ú©ØªÙ‡: Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ù…Ø¬Ø§Ø² Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨â€ŒØ®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù‡Ø³ØªÛŒØ¯.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø§ÙˆÙ„: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§Ø² Ø¢Ø¯Ø±Ø³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø·Ù‡â€ŒÙ‡Ø§ (nb_dots)\n",
    "<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ù„Ø´â€ŒÙ‡Ø§ (nb_slashes)\n",
    "<br>\n",
    "- ØªØ¹Ø¯Ø§Ø¯ Ø®Ø· â€ŒØªÛŒØ±Ù‡â€ŒÙ‡Ø§ (nb_hyphens)\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§ÛŒÙ† Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø±ÙˆÛŒ Ù‡Ø´ØªØ§Ø¯ Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´) Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø±ÙˆÛŒ Ø¨ÛŒØ³Øª Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ†) Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯\n",
    "<br>\n",
    "<br>\n",
    "- Ø¢ÛŒØ§ Ù„Ø§Ø²Ù… Ø§Ø³Øª Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ØŸ Ú†Ø±Ø§ØŸ Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø´Ù…Ø§ Â«Ø¨Ù„Ù‡Â» Ø§Ø³ØªØŒ Ù†ÙˆØ¹ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¢Ù† Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "- Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø§Ø² Ø¨ÛŒÙ† Ù†Ø³Ø®Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Naive Bayes (GaussianNB ÛŒØ§ MultinomialNB) Ú©Ø¯Ø§Ù… Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§ Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³ØªØŸ Ø¯Ù„ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ùˆ Ø¨Ø§ Ø¢Ù† Ù…Ø¯Ù„ Ø¢Ø²Ù…Ø§ÛŒØ´ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø§ÙˆÙ„:</b><br>\n",
    "<b>Û±. Ø¢ÛŒØ§ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ù„Ø§Ø²Ù… Ø§Ø³ØªØŸ</b><br>\n",
    "Ø¨Ù„Ù‡ØŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Logistic Regression Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³ØªØŒ Ø§Ù…Ø§ Ø¨Ø±Ø§ÛŒ Naive Bayes Ø§Ù„Ø²Ø§Ù…ÛŒ Ù†ÛŒØ³Øª.\n",
    "<br><br>\n",
    "<b>Ø¯Ù„ÛŒÙ„ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Logistic Regression:</b><br>\n",
    "- ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ (ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø·Ù‡â€ŒÙ‡Ø§ØŒ Ø§Ø³Ù„Ø´â€ŒÙ‡Ø§ Ùˆ Ø®Ø· ØªÛŒØ±Ù‡â€ŒÙ‡Ø§) Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ù…Ù‚ÛŒØ§Ø³â€ŒÙ‡Ø§ÛŒ Ù…ØªÙØ§ÙˆØªÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯. Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ØŒ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ù„Ø´â€ŒÙ‡Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± Ø¨Ø§Ø²Ù‡ Û² ØªØ§ Û±Û° Ø¨Ø§Ø´Ø¯ØŒ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø·Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ø¨Ø§Ø²Ù‡ Û± ØªØ§ Ûµ.\n",
    "<br>\n",
    "- Logistic Regression Ø¨Ù‡ Ù…Ù‚ÛŒØ§Ø³ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø­Ø³Ø§Ø³ Ø§Ø³Øª Ùˆ ÙˆØ¬ÙˆØ¯ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ø²Ø±Ú¯â€ŒØªØ± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§Ø¹Ø« Ø´ÙˆØ¯ Ù…Ø¯Ù„ Ø¨Ù‡ Ø¢Ù†â€ŒÙ‡Ø§ ÙˆØ²Ù† Ø¨ÛŒØ´ØªØ±ÛŒ Ø¨Ø¯Ù‡Ø¯.\n",
    "<br>\n",
    "- Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² StandardScaler (Ú©Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø±Ø§ ØµÙØ± Ùˆ Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø± Ø±Ø§ ÛŒÚ© Ù…ÛŒâ€ŒÚ©Ù†Ø¯) Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ gradient descent Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø´ÙˆØ¯ Ùˆ Ù…Ø¯Ù„ Ø¨Ù‡ØªØ± Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø¨ÛŒÙ†Ø¯.\n",
    "<br><br>\n",
    "<b>Ú†Ø±Ø§ Ø¨Ø±Ø§ÛŒ Naive Bayes Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ù†ÛŒØ³ØªØŸ</b><br>\n",
    "- GaussianNB Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ùˆ ÙˆØ§Ø±ÛŒØ§Ù†Ø³ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³ Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ù…Ù‚ÛŒØ§Ø³ Ù†Ø³Ø¨ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ØªØ£Ø«ÛŒØ± Ù…Ø³ØªÙ‚ÛŒÙ…ÛŒ Ø¨Ø± Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ Ù†Ø¯Ø§Ø±Ø¯.\n",
    "<br>\n",
    "- MultinomialNB Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§Ø±Ø´ÛŒ Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ùˆ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ø¯Ø§Ø±Ø¯.\n",
    "<br><br>\n",
    "<b>Û². Ú©Ø¯Ø§Ù… Ù†Ø³Ø®Ù‡ Naive Bayes Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³ØªØŸ</b><br>\n",
    "<b>GaussianNB</b> Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ±ÛŒ Ø§Ø³Øª. Ø¯Ù„ÛŒÙ„: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ù…Ø§ (ØªØ¹Ø¯Ø§Ø¯ Ù†Ù‚Ø·Ù‡â€ŒÙ‡Ø§ØŒ Ø§Ø³Ù„Ø´â€ŒÙ‡Ø§ Ùˆ Ø®Ø· ØªÛŒØ±Ù‡â€ŒÙ‡Ø§) Ø¹Ø¯Ø¯Ù‡Ø§ÛŒ Ø´Ù…Ø§Ø±Ø´ÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø®ØªÙ„ÙÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ ØªÙˆØ²ÛŒØ¹ Ù†Ø³Ø¨ØªØ§Ù‹ Ù¾ÛŒÙˆØ³ØªÙ‡â€ŒØ§ÛŒ Ø¯Ø§Ø±Ù†Ø¯. GaussianNB ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø² ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ Ù¾ÛŒØ±ÙˆÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù†ÙˆØ¹ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª.\n",
    "<br><br>\n",
    "MultinomialNB Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Bag of Words Ùˆ ÙØ±Ø§ÙˆØ§Ù†ÛŒ Ú©Ù„Ù…Ø§Øª Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³ØªØŒ Ù†Ù‡ Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ URL Ú©Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù…Ù‚Ø§Ø¯ÛŒØ± ØµÙØ± Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯ ÛŒØ§ Ù…Ù‚ÛŒØ§Ø³ Ù…ØªÙØ§ÙˆØªÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø¬Ø¯ÙˆÙ„ÛŒ (Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…) Ø´Ø§Ù…Ù„ Û³ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ + Ø¨Ø±Ú†Ø³Ø¨ Ù‡Ø¯Ù (status)<br>\n",
    "- Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø´Ø§Ù…Ù„ Accuracy, Precision, Recall, F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression as SkLogReg\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "\n",
    "\n",
    "def load_url_dataset():\n",
    "    candidates = [\n",
    "        'urls.csv', 'phishing.csv', 'phishing_urls.csv', 'url_dataset.csv'\n",
    "    ]\n",
    "    df = None\n",
    "    for path in candidates:\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            print(f\"Loaded dataset: {path} | shape={df.shape}\")\n",
    "            break\n",
    "        except Exception:\n",
    "            continue\n",
    "    if df is None:\n",
    "        df = pd.DataFrame({\n",
    "            'url': [\n",
    "                'http://example.com',\n",
    "                'https://secure.paypal.com/login',\n",
    "                'http://192.168.1.10/verify',\n",
    "                'https://account-update-secure-login.com/pay',\n",
    "                'http://my-bank-example.com/login?session=1234'\n",
    "            ],\n",
    "            'status': [0, 0, 1, 1, 1]\n",
    "        })\n",
    "        print(\"Using fallback URL dataset (demo mode).\")\n",
    "    if 'label' in df.columns and 'status' not in df.columns:\n",
    "        df = df.rename(columns={'label': 'status'})\n",
    "    if df['status'].dtype == object:\n",
    "        mapping = {\n",
    "            'phishing': 1, 'bad': 1, 'malicious': 1, '1': 1,\n",
    "            'legit': 0, 'benign': 0, 'good': 0, '0': 0\n",
    "        }\n",
    "        df['status'] = df['status'].astype(str).str.lower().map(mapping).fillna(0).astype(int)\n",
    "    return df[['url', 'status']]\n",
    "\n",
    "\n",
    "def structural_features(url: str):\n",
    "    return {\n",
    "        'nb_dots': url.count('.'),\n",
    "        'nb_slashes': url.count('/'),\n",
    "        'nb_hyphens': url.count('-')\n",
    "    }\n",
    "\n",
    "\n",
    "def build_feature_frame(df_urls: pd.DataFrame, feat_fn) -> pd.DataFrame:\n",
    "    feats = df_urls['url'].apply(feat_fn).apply(pd.Series)\n",
    "    feats['status'] = df_urls['status'].values\n",
    "    return feats\n",
    "\n",
    "\n",
    "def evaluate_models(X_train, X_test, y_train, y_test, multinomial_ok=False):\n",
    "    results = {}\n",
    "    lr_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "        ('clf', SkLogReg(max_iter=1000))\n",
    "    ])\n",
    "    lr_pipe.fit(X_train, y_train)\n",
    "    y_pred_lr2 = lr_pipe.predict(X_test)\n",
    "    results['Logistic Regression'] = {\n",
    "        'Accuracy':  accuracy(y_test, y_pred_lr2),\n",
    "        'Precision': precision(y_test, y_pred_lr2),\n",
    "        'Recall':    recall(y_test, y_pred_lr2),\n",
    "        'F1-Score':  f1_score(y_test, y_pred_lr2),\n",
    "    }\n",
    "    if multinomial_ok:\n",
    "        nb = MultinomialNB()\n",
    "    else:\n",
    "        nb = GaussianNB()\n",
    "    nb.fit(X_train, y_train)\n",
    "    y_pred_nb2 = nb.predict(X_test)\n",
    "    results['Naive Bayes'] = {\n",
    "        'Accuracy':  accuracy(y_test, y_pred_nb2),\n",
    "        'Precision': precision(y_test, y_pred_nb2),\n",
    "        'Recall':    recall(y_test, y_pred_nb2),\n",
    "        'F1-Score':  f1_score(y_test, y_pred_nb2),\n",
    "    }\n",
    "    return results\n",
    "\n",
    "urls_df = load_url_dataset()\n",
    "struct_df = build_feature_frame(urls_df, structural_features)\n",
    "\n",
    "X = struct_df.drop('status', axis=1).values\n",
    "y = struct_df['status'].values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Structural features sample:\")\n",
    "print(struct_df.head())\n",
    "\n",
    "res = evaluate_models(X_tr, X_te, y_tr, y_te, multinomial_ok=False)\n",
    "\n",
    "print(\"\\nResults (Structural Features):\")\n",
    "res_df = pd.DataFrame(res).T\n",
    "print(res_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø¯ÙˆÙ…: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø§Ø² Ø¢Ø¯Ø±Ø³ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯:\n",
    "<br>\n",
    "- Ø·ÙˆÙ„ Ú©Ù„ Ø¢Ø¯Ø±Ø³ (length_url)\n",
    "<br>\n",
    "- Ù†Ø³Ø¨Øª ØªØ¹Ø¯Ø§Ø¯ Ø§Ø±Ù‚Ø§Ù… (0â€“9) Ø¨Ù‡ Ú©Ù„ Ø·ÙˆÙ„ Ø¢Ø¯Ø±Ø³ (ratio_digits_url)\n",
    "<br>\n",
    "- Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ÛŒÙ† Ú©Ù„Ù…Ù‡ (Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ù„ÙØ¨Ø§ÛŒÛŒ) Ø¯Ø± URL Ú©Ù‡ Ø¨Ø§ Ø¹Ù„Ø§Ø¦Ù… Ø¬Ø¯Ø§Ø³Ø§Ø² (Ù†Ù‚Ø·Ù‡ØŒ Ø¹Ù„Ø§Ù…Øªâ€ŒØ³ÙˆØ§Ù„ØŒ Ø§Ø³Ù„Ø´ Ùˆ...) Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯. (longest_words_raw)\n",
    "<br>\n",
    "Ø³Ù¾Ø³ Ø¨Ø§ Ø§ÛŒÙ† Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø±ÙˆÛŒ Ù‡Ø´ØªØ§Ø¯ Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´) Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…ÙˆÙ† (Ø¨ÛŒØ³Øª Ø¯Ø±ØµØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§) Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "- Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Naive Bayes Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù…Ù†Ø·Ù‚ÛŒ Ø§Ø³ØªØŸ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯. Ú©Ø¯Ø§Ù… Ù†Ø³Ø®Ù‡ Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³ØªØŸ \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø¯ÙˆÙ…:</b><br>\n",
    "<b>Ø¢ÛŒØ§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Naive Bayes Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ù…Ù†Ø·Ù‚ÛŒ Ø§Ø³ØªØŸ</b><br>\n",
    "Ø¨Ù„Ù‡ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Naive Bayes Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒÛŒ Ù…Ù†Ø·Ù‚ÛŒ Ø§Ø³ØªØŒ Ø§Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ù†Ø³Ø®Ù‡ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø´ÙˆØ¯.\n",
    "<br><br>\n",
    "<b>ØªØ­Ù„ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡:</b><br>\n",
    "Û±. <b>Ø·ÙˆÙ„ Ú©Ù„ Ø¢Ø¯Ø±Ø³ (length_url):</b> ÛŒÚ© Ø¹Ø¯Ø¯ Ù¾ÛŒÙˆØ³ØªÙ‡ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ø®ØªÙ„ÙÛŒ Ø§Ø² ØµÙØ± ØªØ§ Ú†Ù†Ø¯ ØµØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\n",
    "<br>\n",
    "Û². <b>Ù†Ø³Ø¨Øª Ø§Ø±Ù‚Ø§Ù… (ratio_digits_url):</b> ÛŒÚ© Ø¹Ø¯Ø¯ Ø§Ø¹Ø´Ø§Ø±ÛŒ Ø¨ÛŒÙ† Û° Ùˆ Û± Ú©Ù‡ Ù†Ø³Ø¨Øª Ø§Ø±Ù‚Ø§Ù… Ø¨Ù‡ Ú©Ù„ Ø·ÙˆÙ„ URL Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
    "<br>\n",
    "Û³. <b>Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ÛŒÙ† Ú©Ù„Ù…Ù‡ (longest_words_raw):</b> ÛŒÚ© Ø¹Ø¯Ø¯ ØµØ­ÛŒØ­ Ú©Ù‡ Ø·ÙˆÙ„ Ø¨Ø²Ø±Ú¯â€ŒØªØ±ÛŒÙ† Ø±Ø´ØªÙ‡ Ø§Ù„ÙØ¨Ø§ÛŒÛŒ Ø±Ø§ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.\n",
    "<br><br>\n",
    "<b>Ú©Ø¯Ø§Ù… Ù†Ø³Ø®Ù‡ Naive Bayes Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³ØªØŸ</b><br>\n",
    "<b>GaussianNB</b> Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒ Ø§Ø³Øª. Ø¯Ù„Ø§ÛŒÙ„:\n",
    "<br><br>\n",
    "- Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¹Ø¯Ø¯Ù‡Ø§ÛŒ <b>Ù¾ÛŒÙˆØ³ØªÙ‡ Ùˆ Ø§Ø¹Ø´Ø§Ø±ÛŒ</b> Ù‡Ø³ØªÙ†Ø¯ØŒ Ù†Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§Ø±Ø´ÛŒ Ø®Ø§Ù… Ù…Ø«Ù„ Bag of Words.\n",
    "<br>\n",
    "- GaussianNB ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø² ØªÙˆØ²ÛŒØ¹ Ù†Ø±Ù…Ø§Ù„ (Ú¯Ø§ÙˆØ³ÛŒ) Ù¾ÛŒØ±ÙˆÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù†ÙˆØ¹ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ùˆ Ù¾ÛŒÙˆØ³ØªÙ‡ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª.\n",
    "<br>\n",
    "- MultinomialNB ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§Ø±Ø´ÛŒ Ù…Ø«Ø¨Øª (Ù…Ø§Ù†Ù†Ø¯ ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø± Ú©Ù„Ù…Ø§Øª) Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ùˆ Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø§Ø¹Ø´Ø§Ø±ÛŒ ÛŒØ§ Ù…Ù†ÙÛŒ Ú©Ø§Ø± Ú©Ù†Ø¯.\n",
    "<br><br>\n",
    "<b>Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ:</b><br>\n",
    "Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…Ø§Ù‡ÛŒØª Ù¾ÛŒÙˆØ³ØªÙ‡ Ùˆ Ø¹Ø¯Ø¯ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ØŒ GaussianNB Ú¯Ø²ÛŒÙ†Ù‡ Ù…Ù†Ø·Ù‚ÛŒ Ùˆ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø§Ø³Øª. Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù‚Ø§Ø¯Ø± Ø§Ø³Øª ØªÙˆØ²ÛŒØ¹ Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³ (ÙÛŒØ´ÛŒÙ†Ú¯ ÛŒØ§ Ù‚Ø§Ù†ÙˆÙ†ÛŒ) ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯ Ùˆ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢Ù† ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ú©Ù†Ø¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø¬Ø¯ÙˆÙ„ÛŒ (Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…) Ø´Ø§Ù…Ù„ Û³ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ + Ø¨Ø±Ú†Ø³Ø¨ Ù‡Ø¯Ù (status)<br>\n",
    "- Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø´Ø§Ù…Ù„ Accuracy, Precision, Recall, F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression as SkLogReg\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "if 'urls_df' not in globals():\n",
    "    def load_url_dataset():\n",
    "        candidates = ['urls.csv', 'phishing.csv', 'phishing_urls.csv', 'url_dataset.csv']\n",
    "        df = None\n",
    "        for path in candidates:\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                print(f\"Loaded dataset: {path} | shape={df.shape}\")\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if df is None:\n",
    "            df = pd.DataFrame({\n",
    "                'url': [\n",
    "                    'http://example.com',\n",
    "                    'https://secure.paypal.com/login',\n",
    "                    'http://192.168.1.10/verify',\n",
    "                    'https://account-update-secure-login.com/pay',\n",
    "                    'http://my-bank-example.com/login?session=1234'\n",
    "                ],\n",
    "                'status': [0, 0, 1, 1, 1]\n",
    "            })\n",
    "            print(\"Using fallback URL dataset (demo mode).\")\n",
    "        if 'label' in df.columns and 'status' not in df.columns:\n",
    "            df = df.rename(columns={'label': 'status'})\n",
    "        if df['status'].dtype == object:\n",
    "            mapping = {\n",
    "                'phishing': 1, 'bad': 1, 'malicious': 1, '1': 1,\n",
    "                'legit': 0, 'benign': 0, 'good': 0, '0': 0\n",
    "            }\n",
    "            df['status'] = df['status'].astype(str).str.lower().map(mapping).fillna(0).astype(int)\n",
    "        return df[['url', 'status']]\n",
    "    urls_df = load_url_dataset()\n",
    "\n",
    "\n",
    "def statistical_features(url: str):\n",
    "    length_url = len(url)\n",
    "    digits = sum(ch.isdigit() for ch in url)\n",
    "    ratio_digits_url = (digits / length_url) if length_url > 0 else 0.0\n",
    "    tokens = re.split(r'[^A-Za-z]+', url)\n",
    "    longest_words_raw = max((len(t) for t in tokens), default=0)\n",
    "    return {\n",
    "        'length_url': length_url,\n",
    "        'ratio_digits_url': ratio_digits_url,\n",
    "        'longest_words_raw': longest_words_raw,\n",
    "    }\n",
    "\n",
    "stat_df = urls_df['url'].apply(statistical_features).apply(pd.Series)\n",
    "stat_df['status'] = urls_df['status'].values\n",
    "\n",
    "X = stat_df.drop('status', axis=1).values\n",
    "y = stat_df['status'].values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Statistical features sample:\")\n",
    "print(stat_df.head())\n",
    "\n",
    "lr_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "    ('clf', SkLogReg(max_iter=1000))\n",
    "])\n",
    "lr_pipe.fit(X_tr, y_tr)\n",
    "y_pred_lr_s2 = lr_pipe.predict(X_te)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_tr, y_tr)\n",
    "y_pred_nb_s2 = nb.predict(X_te)\n",
    "\n",
    "res2 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Logistic Regression': [\n",
    "        accuracy(y_te, y_pred_lr_s2),\n",
    "        precision(y_te, y_pred_lr_s2),\n",
    "        recall(y_te, y_pred_lr_s2),\n",
    "        f1_score(y_te, y_pred_lr_s2)\n",
    "    ],\n",
    "    'Naive Bayes (Gaussian)': [\n",
    "        accuracy(y_te, y_pred_nb_s2),\n",
    "        precision(y_te, y_pred_nb_s2),\n",
    "        recall(y_te, y_pred_nb_s2),\n",
    "        f1_score(y_te, y_pred_nb_s2)\n",
    "    ]\n",
    "})\n",
    "print(\"\\nResults (Statistical/Content Features):\")\n",
    "print(res2.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ø³ÙˆÙ…: ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø² Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ù…Ø§ÛŒÛŒØ¯ Ùˆ Ù…Ø´Ø§Ø¨Ù‡ Ø¯Ùˆ Ø¨Ø®Ø´ Ù‚Ø¨Ù„ÛŒØŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯.\n",
    "Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯ Ú†Ø±Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± ØªØ´Ø®ÛŒØµ ÙÛŒØ´ÛŒÙ†Ú¯ Ù…Ø¤Ø«Ø± Ø¨Ø§Ø´Ø¯ØŸ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ø³ÙˆÙ…:</b><br>\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ØŒ Ø³Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡ Ø²ÛŒØ± Ø±Ø§ Ø§Ø² URLÙ‡Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒÙ… Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ù…Ø¤Ø«Ø± Ø¨Ø§Ø´Ù†Ø¯:\n",
    "<br><br>\n",
    "<b>Û±. ÙˆØ¬ÙˆØ¯ Ø¢Ø¯Ø±Ø³ IP Ø¯Ø± URL (has_ip):</b><br>\n",
    "Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ Ø¯Ø± URL Ø¨Ù‡ Ø¬Ø§ÛŒ Ù†Ø§Ù… Ø¯Ø§Ù…Ù†Ù‡ØŒ Ø§Ø² Ø¢Ø¯Ø±Ø³ IP Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±. Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø§Ø² Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ù…Ù†Ù‡ (Ù…Ø§Ù†Ù†Ø¯ google.com) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø§ØºÙ„Ø¨ Ø¨Ø±Ø§ÛŒ Ù…Ø®ÙÛŒ Ú©Ø±Ø¯Ù† Ù‡ÙˆÛŒØª ÙˆØ§Ù‚Ø¹ÛŒ Ø®ÙˆØ¯ Ø§Ø² Ø¢Ø¯Ø±Ø³â€ŒÙ‡Ø§ÛŒ IP (Ù…Ø§Ù†Ù†Ø¯ http://192.168.1.1) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯. Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù†Ø´Ø§Ù†Ù‡ Ù‚ÙˆÛŒ Ø§Ø² ÙÛŒØ´ÛŒÙ†Ú¯ Ø¨Ø§Ø´Ø¯.\n",
    "<br><br>\n",
    "<b>Û². ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ±Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ (num_subdomains):</b><br>\n",
    "Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ±Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± URL Ø±Ø§ Ù…ÛŒâ€ŒØ´Ù…Ø§Ø±Ø¯. Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø§ØºÙ„Ø¨ Ø§Ø² Ø²ÛŒØ±Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ØªØ¹Ø¯Ø¯ Ùˆ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ ØªØ§ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø±Ø§ ÙØ±ÛŒØ¨ Ø¯Ù‡Ù†Ø¯ØŒ Ù…Ø§Ù†Ù†Ø¯: secure-login.paypal-verify.suspicious-site.com. Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ù…Ù†Ù‡ Ø³Ø§Ø¯Ù‡â€ŒØªØ±ÛŒ Ø¯Ø§Ø±Ù†Ø¯. ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø§Ù„Ø§ÛŒ Ø²ÛŒØ±Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù‡Ø´Ø¯Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø¨Ø§Ø´Ø¯.\n",
    "<br><br>\n",
    "<b>Û³. Ù†Ø³Ø¨Øª Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ Ø¯Ø± Ù…Ø³ÛŒØ± Ùˆ query (ratio_special):</b><br>\n",
    "Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒ Ù†Ø³Ø¨Øª Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ± Ø§Ù„ÙØ¨Ø§ÛŒÛŒ-Ø¹Ø¯Ø¯ÛŒ (Ù…Ø§Ù†Ù†Ø¯ @ØŒ &ØŒ =ØŒ %) Ø¯Ø± Ø¨Ø®Ø´ path Ùˆ query string Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø§ØºÙ„Ø¨ Ø§Ø² Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ Ø²ÛŒØ§Ø¯ Ø¨Ø±Ø§ÛŒ Ø±Ù…Ø²Ú¯Ø°Ø§Ø±ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÛŒØ§ Ø§ÛŒØ¬Ø§Ø¯ URLÙ‡Ø§ÛŒ Ú¯Ù…Ø±Ø§Ù‡â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯. Ù†Ø³Ø¨Øª Ø¨Ø§Ù„Ø§ÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø®Ø§Øµ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ù†Ø´Ø§Ù†Ù‡â€ŒØ§ÛŒ Ø§Ø² ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ ÙØ±ÛŒØ¨ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø§Ø´Ø¯.\n",
    "<br><br>\n",
    "<b>Ú†Ø±Ø§ GaussianNB Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³ØªØŸ</b><br>\n",
    "Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ø§ÛŒÙ†Ø±ÛŒ (has_ip: 0 ÛŒØ§ 1)ØŒ Ø´Ù…Ø§Ø±Ø´ÛŒ (num_subdomains) Ùˆ Ù¾ÛŒÙˆØ³ØªÙ‡ (ratio_special: Ø¨ÛŒÙ† 0 Ùˆ 1) Ù‡Ø³ØªÙ†Ø¯. GaussianNB Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§ Ø§ÛŒÙ† Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ù…Ø®ØªÙ„Ø· Ú©Ø§Ø± Ú©Ù†Ø¯ Ùˆ ØªÙˆØ²ÛŒØ¹ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†Ø¯ØŒ Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ù†Ø·Ù‚ÛŒ Ø§Ø³Øª.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "- Ø¬Ø¯ÙˆÙ„ÛŒ (Ø¯ÛŒØªØ§ÙØ±ÛŒÙ…) Ø´Ø§Ù…Ù„ Û³ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬â€ŒØ´Ø¯Ù‡ + Ø¨Ø±Ú†Ø³Ø¨ Ù‡Ø¯Ù (status)<br>\n",
    "- Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø´Ø§Ù…Ù„ Accuracy, Precision, Recall, F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…Ø¯Ù„\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression as SkLogReg\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "try:\n",
    "    import tldextract\n",
    "    def extract_sub_len(host):\n",
    "        ext = tldextract.extract(host)\n",
    "        return len(ext.subdomain) if ext.subdomain else 0\n",
    "except ImportError:\n",
    "    def extract_sub_len(host):\n",
    "        parts = host.split('.')\n",
    "        if len(parts) > 2:\n",
    "            return sum(len(p) for p in parts[:-2])\n",
    "        return 0\n",
    "\n",
    "if 'urls_df' not in globals():\n",
    "    def load_url_dataset():\n",
    "        candidates = ['urls.csv', 'phishing.csv', 'phishing_urls.csv', 'url_dataset.csv']\n",
    "        df = None\n",
    "        for path in candidates:\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                print(f\"Loaded dataset: {path} | shape={df.shape}\")\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if df is None:\n",
    "            df = pd.DataFrame({\n",
    "                'url': [\n",
    "                    'http://example.com',\n",
    "                    'https://secure.paypal.com/login',\n",
    "                    'http://192.168.1.10/verify',\n",
    "                    'https://account-update-secure-login.com/pay',\n",
    "                    'http://my-bank-example.com/login?session=1234'\n",
    "                ],\n",
    "                'status': [0, 0, 1, 1, 1]\n",
    "            })\n",
    "            print(\"Using fallback URL dataset (demo mode).\")\n",
    "        if 'label' in df.columns and 'status' not in df.columns:\n",
    "            df = df.rename(columns={'label': 'status'})\n",
    "        if df['status'].dtype == object:\n",
    "            mapping = {\n",
    "                'phishing': 1, 'bad': 1, 'malicious': 1, '1': 1,\n",
    "                'legit': 0, 'benign': 0, 'good': 0, '0': 0\n",
    "            }\n",
    "            df['status'] = df['status'].astype(str).str.lower().map(mapping).fillna(0).astype(int)\n",
    "        return df[['url', 'status']]\n",
    "    urls_df = load_url_dataset()\n",
    "\n",
    "\n",
    "def creative_features(url: str):\n",
    "    parsed = urlparse(url)\n",
    "    host = parsed.netloc\n",
    "    path = parsed.path or ''\n",
    "    sub_len = extract_sub_len(host)\n",
    "    has_ip = bool(re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', host))\n",
    "    at_sign = '@' in url\n",
    "    dash_in_host = '-' in host\n",
    "    dot_count_host = host.count('.')\n",
    "    slash_count_path = path.count('/')\n",
    "    percent_encoding = '%' in url\n",
    "    suspicious_words = sum(w in url.lower() for w in ['verify', 'update', 'login', 'secure', 'account'])\n",
    "    has_https = parsed.scheme.lower() == 'https'\n",
    "    return {\n",
    "        'sub_len': sub_len,\n",
    "        'has_ip': int(has_ip),\n",
    "        'at_sign': int(at_sign),\n",
    "        'dash_in_host': int(dash_in_host),\n",
    "        'dot_count_host': dot_count_host,\n",
    "        'slash_count_path': slash_count_path,\n",
    "        'percent_encoding': int(percent_encoding),\n",
    "        'suspicious_words': suspicious_words,\n",
    "        'has_https': int(has_https)\n",
    "    }\n",
    "\n",
    "cre_df = urls_df['url'].apply(creative_features).apply(pd.Series)\n",
    "cre_df['status'] = urls_df['status'].values\n",
    "\n",
    "X = cre_df.drop('status', axis=1).values\n",
    "y = cre_df['status'].values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Creative features sample:\")\n",
    "print(cre_df.head())\n",
    "\n",
    "lr_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "    ('clf', SkLogReg(max_iter=1000))\n",
    "])\n",
    "lr_pipe.fit(X_tr, y_tr)\n",
    "y_pred_lr_s3 = lr_pipe.predict(X_te)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_tr, y_tr)\n",
    "y_pred_nb_s3 = nb.predict(X_te)\n",
    "\n",
    "res3 = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Logistic Regression': [\n",
    "        accuracy(y_te, y_pred_lr_s3),\n",
    "        precision(y_te, y_pred_lr_s3),\n",
    "        recall(y_te, y_pred_lr_s3),\n",
    "        f1_score(y_te, y_pred_lr_s3)\n",
    "    ],\n",
    "    'Naive Bayes (Gaussian)': [\n",
    "        accuracy(y_te, y_pred_nb_s3),\n",
    "        precision(y_te, y_pred_nb_s3),\n",
    "        recall(y_te, y_pred_nb_s3),\n",
    "        f1_score(y_te, y_pred_nb_s3)\n",
    "    ]\n",
    "})\n",
    "print(\"\\nResults (Creative Features):\")\n",
    "print(res3.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ØªÙ…Ø§Ù… Û¹ ÙˆÛŒÚ˜Ú¯ÛŒ (Û³ Ø³Ø§Ø®ØªØ§Ø±ÛŒ + Û³ Ø¢Ù…Ø§Ø±ÛŒ + Û³ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡) Ø±Ø§ Ø¨Ø§ Ù‡Ù… ØªØ±Ú©ÛŒØ¨ Ú©Ù†ÛŒØ¯.\n",
    "Ø³Ù¾Ø³ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ Ø±Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "<br>\n",
    "Ø¢ÛŒØ§ ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§Ø¹Ø« Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ\n",
    "<br>\n",
    "Ø§Ú¯Ø± Ø®ÛŒØ±ØŒ Ø¨Ù‡ Ù†Ø¸Ø± Ø´Ù…Ø§ Ø¯Ù„ÛŒÙ„ Ø¢Ù† Ú†ÛŒØ³ØªØŸ (Ù…Ø«Ù„Ø§Ù‹ ØªØ¯Ø§Ø®Ù„ Ø¨ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ ÛŒØ§ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§Ù„Ø§ Ùˆ...)\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…:</b><br>\n",
    "<b>Ø¢ÛŒØ§ ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø§Ø¹Ø« Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ</b><br>\n",
    "Ø¨Ù‡ Ø·ÙˆØ± Ú©Ù„ÛŒØŒ Ø¨Ù„Ù‡. ØªØ±Ú©ÛŒØ¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÙˆØ¹ Ø§Ø² Ø¬Ù†Ø¨Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù URL (Ø³Ø§Ø®ØªØ§Ø±ÛŒØŒ Ø¢Ù…Ø§Ø±ÛŒ Ùˆ Ø®Ù„Ø§Ù‚) Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ø§Ø¹Ø« Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø²ÛŒØ±Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø± Ø§Ø®ØªÛŒØ§Ø± Ù…Ø¯Ù„ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
    "<br><br>\n",
    "<b>Ø¯Ù„Ø§ÛŒÙ„ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ:</b><br>\n",
    "Û±. <b>ØªÚ©Ù…ÛŒÙ„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª:</b> Ù‡Ø± Ø¯Ø³ØªÙ‡ Ø§Ø² ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¬Ù†Ø¨Ù‡ Ø®Ø§ØµÛŒ Ø§Ø² URL Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ø¨Ù‡ Ø´Ú©Ù„ Ø¸Ø§Ù‡Ø±ÛŒ URL Ù†Ú¯Ø§Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ØŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ø¨Ù‡ Ù…Ø­ØªÙˆØ§ Ùˆ Ø·ÙˆÙ„ ØªÙˆØ¬Ù‡ Ø¯Ø§Ø±Ù†Ø¯ØŒ Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§Ù‚ Ø¨Ù‡ Ù†Ø´Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ ÙÛŒØ´ÛŒÙ†Ú¯ Ù…Ø§Ù†Ù†Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² IP Ù…ÛŒâ€ŒÙ¾Ø±Ø¯Ø§Ø²Ù†Ø¯.\n",
    "<br>\n",
    "Û². <b>Ú©Ø§Ù‡Ø´ False Positives/Negatives:</b> Ø§Ú¯Ø± ÛŒÚ© ÙˆÛŒÚ˜Ú¯ÛŒ Ù†ØªÙˆØ§Ù†Ø¯ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø±Ø§ Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù†Ø¯ØŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ú©Ù…Ú© Ú©Ù†Ù†Ø¯.\n",
    "<br>\n",
    "Û³. <b>Ø§ÙØ²Ø§ÛŒØ´ Ù‚Ø¯Ø±Øª ØªØ´Ø®ÛŒØµ:</b> Ù…Ø¯Ù„ Ø¨Ø§ Ø¯Ø§Ø´ØªÙ† Û¹ ÙˆÛŒÚ˜Ú¯ÛŒ Ù…Ø®ØªÙ„ÙØŒ Ù‚Ø§Ø¯Ø± Ø§Ø³Øª Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ±ÛŒ Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯.\n",
    "<br><br>\n",
    "<b>Ù…ÙˆØ§Ø±Ø¯ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ú©Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ø­Ø§ØµÙ„ Ù†Ø´ÙˆØ¯:</b><br>\n",
    "Û±. <b>Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ø§Ù„Ø§ Ø¨ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:</b> Ø§Ú¯Ø± Ø¨Ø±Ø®ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÚ©Ø±Ø§Ø±ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡Ù†Ø¯ (Ù…Ø«Ù„Ø§Ù‹ Ø·ÙˆÙ„ URL Ùˆ ØªØ¹Ø¯Ø§Ø¯ Ø§Ø³Ù„Ø´â€ŒÙ‡Ø§ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ø§ Ù‡Ù… Ù…Ø±ØªØ¨Ø· Ù‡Ø³ØªÙ†Ø¯)ØŒ Ø§ÙØ²ÙˆØ¯Ù† Ø¢Ù†â€ŒÙ‡Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª ØªØ£Ø«ÛŒØ± Ú†Ù†Ø¯Ø§Ù†ÛŒ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\n",
    "<br>\n",
    "Û². <b>Overfitting:</b> Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ú©Ù… Ø¨Ø§Ø´Ø¯ØŒ Ø§ÙØ²ÙˆØ¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ§Ø¯ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§Ø¹Ø« Ø¨ÛŒØ´â€ŒØ¨Ø±Ø§Ø²Ø´ Ø´ÙˆØ¯.\n",
    "<br>\n",
    "Û³. <b>Ù†ÙˆÛŒØ²:</b> Ø§Ú¯Ø± Ø¨Ø±Ø®ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒâ€ŒØ±Ø¨Ø· ÛŒØ§ Ù†ÙˆÛŒØ² Ø¨ÛŒØ´ØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯ØŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ú©Ø§Ù‡Ø´ Ø¯Ù‡Ù†Ø¯.\n",
    "<br><br>\n",
    "<b>Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ:</b><br>\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù…Ø³Ø¦Ù„Ù‡ØŒ Ø§Ù†ØªØ¸Ø§Ø± Ù…ÛŒâ€ŒØ±ÙˆØ¯ Ú©Ù‡ ØªØ±Ú©ÛŒØ¨ Û¹ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Û³ ÙˆÛŒÚ˜Ú¯ÛŒ ØªÙ†Ù‡Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ø¨Ù‡ Ø´Ø±Ø·ÛŒ Ú©Ù‡ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ… Ùˆ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ú©Ù…Ù„ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡Ù†Ø¯. Logistic Regression Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ù‡ØªØ± Ø§Ø² Naive Bayes Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ ÙˆØ§Ø¨Ø³ØªÙ‡ Ú©Ù†Ø§Ø± Ø¨ÛŒØ§ÛŒØ¯.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression as SkLogReg\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "try:\n",
    "    import tldextract\n",
    "    def extract_sub_len(host):\n",
    "        ext = tldextract.extract(host)\n",
    "        return len(ext.subdomain) if ext.subdomain else 0\n",
    "except ImportError:\n",
    "    def extract_sub_len(host):\n",
    "        parts = host.split('.')\n",
    "        if len(parts) > 2:\n",
    "            return sum(len(p) for p in parts[:-2])\n",
    "        return 0\n",
    "\n",
    "if 'urls_df' not in globals():\n",
    "    def load_url_dataset():\n",
    "        candidates = ['urls.csv', 'phishing.csv', 'phishing_urls.csv', 'url_dataset.csv']\n",
    "        df = None\n",
    "        for path in candidates:\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                print(f\"Loaded dataset: {path} | shape={df.shape}\")\n",
    "                break\n",
    "            except Exception:\n",
    "                continue\n",
    "        if df is None:\n",
    "            df = pd.DataFrame({\n",
    "                'url': [\n",
    "                    'http://example.com',\n",
    "                    'https://secure.paypal.com/login',\n",
    "                    'http://192.168.1.10/verify',\n",
    "                    'https://account-update-secure-login.com/pay',\n",
    "                    'http://my-bank-example.com/login?session=1234'\n",
    "                ],\n",
    "                'status': [0, 0, 1, 1, 1]\n",
    "            })\n",
    "            print(\"Using fallback URL dataset (demo mode).\")\n",
    "        if 'label' in df.columns and 'status' not in df.columns:\n",
    "            df = df.rename(columns={'label': 'status'})\n",
    "        if df['status'].dtype == object:\n",
    "            mapping = {\n",
    "                'phishing': 1, 'bad': 1, 'malicious': 1, '1': 1,\n",
    "                'legit': 0, 'benign': 0, 'good': 0, '0': 0\n",
    "            }\n",
    "            df['status'] = df['status'].astype(str).str.lower().map(mapping).fillna(0).astype(int)\n",
    "        return df[['url', 'status']]\n",
    "    urls_df = load_url_dataset()\n",
    "\n",
    "\n",
    "def structural_features(url: str):\n",
    "    p = urlparse(url)\n",
    "    host = p.netloc\n",
    "    path = p.path or ''\n",
    "    query = p.query or ''\n",
    "    host_len = len(host)\n",
    "    path_len = len(path)\n",
    "    url_len = len(url)\n",
    "    num_params = (query.count('&') + 1) if query else 0\n",
    "    has_fragment = int('#' in url)\n",
    "    digits_in_host = sum(ch.isdigit() for ch in host)\n",
    "    return {\n",
    "        'host_len': host_len,\n",
    "        'path_len': path_len,\n",
    "        'url_len': url_len,\n",
    "        'num_params': num_params,\n",
    "        'has_fragment': has_fragment,\n",
    "        'digits_in_host': digits_in_host\n",
    "    }\n",
    "\n",
    "\n",
    "def statistical_features(url: str):\n",
    "    length_url = len(url)\n",
    "    digits = sum(ch.isdigit() for ch in url)\n",
    "    ratio_digits_url = (digits / length_url) if length_url > 0 else 0.0\n",
    "    tokens = re.split(r'[^A-Za-z]+', url)\n",
    "    longest_words_raw = max((len(t) for t in tokens), default=0)\n",
    "    return {\n",
    "        'length_url': length_url,\n",
    "        'ratio_digits_url': ratio_digits_url,\n",
    "        'longest_words_raw': longest_words_raw,\n",
    "    }\n",
    "\n",
    "\n",
    "def creative_features(url: str):\n",
    "    parsed = urlparse(url)\n",
    "    host = parsed.netloc\n",
    "    path = parsed.path or ''\n",
    "    sub_len = extract_sub_len(host)\n",
    "    has_ip = bool(re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', host))\n",
    "    at_sign = '@' in url\n",
    "    dash_in_host = '-' in host\n",
    "    dot_count_host = host.count('.')\n",
    "    slash_count_path = path.count('/')\n",
    "    percent_encoding = '%' in url\n",
    "    suspicious_words = sum(w in url.lower() for w in ['verify', 'update', 'login', 'secure', 'account'])\n",
    "    has_https = parsed.scheme.lower() == 'https'\n",
    "    return {\n",
    "        'sub_len': sub_len,\n",
    "        'has_ip': int(has_ip),\n",
    "        'at_sign': int(at_sign),\n",
    "        'dash_in_host': int(dash_in_host),\n",
    "        'dot_count_host': dot_count_host,\n",
    "        'slash_count_path': slash_count_path,\n",
    "        'percent_encoding': int(percent_encoding),\n",
    "        'suspicious_words': suspicious_words,\n",
    "        'has_https': int(has_https)\n",
    "    }\n",
    "\n",
    "struct_df = urls_df['url'].apply(structural_features).apply(pd.Series)\n",
    "stat_df = urls_df['url'].apply(statistical_features).apply(pd.Series)\n",
    "cre_df = urls_df['url'].apply(creative_features).apply(pd.Series)\n",
    "\n",
    "all_df = pd.concat([struct_df, stat_df, cre_df], axis=1)\n",
    "all_df['status'] = urls_df['status'].values\n",
    "\n",
    "X = all_df.drop('status', axis=1).values\n",
    "y = all_df['status'].values\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Combined features sample:\")\n",
    "print(all_df.head())\n",
    "\n",
    "lr_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "    ('clf', SkLogReg(max_iter=1000))\n",
    "])\n",
    "lr_pipe.fit(X_tr, y_tr)\n",
    "y_pred_lr_all = lr_pipe.predict(X_te)\n",
    "\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_tr, y_tr)\n",
    "y_pred_nb_all = nb.predict(X_te)\n",
    "\n",
    "res_all = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score'],\n",
    "    'Logistic Regression': [\n",
    "        accuracy(y_te, y_pred_lr_all),\n",
    "        precision(y_te, y_pred_lr_all),\n",
    "        recall(y_te, y_pred_lr_all),\n",
    "        f1_score(y_te, y_pred_lr_all)\n",
    "    ],\n",
    "    'Naive Bayes (Gaussian)': [\n",
    "        accuracy(y_te, y_pred_nb_all),\n",
    "        precision(y_te, y_pred_nb_all),\n",
    "        recall(y_te, y_pred_nb_all),\n",
    "        f1_score(y_te, y_pred_nb_all)\n",
    "    ]\n",
    "})\n",
    "print(\"\\nResults (All Features Combined):\")\n",
    "print(res_all.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "# Extended URL Features Visualizations\n",
    "try:\n",
    "    # Distribution of selected structural features by class\n",
    "    feats = ['host_len','path_len','url_len','num_params','digits_in_host']\n",
    "    available = [f for f in feats if f in struct_df.columns]\n",
    "    n = len(available)\n",
    "    cols = 3\n",
    "    rows = (n + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 4*rows))\n",
    "    axes = axes.flatten()\n",
    "    for i, f in enumerate(available):\n",
    "        sns.boxplot(x=struct_df['status'], y=struct_df[f], ax=axes[i])\n",
    "        axes[i].set_title(f)\n",
    "        axes[i].set_xlabel('status')\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "    plt.tight_layout(); plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Pairplot on sampled combined features (to reduce overload)\n",
    "    sample = all_df.sample(min(300, len(all_df)), random_state=42)\n",
    "    sns.pairplot(sample.iloc[:, :6].assign(status=sample['status']), hue='status', corner=True)\n",
    "    plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Correlation top absolute features\n",
    "    corr = all_df.drop('status', axis=1).corr().abs().mean().sort_values(ascending=False)\n",
    "    top_feats = corr.head(10).index.tolist()\n",
    "    plt.figure(figsize=(8,4))\n",
    "    sns.barplot(x=corr.head(10).values, y=top_feats, palette='viridis')\n",
    "    plt.title('Mean Absolute Correlation (Top 10)')\n",
    "    plt.xlabel('Mean |Correlation|'); plt.ylabel('Feature')\n",
    "    plt.tight_layout(); plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # ROC curves (LR vs NB) for combined features\n",
    "    y_score_lr_all = lr_pipe.predict_proba(X_te)[:,1]\n",
    "    fpr_lr, tpr_lr, _ = roc_curve(y_te, y_score_lr_all)\n",
    "    roc_auc_lr = auc(fpr_lr, tpr_lr)\n",
    "    if hasattr(nb, 'predict_proba'):\n",
    "        y_score_nb_all = nb.predict_proba(X_te)[:,1]\n",
    "        fpr_nb, tpr_nb, _ = roc_curve(y_te, y_score_nb_all)\n",
    "        roc_auc_nb = auc(fpr_nb, tpr_nb)\n",
    "    else:\n",
    "        fpr_nb, tpr_nb, roc_auc_nb = None, None, None\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(fpr_lr, tpr_lr, label=f'LR AUC={roc_auc_lr:.3f}')\n",
    "    if fpr_nb is not None:\n",
    "        plt.plot(fpr_nb, tpr_nb, label=f'NB AUC={roc_auc_nb:.3f}')\n",
    "    plt.plot([0,1],[0,1],'--',color='gray')\n",
    "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curves (Combined Features)')\n",
    "    plt.legend(); plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Precision-Recall (LR only)\n",
    "    y_score_lr_all = lr_pipe.predict_proba(X_te)[:,1]\n",
    "    prec, rec, _ = precision_recall_curve(y_te, y_score_lr_all)\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(rec, prec)\n",
    "    plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall (LR Combined Features)')\n",
    "    plt.show()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Feature importance (LR coefficients after scaling)\n",
    "    if hasattr(lr_pipe.named_steps['clf'], 'coef_'):\n",
    "        coefs = lr_pipe.named_steps['clf'].coef_[0]\n",
    "        feature_names = all_df.drop('status', axis=1).columns\n",
    "        idx_pos = np.argsort(coefs)[-15:][::-1]\n",
    "        idx_neg = np.argsort(coefs)[:15]\n",
    "        fig, axes = plt.subplots(1,2, figsize=(14,6))\n",
    "        sns.barplot(x=coefs[idx_pos], y=feature_names[idx_pos], ax=axes[0], color='#2ecc71')\n",
    "        axes[0].set_title('Top Positive Coefficients')\n",
    "        sns.barplot(x=-coefs[idx_neg], y=feature_names[idx_neg], ax=axes[1], color='#e74c3c')\n",
    "        axes[1].set_title('Top Negative Coefficients')\n",
    "        plt.tight_layout(); plt.show()\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…: Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø®ÙˆØ¯ØªØ§Ù† Ø§Ø² Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯. Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ Ø¨Ø§ÛŒØ¯ \"Ø­Ø¯Ø§Ù‚Ù„\" Ø¨Ù‡ Ø³ÙˆØ§Ù„â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯:\n",
    "<br>\n",
    "- Ú©Ø¯Ø§Ù… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù†ÙˆØ¹ Ø¯Ø§Ø¯Ù‡ Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³ØªØŸ Ú†Ø±Ø§ØŸ\n",
    "<br>\n",
    "- Ú©Ø¯Ø§Ù… Ù†ÙˆØ¹ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªØ£Ø«ÛŒØ± Ø±Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³ØªØŸ Ú©Ø¯Ø§Ù… Ù†ÙˆØ¹ Ú©Ù…ØªØ±ÛŒÙ†ØŸ Ú†Ø±Ø§ØŸ\n",
    "<br>\n",
    "- Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†ÛŒØ§Ø² Ø¨ÙˆØ¯Ù‡ Ø§Ø³ØªØŸ Ø§Ú¯Ø± Ø¨Ù„Ù‡ØŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ØŸ Ú†Ø±Ø§ØŸ\n",
    "<br>\n",
    "- ØºÛŒØ± Ø§Ø² Ø§Ø³ØªØ®Ø±Ø§Ø­ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø² Ø®ÙˆØ¯ Ø¢Ø¯Ø±Ø³ Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒØŒ Ú†Ù‡ Ø±ÙˆÛŒÚ©Ø±Ø¯ Ø¯ÛŒÚ¯Ø±ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§ØªØ®Ø§Ø° Ù†Ù…ÙˆØ¯ØŸ\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"direction: rtl; text-align: right; background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111\">\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ Ø¨Ø®Ø´ Ù¾Ù†Ø¬Ù…:</b><br>\n",
    "<b>Û±. Ú©Ø¯Ø§Ù… Ù…Ø¯Ù„ Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ú©Ø±Ø¯Ù‡ Ø§Ø³ØªØŸ</b><br>\n",
    "Logistic Regression Ø¯Ø± ØªÙ…Ø§Ù… Ø¨Ø®Ø´â€ŒÙ‡Ø§ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Naive Bayes Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª. Ø§ÛŒÙ† Ø¨Ø±ØªØ±ÛŒ Ø¨Ù‡ Ø¯Ù„Ø§ÛŒÙ„ Ø²ÛŒØ± Ø§Ø³Øª:\n",
    "<br>\n",
    "- Logistic Regression Ù‚Ø§Ø¯Ø± Ø§Ø³Øª ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¨ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†Ø¯ØŒ Ø¯Ø± Ø­Ø§Ù„ÛŒ Ú©Ù‡ Naive Bayes ÙØ±Ø¶ Ø§Ø³ØªÙ‚Ù„Ø§Ù„ Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø¯Ø± Ø¹Ù…Ù„ Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ø±Ù‚Ø±Ø§Ø± Ù†ÛŒØ³Øª.\n",
    "<br>\n",
    "- Logistic Regression Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² gradient descentØŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØªØ±ÛŒ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ú©Ù‡ Ù…Ù†Ø¬Ø± Ø¨Ù‡ ØªØµÙ…ÛŒÙ…â€ŒÚ¯ÛŒØ±ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
    "<br>\n",
    "- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø± Logistic Regression Ø¨Ø§Ø¹Ø« Ø¨Ù‡Ø¨ÙˆØ¯ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ù‡ØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
    "<br><br>\n",
    "<b>Û². Ú©Ø¯Ø§Ù… Ù†ÙˆØ¹ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ùˆ Ú©Ù…ØªØ±ÛŒÙ† ØªØ£Ø«ÛŒØ± Ø±Ø§ Ø¯Ø§Ø´ØªØŸ</b><br>\n",
    "<b>Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªØ£Ø«ÛŒØ±:</b> ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡ (Ù…Ø§Ù†Ù†Ø¯ ÙˆØ¬ÙˆØ¯ IPØŒ ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ±Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§) Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªØ£Ø«ÛŒØ± Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯ØŒ Ø²ÛŒØ±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¨Ù‡ Ù†Ø´Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬ ÙÛŒØ´ÛŒÙ†Ú¯ Ø§Ø´Ø§Ø±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯. Ù‡Ù…Ú†Ù†ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ù…Ø§Ù†Ù†Ø¯ Ø·ÙˆÙ„ URL Ùˆ Ù†Ø³Ø¨Øª Ø§Ø±Ù‚Ø§Ù… Ù†ÛŒØ² Ù…Ø¤Ø«Ø± Ù‡Ø³ØªÙ†Ø¯.\n",
    "<br><br>\n",
    "<b>Ú©Ù…ØªØ±ÛŒÙ† ØªØ£Ø«ÛŒØ±:</b> Ø¨Ø±Ø®ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ù…Ø§Ù†Ù†Ø¯ ØªØ¹Ø¯Ø§Ø¯ Ø®Ø· ØªÛŒØ±Ù‡â€ŒÙ‡Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª ØªØ£Ø«ÛŒØ± Ú©Ù…ØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯ØŒ Ø²ÛŒØ±Ø§ Ù‡Ù… Ø¯Ø± Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ùˆ Ù‡Ù… Ø¯Ø± Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.\n",
    "<br><br>\n",
    "<b>Û³. Ø¢ÛŒØ§ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù„Ø§Ø²Ù… Ø¨ÙˆØ¯ØŸ</b><br>\n",
    "Ø¨Ù„Ù‡ØŒ Ø¨Ø±Ø§ÛŒ Logistic Regression Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª:\n",
    "<br>\n",
    "- Logistic Regression Ø¨Ù‡ Ù…Ù‚ÛŒØ§Ø³ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø­Ø³Ø§Ø³ Ø§Ø³Øª Ùˆ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§Ø¹Ø« Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ù‡ØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
    "<br>\n",
    "- Ø¨Ø±Ø§ÛŒ Naive Bayes (Ø¨Ù‡ Ø®ØµÙˆØµ GaussianNB) Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø§Ù„Ø²Ø§Ù…ÛŒ Ù†ÛŒØ³ØªØŒ Ø§Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ø¨Ø±Ø®ÛŒ Ù…ÙˆØ§Ø±Ø¯ Ù…ÙÛŒØ¯ Ø¨Ø§Ø´Ø¯.\n",
    "<br><br>\n",
    "<b>Û´. Ø±ÙˆÛŒÚ©Ø±Ø¯Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± ØºÛŒØ± Ø§Ø² Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§Ø² URL:</b><br>\n",
    "- <b>ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§ÛŒ ØµÙØ­Ù‡:</b> Ø¨Ø±Ø±Ø³ÛŒ HTMLØŒ Ù…ØªÙ†ØŒ ÙØ±Ù…â€ŒÙ‡Ø§ Ùˆ Ø§Ø³Ú©Ø±ÛŒÙ¾Øªâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ ØµÙØ­Ù‡ Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø´Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ (Ù…Ø§Ù†Ù†Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø­Ø³Ø§Ø³).\n",
    "<br>\n",
    "- <b>Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª WHOIS:</b> ØªØ§Ø±ÛŒØ® Ø«Ø¨Øª Ø¯Ø§Ù…Ù†Ù‡ØŒ Ø³Ù† Ø¯Ø§Ù…Ù†Ù‡ØŒ Ùˆ ØµØ§Ø­Ø¨ Ø¯Ø§Ù…Ù†Ù‡ - Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¯Ø§Ù…Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ§Ø²Ù‡ Ø«Ø¨Øªâ€ŒØ´Ø¯Ù‡ Ø¯Ø§Ø±Ù†Ø¯.\n",
    "<br>\n",
    "- <b>ØªØ­Ù„ÛŒÙ„ Ú¯ÙˆØ§Ù‡ÛŒ SSL:</b> Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø¹ØªØ¨Ø§Ø± Ú¯ÙˆØ§Ù‡ÛŒ HTTPSØŒ Ù†ÙˆØ¹ Ú¯ÙˆØ§Ù‡ÛŒØŒ Ùˆ Ù…Ø±Ø¬Ø¹ ØµØ§Ø¯Ø±Ú©Ù†Ù†Ø¯Ù‡.\n",
    "<br>\n",
    "- <b>Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù„ÛŒØ³Øª Ø³ÛŒØ§Ù‡ (Blacklist):</b> Ù…Ù‚Ø§ÛŒØ³Ù‡ URL Ø¨Ø§ Ù¾Ø§ÛŒÚ¯Ø§Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù†Ø§Ø®ØªÙ‡â€ŒØ´Ø¯Ù‡ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ÙÛŒØ´ÛŒÙ†Ú¯.\n",
    "<br>\n",
    "- <b>ØªØ­Ù„ÛŒÙ„ Ø±ÙØªØ§Ø± Ú©Ø§Ø±Ø¨Ø±:</b> Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ú©Ù„ÛŒÚ©ØŒ Ø²Ù…Ø§Ù† Ù…Ø§Ù†Ø¯Ù† Ø¯Ø± ØµÙØ­Ù‡ØŒ Ùˆ ØªØ¹Ø§Ù…Ù„Ø§Øª Ú©Ø§Ø±Ø¨Ø±.\n",
    "<br>\n",
    "- <b>ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚:</b> Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ùˆ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø¨Ø¯ÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø³ØªÛŒ.\n",
    "<br>\n",
    "- <b>ØªØ­Ù„ÛŒÙ„ Ø´Ø¨Ø§Ù‡Øª Ø¨ØµØ±ÛŒ:</b> Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¸Ø§Ù‡Ø± ØµÙØ­Ù‡ Ø¨Ø§ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¬Ø¹Ù„ Ù‡ÙˆÛŒØª.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_title"
   },
   "source": [
    "# <h1 style=\"text-align: right;\">**Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ùˆ Ù‚ÙˆØ§Ù†ÛŒÙ† ØªØ­ÙˆÛŒÙ„**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_body"
   },
   "source": [
    "\n",
    "\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ÙØ±Ù…Øª Ø²ÛŒØ± Ù†Ø§Ù…Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆØ¯: <code>NLP_CA{n}_{LASTNAME}_{STUDENTID}.ipynb</code></h4>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">Ù†Ø­ÙˆÙ‡ Ø§Ù†Ø¬Ø§Ù… ØªÙ…Ø±ÛŒÙ†:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "  <li>Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø¯ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ <code>WRITE YOUR CODE HERE</code> Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ Ú©Ù†ÛŒØ¯.</li>\n",
    "  <li>Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒØŒ Ù…ØªÙ† <code>{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}</code> Ø±Ø§ Ø¨Ø§ Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯.</li>\n",
    "</ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>Ù…Ø§ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´Ø®ØµÛŒ Ø§Ø² Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ø®ÙˆØ§Ù‡ÛŒÙ… Ú©Ø±Ø¯. Ø§ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ú©Ù‡ Ú©Ø¯ÛŒ Ú©Ù‡ Ù†ÙˆØ´ØªÛŒØ¯ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø± Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ ØµØ­ÛŒØ­ Ø±Ø§ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø®ÙˆØ¯ Ø¨Ø¯ÙˆÙ† Ú©Ø¯ÛŒ Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¢Ù† Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯ ØªØ­ÙˆÛŒÙ„ Ø¯Ù‡ÛŒØ¯ØŒ Ø§ÛŒÙ† ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> <li>Ù…Ø§ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³Ø±Ù‚Øª Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÙ„Ø¨ Ø§Ù†Ø¬Ø§Ù… Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø¯. Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ø¯ Ø§Ø² Ø¯ÛŒÚ¯Ø±Ø§Ù† Ù†ÛŒØ² ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> </ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØªÙˆØ¶ÛŒØ­Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ùˆ Ø¯Ù‚Øª Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø± Ø§Ø³Øª. Ø¨Ù‡ ØªÙ…Ø±ÛŒÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§ØºØ°ÛŒ ØªØ­ÙˆÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ ÛŒØ§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ú©Ø³ Ø¯Ø± Ø³Ø§ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆÙ†Ø¯ØŒ ØªØ±ØªÛŒØ¨ Ø§Ø«Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.</li>\n",
    "<li>\n",
    " Ù‡Ù…Ù‡â€ŒÛŒ Ú©Ø¯Ù‡Ø§ÛŒ Ù¾ÛŒÙˆØ³Øª Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø§ÛŒØ³ØªÛŒ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯. Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ Ù…Ø¬Ø¯Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø®Ø§ØµÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ Ø¨Ø§ÛŒØ³ØªÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ù†ÛŒØ² Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ø®ÙˆØ¯ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯.  Ø¯Ù‚Øª Ú©Ù†ÛŒØ¯ Ú©Ù‡  ØªÙ…Ø§Ù…ÛŒ Ú©Ø¯Ù‡Ø§ Ø¨Ø§ÛŒØ¯ ØªÙˆØ³Ø· Ø´Ù…Ø§ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ú©Ø¯Ù‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ø¨Ø§Ø´Ø¯. Ø¨Ù‡ Ú©Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ù†Ø¨Ø§Ø´Ø¯ Ù†Ù…Ø±Ù‡â€ŒØ§ÛŒ ØªØ¹Ù„Ù‚ Ù†Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
    "</li>\n",
    "<li>ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª ØªÚ©â€ŒÙ†ÙØ±Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ Ùˆ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ÛŒØ¯ Ù†ØªÛŒØ¬Ù‡ ÙØ¹Ø§Ù„ÛŒØª ÙØ±Ø¯ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ (Ù‡Ù…ÙÚ©Ø±ÛŒ Ùˆ Ø¨Ù‡ Ø§ØªÙØ§Ù‚ Ù‡Ù… Ù†ÙˆØ´ØªÙ† ØªÙ…Ø±ÛŒÙ† Ù†ÛŒØ² Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³Øª). Ø¯Ø± ØµÙˆØ±Øª Ù…Ø´Ø§Ù‡Ø¯Ù‡\n",
    " ØªØ´Ø§Ø¨Ù‡ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø§ÙØ±Ø§Ø¯ Ù…Ø´Ø§Ø±Ú©Øªâ€ŒÚ©Ù†Ù†Ø¯Ù‡ØŒ Ù†Ù…Ø±Ù‡ ØªÙ…Ø±ÛŒÙ† ØµÙØ± Ùˆ Ø¨Ù‡ Ø§Ø³ØªØ§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.\n",
    " </li>\n",
    "\n",
    " <li>\n",
    "Ù„Ø·ÙØ§Ù‹ ØªÙ…Ø§Ù…ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ <b>ÙÙˆÙ†Øª ÙˆØ²ÛŒØ± (Vazir)</b> Ùˆ Ø¨Ù‡â€ŒØµÙˆØ±Øª <b>Ø±Ø§Ø³Øªâ€ŒÚ†ÛŒÙ†</b> Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙÙˆÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§ Ø¸Ø§Ù‡Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ ÛŒÚ©â€ŒØ¯Ø³Øª Ùˆ Ø®ÙˆØ§Ù†Ø§ Ø¨Ø§Ø´Ø¯.  \n",
    "Ø¯Ø± Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ ØªØ´Ø±ÛŒØ­ÛŒØŒ Ø³Ø¹ÛŒ Ú©Ù†ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ Ú©Ø§Ù…Ù„ØŒ Ù…Ù†Ø³Ø¬Ù… Ùˆ Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª Ù†Ú¯Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ù‡ Ú†ÛŒÙ†Ø´ ØªÙ…ÛŒØ² Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø±Ø³Øª Ú©Ø¯Ù‡Ø§ ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ ØªÙ…Ø±ÛŒÙ† Ø´Ù…Ø§ Ø¨Ø§ ÙØ±Ù…Øª Ø®ÙˆØ§Ø³ØªÙ‡â€ŒØ´Ø¯Ù‡ Ùˆ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø§Ø±Ø§Ø¦Ù‡ Ø´ÙˆØ¯.\n",
    "</li>\n",
    " <li>Ø¨Ø±Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø¨ÛŒØ´ØªØ± Ø¯Ø±Ø¨Ø§Ø±Ù‡â€ŒÛŒ ÙØ±Ù…Øª Markdown Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">Ø§ÛŒÙ† Ù„ÛŒÙ†Ú©</a> Ù…Ø·Ø§Ù„Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n",
    " </li>\n",
    " </ul>\n",
    "    \n",
    "\n",
    " </div>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "section2_title",
    "section3_title",
    "section4_title",
    "eval_title",
    "policies_title"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
