{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cover_header",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"text-align: center; padding: 20px; font-family: Vazir;\">\n",
    "<h1 align=\"center\" style=\"font-size: 28px; color:rgb(64, 244, 202); width: 100%;\">โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ<br>ุชูุฑู 1<br>โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ</h1>\n",
    "<h2 dir='rtl' style=\"color:rgb(90, 255, 184); font-size: 20px;\">ุขุดูุง ุจุง ุชูฺฉูุงุฒุฑูุง ู N-gram</h2>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">ุดูุฑุฒุงุฏ ุขุฐุฑ ุขุฒุงุฏ - ูุฑุดุงุฏ ุญุณุงู</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">shahrzad.azari@ut.ac.ir - farshad.hessami@ut.ac.ir</p>\n",
    "\n",
    "<div dir='rtl' style=\"border: 2px dashed rgb(90, 255, 184); border-radius: 8px; padding: 20px; margin: 20px auto; max-width: 500px; text-align: right;\">\n",
    "<p dir='rtl' style=\"color: rgb(64, 244, 202); font-size: 18px; margin-bottom: 15px;\">๐ ูุดุฎุตุงุช ุฏุงูุดุฌู:</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">ูุงู ู ูุงู ุฎุงููุงุฏฺฏ: {{ูุงู_ุฏุงูุดุฌู}}</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">ุดูุงุฑู ุฏุงูุดุฌู: {{ุดูุงุฑู_ุฏุงูุดุฌู}}</p>\n",
    "<p dir='rtl' style=\"color: #666; margin: 5px;\">ุชุงุฑุฎ ุงุฑุณุงู: {{ุชุงุฑุฎ_ุงุฑุณุงู}}</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; padding: 25px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir; max-width: 100%;word-wrap: break-word;\">\n",
    "<div style=\"line-height: 2.0; font-size: 17px; color: black; font-family: Vazir;\">\n",
    "<div style=\"padding-right:40px\">\n",
    "ุฏุฑ ุจุฎุดโูุง ูุฎุชูู ุงู ุชูุฑู ุจุง ููุงูู Tokenization, Regular Expression , N-gram Language Modeling ุขุดูุง ูโุดูุฏ ู ุขูโูุง ุฑุง ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ. \n",
    "</div>\n",
    "<br>\n",
    "<div style=\"padding-right:100px\">\n",
    "๐ <b>ุณุงุฎุชุงุฑ ุชูุฑู:</b>\n",
    "<li><b>ุณูุงู ุงูู - <span dir=\"ltr\">Regular Expression & Min Distance</span> (20)</b></li>\n",
    "<ul>\n",
    "<li>ุจุฎุด ุงูู: ุชุดุฎุต ุงููโูุง ูุงุจู ูุจูู ุจุง Regex</li>\n",
    "<li>ุจุฎุด ุฏูู: ูพุงุฏูโุณุงุฒ Auto-Correction ุจุง Minimum Edit Distance</li>\n",
    "</ul>\n",
    "<li><b>ุณูุงู ุฏูู - <span dir=\"ltr\">Tokenization</span> (25)</b></li>\n",
    "<ul>\n",
    "<li>ุจุฎุด ุงูู: Rule-based Tokenizer</li>\n",
    "<li>ุจุฎุด ุฏูู: BPE Tokenizer</li>\n",
    "<li>ุจุฎุด ุณูู: Wordpiece Tokenizer</li>\n",
    "<li>ุจุฎุด ฺูุงุฑู: Tokenization Visualization</li>\n",
    "</ul>\n",
    "<li><b>ุณูุงู ุณูู - <span dir=\"ltr\">N-gram Language Modeling</span> (55)</b></li>\n",
    "<ul>\n",
    "<li>ุจุฎุด ุงูู: Data cleaning & Tokenization</li>\n",
    "<li>ุจุฎุด ุฏูู: ูพุงุฏูโุณุงุฒ N-gram</li>\n",
    "<li>ุจุฎุด ุณูู: ูุนุงุฑ Perplexity</li>\n",
    "<li>ุจุฎุด ฺูุงุฑู: ุฑูุดโูุง ูููุงุฑุณุงุฒ</li>\n",
    "<li>ุจุฎุด ูพูุฌู: ุดุจูโุณุงุฒ Temperature ุจุง ุฑูุดโูุง ูููุงุฑุณุงุฒ</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "<div dir='rtl' style=\"line-height: 1.8; font-family: Vazir; font-size: 16px; margin-top: 20px; background-color: #e8eaf6; padding: 15px; border-radius: 8px; color:black\">\n",
    "๐ก <b>ูฺฉุงุช ููู:</b>\n",
    "<br>\n",
    "ุฏุฑ ูุชู ุณูุงูุงุชุ ุจุฎุดโูุง ฺฉู ุฏุฑ ุขูโูุง ูุฌุงุฒ ุจู ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจุฎุงููโูุง ุขูุงุฏู ูุณุชุฏ ุฐฺฉุฑ ุดุฏู ุงุณุช.\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ<br>ุณูุงู ุงูู - <span dir=\"ltr\">Regular Expression & Min Distance</span> (20)<br>โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "ุฏุฑ ุงู ุณูุงู ุดูุง ุจุง ูููููโูุง ุนูู ุงุฒ ุงุณุชูุงุฏูโ Regex ู ูููุทูุฑ Minimum Distance ููุงุฌู ูโุดูุฏ. ุฏุฑ ุจุฎุด ุงูู ุงู ุณูุงูุ ุดูุง ุจุงุฏ ุงููโูุง ูุงุจูโูุจูู ุฑุง ุชุดุฎุต ุฏุงุฏู ู ุฏุฑ ุจุฎุด ุฏูู ุณูุงู ุดูุง ุจุง ุงุณุชูุงุฏู ุงุฒ Minimum Distance ฺฉ ุณุณุชู Auto-Correction ุณุงุฏู ุฑุง ูพุงุฏูโุณุงุฒ ุฎูุงูุฏ ฺฉุฑุฏ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุชุดุฎุต ุงููโูุง ูุงุจู ูุจูู ุจุง Regex</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ูุงู emails.txt ฺฉู ุฏุฑ ุงุฎุชุงุฑ ุดูุง ูุฑุงุฑ ุฏุงุฏูโุดุฏูุ ุดุงูู ุชุนุฏุงุฏ ุงุณู ุจูโููุฑุงู ุงูู ุซุจุชโุดุฏูโโุดุงู ุฏุฑ ฺฉ ุณุงูุงูู ูโุจุงุดุฏ.\n",
    "<br>\n",
    "ุงุฒ ุดูุง ุฎูุงุณุชูโุดุฏู ุงุณุช ุชุง ุงููโูุง ฺฉู ูุนุชุจุฑ ูุณุชูุฏ ุฑุง ุจุง ุงุณุชูุงุฏู ุงุฒ regex ูุดุฎุต ฺฉูุฏ.\n",
    "<br>\n",
    "ุงูู ุงุฒ ุฏู ุจุฎุด ุชุดฺฉู ูโุดูุฏ. ฺฉู ุจุง @ ุงุฒ ูู ุฌุฏุง ูโุดููุฏ. ุจุฎุด ุงูู (ูุจู ุงุฒ @) local-part ูุงู ุฏุงุฑุฏ ู ุจุฎุด ุฏูู domain.\n",
    "<br>\n",
    "ููุธูุฑ ุงุฒ ุงูู ูุนุชุจุฑ ุงู ุงุณุช ฺฉู ููุงุฑุฏ ุฒุฑ ุฏุฑ ุขูโูุง ุฑุนุงุช ุดุฏูโุจุงุดูุฏ:\n",
    "<br>\n",
    "ฑ. ุฏู ุจุฎุด ุงูู ุจุง ฺฉ ู ุชููุง ฺฉ @ ุงุฒ ูู ุฌุฏุง ุดุฏูโุจุงุดูุฏ.\n",
    "<br>\n",
    "ฒ. ุฏุฑ local-part ูู ูุงู ู ูู ูุงู ุฎุงููุงุฏฺฏ ุดุฎุต ูุฌูุฏ ุฏุงุดุชู ุจุงุดุฏ.\n",
    "<br>\n",
    "ณ. ุฏุฑ local-part ุชููุง ุญุฑูู ุงูฺฏูุณุ ุงุนุฏุงุฏ ู ฺฉุงุฑุงฺฉุชุฑูุง -ุ_ ู . ูุฌุงุฒ ูุณุชูุฏ. ููฺูู ุฏู ููุทู ููโุชูุงููุฏ ูพุดุช ูู ุจุงูุฏ.\n",
    "<br>\n",
    "ด. ุฏุฑ ุจุฎุด domain ฺฉ ูุฒุจุงู ุฏุงุฑู ู ฺฉ ูพุณููุฏ. ูุฒุจุงู ู ูพุณููุฏ ููุดู ุจุง ฺฉ ููุทู ุงุฒ ฺฉุฏฺฏุฑ ุฌุฏุง ูโุดููุฏ. (ูุฒุจุงู ูโุชูุงูุฏ ุฏุฑ ุฎูุฏ ููุทู ุฏุงุดุชู ุจุงุดุฏุ ุงูุง ุฏู ููุทูโ ูุชูุงู ุฏุฑ domain ูุฌุงุฒ ูุณุช.)\n",
    "<br>\n",
    "ต. ูพุณููุฏ ุงุฒ ุญุฑูู ุงูฺฏูุณ ุชุดฺฉู ูโุดูุฏ ู ุญุฏุงูู ุฏู ฺฉุงุฑุงฺฉุชุฑ ุฏุงุฑุฏ.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ูุณุช ุงููโูุง ูุงุจู ูุจูู ููุฌูุฏ ุฏุฑ ูุงู emails.txt\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('emails.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "email_pattern = r'^[a-zA-Z0-9]+[._-][a-zA-Z0-9]+(?:[._-][a-zA-Z0-9]+)*@(?:[a-zA-Z0-9]+\\.)+[a-zA-Z]{2,}$'\n",
    "\n",
    "valid_emails = []\n",
    "print(\"Valid emails:\\n\")\n",
    "for line in lines:\n",
    "    name_match = re.search(r'name=(.+?),', line)\n",
    "    name = name_match.group(1) if name_match else ''\n",
    "    match = re.search(r'email=([^\\s]+)', line.strip())\n",
    "    if match:\n",
    "        email = match.group(1)\n",
    "        if re.match(email_pattern, email) and '..' not in email:\n",
    "            valid_emails.append(email)\n",
    "            print(f\"{name:25s} -> {email}\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Total rows: {len(lines)}\")\n",
    "print(f\"Valid emails: {len(valid_emails)}\")\n",
    "print(\"\\nList of valid emails:\")\n",
    "for i, email in enumerate(valid_emails, 1):\n",
    "    print(f\"{i}. {email}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุฏุฑ ุงู ุจุฎุด ุจุงุฏ ุจุง ุงุณุชูุงุฏู ุงุฒ Regex ฺฉ ุชุงุจุน ุจููุณุฏ ฺฉู ุงุณู ฺฉ ุดุฎุต ุฑุง ุจูโุนููุงู ูุฑูุฏ ุฏุฑุงูุช ฺฉูุฏ ู ุฏุฑุตูุฑุช ูุฌูุฏ ุงูู ูุนุชุจุฑ ุจุฑุง ุงู ุงุณู ุฏุฑ ุจู ุงููโูุง ุซุจุชโุดุฏูุ ุงูู ุฑุง ุจุฑฺฏุฑุฏุงูุฏ. ุฏุฑ ุบุฑ ุงูโุตูุฑุชุ ฺฉ ูพุบุงู ุนุฏู ูุฌูุฏ ฺุงูพ ฺฉูุฏ.\n",
    "<br>\n",
    "ุชูุฌู: ููฺฉู ุงุณุช ุงูู ุงู ุงุดุฎุงุตุ ุชุญุช ูุงู ุดุฎุต ุฏฺฏุฑ ุซุจุช ุดุฏู ุจุงุดุฏ. ฺฉุงุฑ ุดูุง ุงู ุงุณุช ฺฉู ุงูู ูุนุชุจุฑ ุงู ุงูุฑุงุฏ ุฑุง ุงุฒ ุจู ุชูุงู ุงููโูุง ูพุฏุง ฺฉูุฏ.\n",
    "<br>\n",
    "ุชุงุจุน ุฑุง ุจุง ูุฑูุฏโูุง ุฒุฑ ุงุฌุฑุง ฺฉูุฏ:\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "name1 = Behnam Khatibi\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "name2 = Mehrdad Ebrahimi\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ุงููโูุง ูุนุชุจุฑ ูุฑุจูุท ุจู ุงูุฑุงุฏ ุฐฺฉุฑ ุดุฏู ุฏุฑ ุชูุถุญุงุช\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_email_by_name(name):\n",
    "    with open('emails.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    parts = name.lower().split()\n",
    "    if len(parts) < 2:\n",
    "        print(f\"Name '{name}' must include first and last name\")\n",
    "        return []\n",
    "    first_name, last_name = parts[0], parts[1]\n",
    "    email_pattern = r'^[a-zA-Z0-9]+[._-][a-zA-Z0-9]+(?:[._-][a-zA-Z0-9]+)*@(?:[a-zA-Z0-9]+\\.)+[a-zA-Z]{2,}$'\n",
    "    found = []\n",
    "    for line in lines:\n",
    "        email_match = re.search(r'email=([^\\s]+)', line.strip())\n",
    "        if email_match:\n",
    "            email = email_match.group(1)\n",
    "            if re.match(email_pattern, email) and '..' not in email:\n",
    "                local = email.split('@')[0].lower()\n",
    "                patterns = [\n",
    "                    f'{first_name}.*{last_name}',\n",
    "                    f'{last_name}.*{first_name}',\n",
    "                    f'{first_name}[._-]{last_name}',\n",
    "                    f'{last_name}[._-]{first_name}'\n",
    "                ]\n",
    "                for p in patterns:\n",
    "                    if re.search(p, local):\n",
    "                        found.append((email, line))\n",
    "                        break\n",
    "    if found:\n",
    "        print(f\"Valid email(s) for '{name}':\")\n",
    "        for email, line in found:\n",
    "            nm = re.search(r'name=(.+?),', line)\n",
    "            registered_name = nm.group(1) if nm else ''\n",
    "            print(f\"  {email} (registered to: {registered_name})\")\n",
    "    else:\n",
    "        print(f\"No valid email found for '{name}'\")\n",
    "    return found\n",
    "\n",
    "print(\"Searching...\\n\")\n",
    "name1 = \"Behnam Khatibi\"\n",
    "print(f\"Query: {name1}\")\n",
    "find_email_by_name(name1)\n",
    "print()\n",
    "name2 = \"Mehrdad Ebrahimi\"\n",
    "print(f\"Query: {name2}\")\n",
    "find_email_by_name(name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ูพุงุฏูโุณุงุฒ Auto-Correction ุจุง Minimum Edit Distance</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ฑ. ุงุจุชุฏุง ุงูฺฏูุฑุชู levenshtein_distance ุฑุง ูพุงุฏูโุณุงุฒ ฺฉูุฏ ู ุจุง ุงุณุชูุงุฏู ุงุฒ ุขู minimum_distance ุจู ฺฉููุงุช ุฒุฑ ุฑุง ุจูโุฏุณุช ุขูุฑุฏ.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair1 = \"Athletic\", \"Atlantic\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair2 = \"London\", \"Boston\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair3 = \"Action\", \"Compact\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "pair3 = \"\", \"Sting\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ููุฏุงุฑ minimum distance ุจู ุฌูุช ฺฉูููโูุง ุฏุงุฏูโุดุฏู ุฏุฑ ุจุฎุด ุชูุถุญุงุช\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(str1, str2):\n",
    "    m, n = len(str1), len(str2)\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    for i in range(m + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        dp[0][j] = j\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if str1[i - 1] == str2[j - 1]:\n",
    "                dp[i][j] = dp[i - 1][j - 1]\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(\n",
    "                    dp[i - 1][j],\n",
    "                    dp[i][j - 1],\n",
    "                    dp[i - 1][j - 1]\n",
    "                )\n",
    "    return dp[m][n]\n",
    "\n",
    "test_pairs = [\n",
    "    (\"Athletic\", \"Atlantic\"),\n",
    "    (\"London\", \"Boston\"),\n",
    "    (\"Action\", \"Compact\"),\n",
    "    (\"\", \"Sting\")\n",
    "]\n",
    "\n",
    "print(\"Minimum Edit Distance (Levenshtein)\\n\")\n",
    "for a, b in test_pairs:\n",
    "    d = levenshtein_distance(a, b)\n",
    "    print(f\"'{a}' vs '{b}' -> {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ฒ. ุจุนุฏ ุงุฒ ูพุงุฏูโุณุงุฒ minimum distance ุญุงู ุจุงุฏ ุจุง ุงุณุชูุงุฏู ุงุฒ ุขูุ ุฌูููโ ุฒุฑ ุฑุง ุงุตูุงุญ ุงููุง ฺฉูุฏ. ุฏุฑ ุงู ุฌููู ุชุนุฏุงุฏ ฺฉููู ูุฌูุฏ ุฏุงุฑูุฏ ฺฉู ุงููุงุดุงู ูุงุฏุฑุณุช ุงุณุช. ฺฉ ูุณุช ุงุฒ ุงููุง ุตุญุญ ฺฉููุงุช ฺฉู ฺฉููุงุช ุงู ุฌููู ุฑุง ูุฒ ุดุงูู ูโุดููุฏ ุฏุฑ ูุงู vocab.txt ููุฌูุฏ ูุณุชูุฏ.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "sentence_to_be_corrected = \n",
    "\"helo studnts at the universty are wrting ther frst edit distnce algorthm in pythn, and they reely enjy it!\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ุงุตูุงุญโุดุฏูโ ุฌูููโ ุฏุงุฏูโุดุฏู ุฏุฑ ุจุฎุด ุชูุถุญุงุช ุจุง ฺฉูฺฉ ูุงู vocab.txt\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open('vocab.txt', 'r', encoding='utf-8') as f:\n",
    "    vocab_text = f.read()\n",
    "    vocab = [w.strip() for w in vocab_text.split(',')]\n",
    "\n",
    "print(f\"Vocab size: {len(vocab)}\")\n",
    "print(f\"Sample: {vocab[:10]}\")\n",
    "\n",
    "\n",
    "def auto_correct(word, vocabulary):\n",
    "    min_distance = float('inf')\n",
    "    best_match = word\n",
    "    lw = word.lower()\n",
    "    for vw in vocabulary:\n",
    "        d = levenshtein_distance(lw, vw.lower())\n",
    "        if d < min_distance:\n",
    "            min_distance = d\n",
    "            best_match = vw\n",
    "    return best_match, min_distance\n",
    "\n",
    "\n",
    "def correct_sentence(sentence, vocabulary):\n",
    "    words = re.findall(r'\\b\\w+\\b', sentence)\n",
    "    corrected_words = []\n",
    "    corrections_made = []\n",
    "    vocab_lower = {v.lower() for v in vocabulary}\n",
    "    for w in words:\n",
    "        if w.lower() in vocab_lower:\n",
    "            corrected_words.append(w)\n",
    "        else:\n",
    "            cw, dist = auto_correct(w, vocabulary)\n",
    "            corrected_words.append(cw)\n",
    "            if w != cw:\n",
    "                corrections_made.append((w, cw, dist))\n",
    "    return ' '.join(corrected_words), corrections_made\n",
    "\n",
    "sentence_to_be_corrected = \"helo studnts at the universty are wrting ther frst edit distnce algorthm in pythn, and they reely enjy it!\"\n",
    "\n",
    "print(\"\\nAuto-correction\\n\")\n",
    "print(f\"Original:\\n{sentence_to_be_corrected}\")\n",
    "corrected_sentence, corrections = correct_sentence(sentence_to_be_corrected, vocab)\n",
    "print(f\"\\nCorrected:\\n{corrected_sentence}\")\n",
    "print(\"\\nCorrections:\")\n",
    "if corrections:\n",
    "    for original, corrected, distance in corrections:\n",
    "        print(f\"  '{original}' -> '{corrected}' (distance={distance})\")\n",
    "else:\n",
    "    print(\"  None\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_title"
   },
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ<br>ุณูุงู ุฏูู - <span dir=\"ltr\">Tokenization</span> (25)<br>โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "ุฏุฑ ุงู ุณูุงูุ ุจุง ุงููุงุน ูุฎุชูู ุชูฺฉูุงุฒุฑ ู ุฑูุด  ูพุงุฏูโุณุงุฒ ุขูโูุง ุขุดูุง ูโุดูุฏ.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Rule-based Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุจุง ฺฉูฺฉ ุฏุณุชูุฑุงุช regex ฺฉ ุชูฺฉูุงุฒุฑ ุจููุณุฏ ฺฉู ูุชู ุฑุง ุจุง ุงุณุชูุงุฏู ุงุฒ ุนูุงุฆู ูฺฏุงุฑุด ุชูุณู ฺฉูุฏ.\n",
    "<br>\n",
    "ุณูพุณ ุนููฺฉุฑุฏ ุขู ุฑุง ุจุฑ ุฑู ุฌููุงุช ุฏุงุฏู ุดุฏู ุฒุฑ ุขุฒูุงุด ฺฉูุฏ.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"Hello, world! NLP is fun.\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"That U.S.A. poster-print costs $12.40...\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ุชุนุฏุงุฏ ู ูุณุช ุชูฺฉูโูุง ุงุฌุงุฏุดุฏู ุจุง ุชูฺฉูุงุฒุฑ ุตูุฑุช ุณูุงู ุจุฑุง ูุฑ ุฌููู<br>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def rule_based_tokenizer(text):\n",
    "    return re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "\n",
    "sentences = [\n",
    "    \"Hello, world! NLP is fun.\",\n",
    "    \"That U.S.A. poster-print costs $12.40...\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Rule-based Tokenizer\\n\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\")\n",
    "    tokens = rule_based_tokenizer(sentence)\n",
    "    print(f\"Count: {len(tokens)}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    print(\"-\" * 70)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุงุฑุงุฏุงุช ุงู ุชูฺฉูุงุฒุฑ ฺุณุชุ\n",
    "<br>\n",
    "ฺูุฏ ุฑุงูโุญู ุจุฑุง ุจูุจูุฏ ุนููฺฉุฑุฏ ุงู ุชูฺฉูุงุฒุฑ ุงุฑุงุฆู ุฏูุฏ.\n",
    "<br>\n",
    "ฺฉ ุงุฒ ุขูโูุง ุฑุง ูพุงุฏูโุณุงุฒ ฺฉูุฏ.\n",
    "<br>\n",
    "ุนููฺฉุฑุฏ ุชูฺฉูุงุฒุฑ ุฌุฏุฏ ุฑุง ุจุฑ ุฑู ุฌููุงุช ุฏุงุฏู ุดุฏู ุขุฒูุงุด ฺฉูุฏ.\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"Hello, world! NLP is fun.\"\n",
    "</span>\n",
    "<span dir=\"ltr\" style=\"text-align: left; display: block;\">\n",
    "\"That U.S.A. poster-print costs $12.40...\"\n",
    "</span>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ุชุนุฏุงุฏ ู ูุณุช ุชูฺฉูโูุง ุงุฌุงุฏุดุฏู ุจุง ุชูฺฉูุงุฒุฑ ุจูุจูุฏุงูุชู ุจุฑุง ูุฑ ุฌููู\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def improved_tokenizer(text):\n",
    "    patterns = [\n",
    "        r'\\$\\d+\\.\\d+',\n",
    "        r'\\d+\\.\\d+',\n",
    "        r'\\b[A-Z](?:\\.[A-Z])+\\.?',\n",
    "        r'\\w+(?:-\\w+)+',\n",
    "        r'\\w+',\n",
    "        r'\\.{2,}',\n",
    "        r'[^\\w\\s]'\n",
    "    ]\n",
    "    combined_pattern = '|'.join(patterns)\n",
    "    return re.findall(combined_pattern, text)\n",
    "\n",
    "sentences = [\n",
    "    \"Hello, world! NLP is fun.\",\n",
    "    \"That U.S.A. poster-print costs $12.40...\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Improved Rule-based Tokenizer\\n\")\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\")\n",
    "    simple_tokens = rule_based_tokenizer(sentence)\n",
    "    print(f\"Simple tokenizer - count: {len(simple_tokens)}\")\n",
    "    print(f\"Tokens: {simple_tokens}\")\n",
    "    improved_tokens = improved_tokenizer(sentence)\n",
    "    print(f\"\\nImproved tokenizer - count: {len(improved_tokens)}\")\n",
    "    print(f\"Tokens: {improved_tokens}\")\n",
    "    print(\"-\" * 70)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_answer_1"
   },
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุฒุฑุจุฎุด ุงูู:</b><br>\n",
    "\n",
    "<b>ุงุฑุงุฏุงุช ุชูฺฉูุงุฒุฑ ุณุงุฏู:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ุงุฎุชุตุงุฑุงุช:</b> ฺฉููุงุช ูุงููุฏ U.S.A. ุจู ุชูฺฉูโูุง ุฌุฏุงฺฏุงูู (U, ., S, ., A, .) ุชูุณู ูโุดููุฏุ ุฏุฑ ุญุงู ฺฉู ุจุงุฏ ฺฉ ูุงุญุฏ ุจุงุดูุฏ</li>\n",
    "<li><b>ฺฉููุงุช ุชุฑฺฉุจ:</b> ฺฉููุงุช ูุงููุฏ poster-print ุจู ุฏู ฺฉููู ุฌุฏุงฺฏุงูู ุชูุณู ูโุดููุฏ</li>\n",
    "<li><b>ุงุนุฏุงุฏ:</b> ุงุนุฏุงุฏ ุงุนุดุงุฑ ู ูุจุงูุบ ูพูู ($12.40) ุจู ฺูุฏ ุชูฺฉู ุชูุณู ูโุดููุฏ</li>\n",
    "<li><b>ุนูุงุฆู ุชฺฉุฑุงุฑ:</b> ุณู ููุทู (...) ุจู ุณู ุชูฺฉู ุฌุฏุงฺฏุงูู ุชุจุฏู ูโุดูุฏ</li>\n",
    "</ul>\n",
    "\n",
    "<b>ุฑุงูโุญูโูุง ูพุดููุงุฏ:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ุงุณุชูุงุฏู ุงุฒ ุงูฺฏููุง ูพุดุฑูุชูโุชุฑ regex:</b> ุชุนุฑู ุงูฺฏููุง ุฎุงุต ุจุฑุง ุงุฎุชุตุงุฑุงุชุ ฺฉููุงุช ุชุฑฺฉุจุ ู ุงุนุฏุงุฏ</li>\n",
    "<li><b>ูพุดโูพุฑุฏุงุฒุด:</b> ุดูุงุณุง ู ุฌุงฺฏุฒู ููุงุฑุฏ ุฎุงุต ูุจู ุงุฒ ุชูฺฉูุงุฒ</li>\n",
    "<li><b>ุงุณุชูุงุฏู ุงุฒ ูุณุช ุงุณุชุซูุงุฆุงุช:</b> ูฺฏูโุฏุงุฑ ูุณุช ุงุฒ ฺฉููุงุช ู ุงูฺฏููุง ุฎุงุต</li>\n",
    "<li><b>ุชูฺฉูุงุฒุฑูุง ุขูุงุฏู:</b> ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจุฎุงููโูุง ูุงููุฏ NLTK, spaCy ฺฉู ุงู ููุงุฑุฏ ุฑุง ูุฏุฑุช ูโฺฉููุฏ</li>\n",
    "</ul>\n",
    "\n",
    "<b>ูพุงุฏูโุณุงุฒ ุดุฏู:</b> ุชูฺฉูุงุฒุฑ ุจูุจูุฏ ุงูุชู ุจุง ุงุณุชูุงุฏู ุงุฒ ุงูฺฏููุง regex ูพุดุฑูุชู ฺฉู ููุงุฑุฏ ุจุงูุง ุฑุง ูุฏุฑุช ูโฺฉูุฏ.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">BPE Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุงุจุชุฏุง ุฏุชุงุณุช TinyStories-Farsi ุฑุง ุงุฒ HuggingFace ููุฏ ฺฉูุฏ.\n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    (ููฺฉ ุฏุชุงุณุช)\n",
    "</a>\n",
    "<br>\n",
    "ุณูพุณ ุงุฒ ุฏุงุฏู ุขููุฒุด (train) ุขู ุฌููุงุช ูุงุฑุณ ุฑุง ุฏุฑ ฺฉ ูุณุช ุงุถุงูู ฺฉูุฏ.\n",
    "<br>\n",
    "ุงฺฉููู ุจุง ุงุณุชูุงุฏู ุงุฒ ุงู ูุฌููุนู ุฏุงุฏูุ ฺฉ ุชูฺฉูุงุฒุฑ BPE ุขููุฒุด ุฏูุฏ. ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจุฎุงููโูุง ุขูุงุฏู ูุงูุน ูุฏุงุฑุฏ.\n",
    "<br>\n",
    "ุนููฺฉุฑุฏ ุชูฺฉูุงุฒุฑ ุฑุง ุฑู ุฌููู ุฒุฑ ุขุฒูุงุด ฺฉูุฏ:\n",
    "<br>\n",
    "\"ุฑูุฒ ฺฉ ูุฑุฏ ุซุฑูุชููุฏุ ูพุณุฑ ุจฺู ฺฉูฺฺฉุด ุฑุง ุจูู ุฏู ุจุฑุฏ ุชุง ุจูู ุงู ูุดุงู ุฏูุฏ ูุฑุฏู ฺฉู ุฏุฑ ุขูุฌุง ุฒูุฏฺฏ ูโฺฉููุฏุ ฺูุฏุฑ ููุฑ ูุณุชูุฏ.\"\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ูุฌููุนู ุฏุงุฏู ุขููุฒุด<br>\n",
    "- ุชุนุฏุงุฏ ู ูุณุช ุชูฺฉูโูุง ุงุฌุงุฏุดุฏู ุจุฑุง ุฌููู\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "print(\"Loading TinyStories-Farsi train split...\")\n",
    "dataset = load_dataset(\"taesiri/TinyStories-Farsi\", split='train')\n",
    "print(\"Loaded.\")\n",
    "print(f\"Samples: {len(dataset)}\")\n",
    "\n",
    "print(\"Collecting sentences...\")\n",
    "sentences = []\n",
    "for i, item in enumerate(dataset):\n",
    "    if i >= 10000:\n",
    "        break\n",
    "    if 'story' in item:\n",
    "        sentences.append(item['story'])\n",
    "print(f\"Collected: {len(sentences)}\")\n",
    "print(f\"Example: {sentences[0][:100]}...\")\n",
    "\n",
    "print(\"Training BPE tokenizer...\")\n",
    "tokenizer_bpe = Tokenizer(BPE(unk_token=\"<unk>\"))\n",
    "tokenizer_bpe.pre_tokenizer = Whitespace()\n",
    "trainer = BpeTrainer(vocab_size=5000, special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\"], min_frequency=2)\n",
    "tokenizer_bpe.train_from_iterator(sentences, trainer=trainer)\n",
    "print(\"BPE trained.\")\n",
    "print(f\"Vocab size: {tokenizer_bpe.get_vocab_size()}\")\n",
    "\n",
    "test_sentence = \"ุฑูุฒ ฺฉ ูุฑุฏ ุซุฑูุชููุฏุ ูพุณุฑ ุจฺู ฺฉูฺฺฉุด ุฑุง ุจูู ุฏู ุจุฑุฏ ุชุง ุจูู ุงู ูุดุงู ุฏูุฏ ูุฑุฏู ฺฉู ุฏุฑ ุขูุฌุง ุฒูุฏฺฏ ูโฺฉููุฏุ ฺูุฏุฑ ููุฑ ูุณุชูุฏ.\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BPE Tokenizer Test\")\n",
    "print(f\"Sentence: {test_sentence}\")\n",
    "output = tokenizer_bpe.encode(test_sentence)\n",
    "tokens = output.tokens\n",
    "print(f\"Count: {len(tokens)}\")\n",
    "print(\"Tokens:\")\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f\"  {i+1}. {token}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tokenizer_bpe.save(\"bpe_tokenizer_farsi.json\")\n",
    "print(\"Saved: bpe_tokenizer_farsi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Wordpiece Tokenizer</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุฏุฑุจุงุฑู Wordpiece Tokenizer ุชุญูู ฺฉูุฏ.\n",
    "<br>\n",
    "ูุญูู ุขููุฒุด ุงู ุชูฺฉูุงุฒุฑ ุฑุง ุจู ุทูุฑ ุฏูู ุดุฑุญ ุฏูุฏ ู ุณูพุณ ุขู ุฑุง ุจุง BPE ููุงุณู ฺฉูุฏ.\n",
    "<br>\n",
    "ุงฺฉููู ฺฉ ุชูฺฉูุงุฒุฑ Wordpiece ุจุฑ ุฑู ูุฌููุนู ุฏุงุฏู ุฎูุฏ ุขููุฒุด ุฏูุฏ. ุงุณุชูุงุฏู ุงุฒ ฺฉุชุงุจุฎุงููโูุง ุขูุงุฏู ูุงูุน ูุฏุงุฑุฏ.\n",
    "<br>\n",
    "ุนููฺฉุฑุฏ ุชูฺฉูุงุฒุฑ ุฑุง ุฑู ุฌููู ุฒุฑ ุขุฒูุงุด ฺฉูุฏ:\n",
    "<br>\n",
    "\"ุฑูุฒ ฺฉ ูุฑุฏ ุซุฑูุชููุฏุ ูพุณุฑ ุจฺู ฺฉูฺฺฉุด ุฑุง ุจูู ุฏู ุจุฑุฏ ุชุง ุจูู ุงู ูุดุงู ุฏูุฏ ูุฑุฏู ฺฉู ุฏุฑ ุขูุฌุง ุฒูุฏฺฏ ูโฺฉููุฏุ ฺูุฏุฑ ููุฑ ูุณุชูุฏ.\"\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ุชุนุฏุงุฏ ู ูุณุช ุชูฺฉูโูุง ุงุฌุงุฏุดุฏู ุจุฑุง ุฌููู\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "print(\"Training WordPiece tokenizer...\")\n",
    "tokenizer_wp = Tokenizer(WordPiece(unk_token=\"<unk>\"))\n",
    "tokenizer_wp.pre_tokenizer = Whitespace()\n",
    "trainer_wp = WordPieceTrainer(vocab_size=5000, special_tokens=[\"<unk>\", \"<s>\", \"</s>\", \"<pad>\"], min_frequency=2)\n",
    "tokenizer_wp.train_from_iterator(sentences, trainer=trainer_wp)\n",
    "print(\"WordPiece trained.\")\n",
    "print(f\"Vocab size: {tokenizer_wp.get_vocab_size()}\")\n",
    "\n",
    "test_sentence = \"ุฑูุฒ ฺฉ ูุฑุฏ ุซุฑูุชููุฏุ ูพุณุฑ ุจฺู ฺฉูฺฺฉุด ุฑุง ุจูู ุฏู ุจุฑุฏ ุชุง ุจูู ุงู ูุดุงู ุฏูุฏ ูุฑุฏู ฺฉู ุฏุฑ ุขูุฌุง ุฒูุฏฺฏ ูโฺฉููุฏุ ฺูุฏุฑ ููุฑ ูุณุชูุฏ.\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"WordPiece Tokenizer Test\")\n",
    "print(f\"Sentence: {test_sentence}\")\n",
    "output_wp = tokenizer_wp.encode(test_sentence)\n",
    "tokens_wp = output_wp.tokens\n",
    "print(f\"Count: {len(tokens_wp)}\")\n",
    "print(\"Tokens:\")\n",
    "for i, token in enumerate(tokens_wp):\n",
    "    print(f\"  {i+1}. {token}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "tokenizer_wp.save(\"wordpiece_tokenizer_farsi.json\")\n",
    "print(\"Saved: wordpiece_tokenizer_farsi.json\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BPE vs WordPiece\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"BPE tokens: {len(tokens)}\")\n",
    "print(f\"WordPiece tokens: {len(tokens_wp)}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุฒุฑุจุฎุด ุณูู:</b><br>\n",
    "\n",
    "<b>ุชูุถุญ WordPiece Tokenizer:</b>\n",
    "\n",
    "<b>ฑ. ูุญูู ุขููุฒุด:</b>\n",
    "<ul dir='rtl'>\n",
    "<li>WordPiece ูุดุงุจู BPE ุนูู ูโฺฉูุฏ ุงูุง ุชูุงูุช ฺฉูุฏ ุฏุฑ ูุนุงุฑ ุชุฑฺฉุจ ุฏุงุฑุฏ</li>\n",
    "<li>ุจู ุฌุง ุงุณุชูุงุฏู ุงุฒ ูุฑฺฉุงูุณุ ุงุฒ <b>likelihood</b> ุงุณุชูุงุฏู ูโฺฉูุฏ</li>\n",
    "<li>ุฏุฑ ูุฑ ูุฑุญููุ ุฌูุช ุงุฒ subword ูุง ฺฉู ุจุดุชุฑู ุงูุฒุงุด likelihood ุฑุง ุงุฌุงุฏ ูโฺฉููุฏุ ุชุฑฺฉุจ ูโุดููุฏ</li>\n",
    "<li>ูุฑููู: score(x,y) = P(xy) / (P(x) ร P(y))</li>\n",
    "</ul>\n",
    "\n",
    "<b>ฒ. ููุงุณู ุจุง BPE:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>BPE:</b> ุจุฑ ุงุณุงุณ ูุฑฺฉุงูุณ - ุฌูุชโูุง ฺฉู ุจุดุชุฑู ุชฺฉุฑุงุฑ ุฑุง ุฏุงุฑูุฏ ุชุฑฺฉุจ ูโุดููุฏ</li>\n",
    "<li><b>WordPiece:</b> ุจุฑ ุงุณุงุณ likelihood - ุฌูุชโูุง ฺฉู ุจุดุชุฑู ุงุทูุงุนุงุช ุฑุง ุงุฑุงุฆู ูโุฏููุฏ ุชุฑฺฉุจ ูโุดููุฏ</li>\n",
    "<li><b>ุนููฺฉุฑุฏ:</b> WordPiece ูุนูููุงู ูุชุงุฌ ุจูุชุฑ ุฏุฑ ูุฏูโูุง ุฒุจุงู ุฏุงุฑุฏ (ูุซูุงู ุฏุฑ BERT)</li>\n",
    "<li><b>ุงุณุชูุงุฏู:</b> BPE ุฏุฑ GPT ู WordPiece ุฏุฑ BERT ุงุณุชูุงุฏู ูโุดูุฏ</li>\n",
    "<li><b>ูุดุงููโฺฏุฐุงุฑ:</b> WordPiece ุงุฒ ## ุจุฑุง ูุดุงู ุฏุงุฏู ุงุฏุงูู ฺฉููู ุงุณุชูุงุฏู ูโฺฉูุฏ</li>\n",
    "</ul>\n",
    "\n",
    "<b>ณ. ูุฒุงุง:</b>\n",
    "- ฺฉูุช ุจูุชุฑ ุฏุฑ ุชุดุฎุต ูุนูุง subword ูุง\n",
    "- ุนููฺฉุฑุฏ ุจูุชุฑ ุฏุฑ ูุธุงู downstream\n",
    "- ูุฏุฑุช ุจูุชุฑ ฺฉููุงุช ูุงุฏุฑ\n",
    "\n",
    "<b>ด. ูุดุงูุฏุงุช:</b>\n",
    "ูุฑ ุฏู ุฑูุด ุชูุงูุง ูุฏุฑุช ฺฉููุงุช ุฎุงุฑุฌ ุงุฒ ูุงฺฺฏุงู ุฑุง ุฏุงุฑูุฏ ู ุจุง ุชูุณู ฺฉููุงุช ุจู subword ูุง ฺฉูฺฺฉโุชุฑุ ูโุชูุงููุฏ ุจุง ฺฉููุงุช ุฌุฏุฏ ุจุฑุฎูุฑุฏ ฺฉููุฏ.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Tokenization Visualization</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุจุง ุงุณุชูุงุฏู ุงุฒ ุงุจุฒุงุฑ \n",
    "<a href=\"https://tiktokenizer.vercel.app/\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    tiktokenizer\n",
    "</a>\n",
    "ุชูฺฉูโูุง ุชููุฏ ุดุฏู ุจุฑุง ุฌููู ุฒุฑ ุฑุง ุฏุฑ ูุฑ ฺฉ ุงุฒ ูุฏูโูุง gpt2 ู gpt4 ู Meta-Llama-3-8B ุฑุง ูุดุงูุฏู ู ุชูุงูุชโูุง ุฑุง ฺฏุฒุงุฑุด ฺฉูุฏ.\n",
    "<br>\n",
    "\"ุฑูุฒ ฺฉ ูุฑุฏ ุซุฑูุชููุฏุ ูพุณุฑ ุจฺู ฺฉูฺฺฉุด ุฑุง ุจูู ุฏู ุจุฑุฏ ุชุง ุจูู ุงู ูุดุงู ุฏูุฏ ูุฑุฏู ฺฉู ุฏุฑ ุขูุฌุง ุฒูุฏฺฏ ูโฺฉููุฏุ ฺูุฏุฑ ููุฑ ูุณุชูุฏ.\"\n",
    "<br>\n",
    "ุณูพุณ ุฏุฑ ููุฑุฏ ุชูฺฉูุงุฒุฑ ุงุณุชูุงุฏู ุดุฏู ุฏุฑ ูุฑ ฺฉ ุชุญูู ฺฉูุฏ. ุจู ูุธุฑ ุดูุง ุนูุช ุชูุงูุช ูุชุฌู ุขูโูุง ุฏุฑ ฺุณุชุ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ุชูฺฉูโูุง ุงุฌุงุฏุดุฏู ุจุง ูุฑ ุชูฺฉูุงุฒุฑ ุจุฑุง ุฌููู\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุฒุฑุจุฎุด ฺูุงุฑู:</b><br>\n",
    "\n",
    "<b>ูุชุงุฌ ุชูฺฉูุงุฒุดู ุฌููู ูุงุฑุณ ุฏุฑ ูุฏูโูุง ูุฎุชูู:</b>\n",
    "\n",
    "<b>ฑ. GPT-2:</b>\n",
    "- ุชุนุฏุงุฏ ุชูฺฉูโูุง: ุจุณุงุฑ ุฒุงุฏ (ูุนูููุงู 150-200+ ุชูฺฉู)\n",
    "- ูุดฺฉู: GPT-2 ุจุฑุง ุฒุจุงู ุงูฺฏูุณ ุขููุฒุด ุฏุฏู ู ูุงุฑุณ ุฑุง ููโุดูุงุณุฏ\n",
    "- ูุฑ ฺฉุงุฑุงฺฉุชุฑ ูุงุฑุณ ุจู ุตูุฑุช byte-level encoding ูพุฑุฏุงุฒุด ูโุดูุฏ\n",
    "- ูุชุฌู: ุชูฺฉูโูุง ุจุณุงุฑ ฺฉูฺฺฉ ู ูุงููููู\n",
    "\n",
    "<b>ฒ. GPT-4:</b>\n",
    "- ุชุนุฏุงุฏ ุชูฺฉูโูุง: ูุชูุณุท (ูุนูููุงู 40-60 ุชูฺฉู)\n",
    "- ุจูุจูุฏ ูุงุจู ุชูุฌู ูุณุจุช ุจู GPT-2\n",
    "- ูพุดุชุจุงู ุจูุชุฑ ุงุฒ ุฒุจุงูโูุง ุบุฑุงูฺฏูุณ\n",
    "- ุชูฺฉูโูุง ูุนูุงุฏุงุฑุชุฑ ุงูุง ูููุฒ ูู ุจููู ุจุฑุง ูุงุฑุณ\n",
    "\n",
    "<b>ณ. Meta-Llama-3-8B:</b>\n",
    "- ุชุนุฏุงุฏ ุชูฺฉูโูุง: ฺฉูุชุฑ (ูุนูููุงู 30-50 ุชูฺฉู)\n",
    "- ุจูุชุฑู ุนููฺฉุฑุฏ ุฏุฑ ูุงู ุณู ูุฏู\n",
    "- ุขููุฒุด ุจุง ุฏุงุฏูโูุง ฺูุฏุฒุจุงูู ุจุดุชุฑ\n",
    "- ุชูฺฉูโูุง ุจุฒุฑฺฏโุชุฑ ู ูุนูุงุฏุงุฑุชุฑ ุจุฑุง ูุงุฑุณ\n",
    "\n",
    "<b>ุนูุช ุชูุงูุชโูุง:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ุฏุงุฏูโูุง ุขููุฒุด:</b> GPT-2 ุนูุฏุชุงู ุงูฺฏูุณุ GPT-4 ู Llama-3 ฺูุฏุฒุจุงููโุชุฑ</li>\n",
    "<li><b>ุงูุฏุงุฒู ูุงฺฺฏุงู:</b> ูุฏูโูุง ุฌุฏุฏุชุฑ ูุงฺฺฏุงู ุจุฒุฑฺฏโุชุฑ ู ูุชููุนโุชุฑ ุฏุงุฑูุฏ</li>\n",
    "<li><b>ูุนูุงุฑ ุชูฺฉูุงุฒุฑ:</b> ูุฏูโูุง ุฌุฏุฏ ุงุฒ byte-level BPE ุจูุจูุฏ ุงูุชู ุงุณุชูุงุฏู ูโฺฉููุฏ</li>\n",
    "<li><b>ูพุดุชุจุงู ฺูุฏุฒุจุงูู:</b> Llama-3 ุจู ุตูุฑุช ุฎุงุต ุจุฑุง ฺูุฏุฒุจุงูู ุจูุฏู ุทุฑุงุญ ุดุฏู</li>\n",
    "</ul>\n",
    "\n",
    "<b>ูุชุฌูโฺฏุฑ:</b>\n",
    "ุจุฑุง ฺฉุงุฑ ุจุง ูุชูู ูุงุฑุณุ ุงุณุชูุงุฏู ุงุฒ ุชูฺฉูุงุฒุฑูุง ุงุฎุชุตุงุต ฺฉู ุฑู ุฏุงุฏูโูุง ูุงุฑุณ ุขููุฒุด ุฏุฏูโุงูุฏ (ูุงููุฏ BPE ู WordPiece ฺฉู ุฏุฑ ุจุฎุดโูุง ูุจู ูพุงุฏูโุณุงุฒ ฺฉุฑุฏู) ุจุณุงุฑ ฺฉุงุฑุขูุฏุชุฑ ุงุณุช.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ<br>ุณูุงู ุณูู - <span dir=\"ltr\">N-gram Language Modeling</span> (55)<br>โ๏ธโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโโ๏ธ</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "ุฏุฑ ุงู ุณูุงูุ ุจุง N-gram Language Modeling ู ุขู ุฑุง ูพุงุฏูโุณุงุฒ ูโฺฉูุฏุ ุจุง ุงุณุชูุงุฏู ุงุฒ ุขู ุจู ุชููุฏ ูุชู ูโูพุฑุฏุงุฒุฏ. ุณูพุณ ุจุง ูุนุงุฑ perplexity ู ูุญูู ฺฉุงุฑุจุฑุฏ ุขู ุขุดูุง ูโุดูุฏ. ุฏุฑ ููุงุช ุงูฺฏูุฑุชูโูุง smoothing ู ุจุง ฺฉุงุฑุจุฑุฏูุง ุขู ุขุดูุง ูโุดูุฏ. ุฑุง ูพุงุฏูโุณุงุฒ ูโฺฉูุฏ\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Data cleaning & Tokenization</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ูุฌููุนู ุฏุงุฏู \n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    TinyStories-Farsi\n",
    "</a>\n",
    "- ฺฉู ุฏุฑ ุณูุงู ุงูู ูุฒ ุงุฒ ุขู ุงุณุชูุงุฏู ฺฉุฑุฏุฏ - ุฑุง ููุฏ ฺฉูุฏ.\n",
    "<br>\n",
    "ุงุจุชุฏุง ุฏุงุฏฺฏุงู ูุงุฑุณ ุฏุฑ ูุฌููุนู ุขููุฒุด (train) ุขู ุฑุง ุชูุฒ ฺฉุฑุฏู ู ูพุดโูพุฑุฏุงุฒุดโูุง ููุฑุฏ ูุงุฒ ุฑุง ุจุฑ ุฑู ุขู ุงูุฌุงู ุฏูุฏ. (ุจุง ุจุฑุฑุณ ุฏุงุฏูโูุงุ ูุดุฎุตโฺฉูุฏ ฺฉู ุงู ุฏุงุฏฺฏุงู ุจู ฺู ูพุดโูพุฑุฏุงุฒุดโูุง ูุงุฒ ุฏุงุฑูุฏ.)\n",
    "<br>\n",
    "ุณูพุณ ุจุง ุงุณุชูุงุฏู ุงุฒ BPE Tokenizer ฺฉู ุจุฑ ุฑู ุงู ุฏุงุฏฺฏุงู ุขููุฒุดโุฏุงุฏูโุงุฏุ ุฏุงุฏฺฏุงู ูพุฑุฏุงุฒุดโุดุฏู ุฑุง ุชูฺฉูุงุฒ ฺฉูุฏ.\n",
    "<br>\n",
    "ุชูฺฉูโูุง ฺฉ ุฌููู ุฑุง ุจู ุงูุชุฎุงุจ ุฎูุฏุ ฺุงูพ ฺฉูุฏ.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b>\n",
    "<br>\n",
    "- ูุฌููุนู ุฏุงุฏู ุชูุฒุดุฏู ูุงุฑุณ\n",
    "<br>\n",
    "- ูุฌููุนู ุฏุงุฏู ุชูฺฉูุงุฒ ุดุฏู ูุงุฑุณ\n",
    "<br>\n",
    "- ุชูฺฉูโูุง ฺฉ ุฌูููโ ุฏูุฎูุงู\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading TinyStories-Farsi train split...\")\n",
    "dataset_train = load_dataset(\"taesiri/TinyStories-Farsi\", split='train')\n",
    "print(\"Loaded.\")\n",
    "print(f\"Train samples: {len(dataset_train)}\")\n",
    "\n",
    "print(\"Collecting and cleaning texts...\")\n",
    "raw_texts = []\n",
    "for i, item in enumerate(dataset_train):\n",
    "    if i >= 15000:\n",
    "        break\n",
    "    if 'story' in item and item['story']:\n",
    "        raw_texts.append(item['story'])\n",
    "\n",
    "print(f\"Collected: {len(raw_texts)}\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text if text else None\n",
    "\n",
    "cleaned_texts = []\n",
    "for text in raw_texts:\n",
    "    cleaned = clean_text(text)\n",
    "    if cleaned:\n",
    "        cleaned_texts.append(cleaned)\n",
    "\n",
    "print(f\"Cleaned: {len(cleaned_texts)}\")\n",
    "print(f\"Sample: {cleaned_texts[0][:150]}...\")\n",
    "\n",
    "print(\"Tokenizing with BPE...\")\n",
    "tokenized_texts = []\n",
    "for text in cleaned_texts:\n",
    "    output = tokenizer_bpe.encode(text)\n",
    "    tokens = output.tokens\n",
    "    tokenized_texts.append(tokens)\n",
    "\n",
    "print(f\"Tokenized documents: {len(tokenized_texts)}\")\n",
    "\n",
    "sample_idx = 5\n",
    "print(\"=\" * 70)\n",
    "print(\"Sample tokens (first 20)\")\n",
    "print(f\"Text: {cleaned_texts[sample_idx][:200]}...\")\n",
    "for i, token in enumerate(tokenized_texts[sample_idx][:20]):\n",
    "    print(f\"  {i+1}. {token}\")\n",
    "print(f\"... and {len(tokenized_texts[sample_idx]) - 20} more\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"Data ready for N-gram.\")\n",
    "print(f\"Total tokens: {sum(len(t) for t in tokenized_texts):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุฒุฑุจุฎุด ุงูู:</b><br>\n",
    "\n",
    "<b>ูพุดโูพุฑุฏุงุฒุดโูุง ุงูุฌุงู ุดุฏู:</b>\n",
    "\n",
    "<ul dir='rtl'>\n",
    "<li><b>ุญุฐู ฺฉุงุฑุงฺฉุชุฑูุง ฺฉูุชุฑู:</b> ฺฉุงุฑุงฺฉุชุฑูุง ุบุฑูุงุจู ฺุงูพ ู ฺฉูุชุฑู ฺฉู ููฺฉู ุงุณุช ุฏุฑ ูุชู ุจุงุดูุฏ ุญุฐู ุดุฏูุฏ</li>\n",
    "\n",
    "<li><b>ูุฑูุงูโุณุงุฒ ูุงุตููโูุง:</b> ูุงุตููโูุง ูุชูุงู (ุฏู ุง ฺูุฏ ูุงุตูู ูพุดุช ุณุฑ ูู) ุจู ฺฉ ูุงุตูู ุชุจุฏู ุดุฏูุฏ</li>\n",
    "\n",
    "<li><b>ุญุฐู ูุงุตููโูุง ุงุถุงู:</b> ูุงุตููโูุง ุงุจุชุฏุง ู ุงูุชูุง ูุชู ุญุฐู ุดุฏูุฏ</li>\n",
    "\n",
    "<li><b>ุญุฐู ูุชูู ุฎุงู:</b> ุฎุทูุท ุฎุงู ุง ูุชูู ุจุฏูู ูุญุชูุง ุญุฐู ุดุฏูุฏ</li>\n",
    "\n",
    "<li><b>ุงุณุชูุงุฏู ุงุฒ BPE:</b> ุจุฑุง ุชูฺฉูุงุฒ ุงุฒ ุชูฺฉูุงุฒุฑ BPE ฺฉู ูุจูุงู ุขููุฒุด ุฏุงุฏู ุดุฏู ุงุณุชูุงุฏู ฺฉุฑุฏู</li>\n",
    "</ul>\n",
    "\n",
    "<b>ุฏูู ุงูุชุฎุงุจ ุงู ูพุดโูพุฑุฏุงุฒุดโูุง:</b>\n",
    "- ุฏุงุฏูโูุง TinyStories-Farsi ูุณุจุชุงู ุชูุฒ ูุณุชูุฏ\n",
    "- ุงุฒ ุขูุฌุง ฺฉู ุงุฒ BPE ุงุณุชูุงุฏู ูโฺฉููุ ูุงุฒ ุจู stemming ุง lemmatization ูุณุช\n",
    "- BPE ุฎูุฏุด ุจุง ฺฉููุงุช ูุฎุชูู ุจู ุฎูุจ ุจุฑุฎูุฑุฏ ูโฺฉูุฏ\n",
    "- ุชูุฑฺฉุฒ ูุง ุจุฑ ุฑู ุญูุธ ุณุงุฎุชุงุฑ ุทุจุน ุฒุจุงู ุงุณุช\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ูพุงุฏูโุณุงุฒ N-gram</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ฺฉ ฺฉูุงุณ ุจุฑุง ุณุงุฎุช ู ุขููุฒุด N-gram ุจููุณุฏ.\n",
    "<br>\n",
    "ุจุง ุงุณุชูุงุฏู ุงุฒ ุงู ฺฉูุงุณ ู ูุฌููุนู ุฏุงุฏู ุชูฺฉูุงุฒ ุดุฏู ฺฉู ุฏุฑ ุจุฎุด ูุจู ุขูุงุฏู ฺฉุฑุฏุฏุ \n",
    "<span dir=\"ltr\"> 2-gram, 4-gram, 8-gram</span>\n",
    "ุจุณุงุฒุฏ ู ุฑู ูุฌููุนู ุฏุงุฏู ุฎูุฏ ุขููุฒุด ุฏูุฏ.\n",
    "<br>\n",
    "ุจุง ุงุณุชูุงุฏู ุงุฒ ูุฏูโูุง ุขููุฒุด ุฏุงุฏู ุดุฏูุ ูุชูโูุง 100 ุชูฺฉู ุชููุฏ ฺฉูุฏ ู ฺฉูุช ูุชูู ุชููุฏุดุฏู ุฑุง ุจุง ูู ููุงุณู ฺฉูุฏ ู ุชูุงูุช ุนููฺฉุฑุฏ ูุฏูโูุง ุงุฒ ูุธุฑ ูพูุณุชฺฏ ู ุฑูุงู ูุชูู ุฑุง ุชุญูู ฺฉูุฏ.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ูุฏูโูุง N-gram ุขููุฒุด ุงูุชู\n",
    "<br>\n",
    "- ูุชูโูุง 100 ุชูฺฉู ุชููุฏุดุฏู ุจุง ูุฑ N-gram\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class NGramModel:\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.ngram_counts = defaultdict(Counter)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        self.vocab = set()\n",
    "        \n",
    "    def train(self, tokenized_texts):\n",
    "        print(f\"Training {self.n}-gram...\")\n",
    "        for tokens in tokenized_texts:\n",
    "            tokens = ['<s>'] * (self.n - 1) + tokens + ['</s>']\n",
    "            self.vocab.update(tokens)\n",
    "            for i in range(len(tokens) - self.n + 1):\n",
    "                context = tuple(tokens[i:i+self.n-1])\n",
    "                next_word = tokens[i+self.n-1]\n",
    "                self.ngram_counts[context][next_word] += 1\n",
    "                self.context_counts[context] += 1\n",
    "        print(\"Done.\")\n",
    "        print(f\"Vocab size: {len(self.vocab)}\")\n",
    "        print(f\"Unique contexts: {len(self.context_counts)}\")\n",
    "        \n",
    "    def get_probability(self, context, word):\n",
    "        context = tuple(context)\n",
    "        if self.context_counts[context] == 0:\n",
    "            return 0\n",
    "        return self.ngram_counts[context][word] / self.context_counts[context]\n",
    "    \n",
    "    def generate_next_word(self, context):\n",
    "        context = tuple(context)\n",
    "        if context not in self.ngram_counts or len(self.ngram_counts[context]) == 0:\n",
    "            return random.choice(list(self.vocab))\n",
    "        possible_words = list(self.ngram_counts[context].keys())\n",
    "        counts = [self.ngram_counts[context][w] for w in possible_words]\n",
    "        total = sum(counts)\n",
    "        probabilities = [c / total for c in counts]\n",
    "        next_word = np.random.choice(possible_words, p=probabilities)\n",
    "        return next_word\n",
    "    \n",
    "    def generate_text(self, length=100, start_tokens=None):\n",
    "        if start_tokens is None:\n",
    "            current_context = ['<s>'] * (self.n - 1)\n",
    "        else:\n",
    "            current_context = start_tokens[-(self.n-1):]\n",
    "        generated = list(current_context)\n",
    "        for _ in range(length):\n",
    "            next_word = self.generate_next_word(current_context)\n",
    "            if next_word == '</s>':\n",
    "                break\n",
    "            generated.append(next_word)\n",
    "            current_context = generated[-(self.n-1):]\n",
    "        generated = [t for t in generated if t != '<s>']\n",
    "        return generated\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Training N-gram models\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_2gram = NGramModel(n=2)\n",
    "model_2gram.train(tokenized_texts)\n",
    "\n",
    "model_4gram = NGramModel(n=4)\n",
    "model_4gram.train(tokenized_texts)\n",
    "\n",
    "model_8gram = NGramModel(n=8)\n",
    "model_8gram.train(tokenized_texts)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Text generation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "models = [\n",
    "    (model_2gram, \"2-gram\"),\n",
    "    (model_4gram, \"4-gram\"),\n",
    "    (model_8gram, \"8-gram\")\n",
    "]\n",
    "\n",
    "for model, name in models:\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Generated with {name}:\")\n",
    "    print(\"-\"*70)\n",
    "    generated_tokens = model.generate_text(length=100)\n",
    "    generated_text = ' '.join(generated_tokens)\n",
    "    print(generated_text)\n",
    "    print(f\"\\nToken count: {len(generated_tokens)}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "print(\"Models ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุฒุฑุจุฎุด ุฏูู:</b><br>\n",
    "\n",
    "<b>ุชุญูู ู ููุงุณู ูุฏูโูุง N-gram:</b>\n",
    "\n",
    "<b>ฑ. ูุฏู 2-gram:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ูพูุณุชฺฏ:</b> ุถุนู - ุฌููุงุช ูุนูููุงู ูุนูุง ฺฉู ูุฏุงุฑูุฏ</li>\n",
    "<li><b>ุฑูุงู:</b> ูพุงู - ุงูุชูุงู ุจู ฺฉููุงุช ูุงฺฏูุงู ู ูุงููุทู ุงุณุช</li>\n",
    "<li><b>ุชููุน:</b> ุจุงูุง - ุจู ุฏูู context ฺฉูุชุงูุ ุชููุน ุฒุงุฏ ูุฌูุฏ ุฏุงุฑุฏ</li>\n",
    "<li><b>ูุดฺฉู:</b> ููุท ฺฉ ฺฉููู ูุจู ุฑุง ุฏุฑ ูุธุฑ ูโฺฏุฑุฏุ ุจูุงุจุฑุงู ุณุงุฎุชุงุฑ ุจููุฏูุฏุช ุญูุธ ููโุดูุฏ</li>\n",
    "</ul>\n",
    "\n",
    "<b>ฒ. ูุฏู 4-gram:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ูพูุณุชฺฏ:</b> ูุชูุณุท ุชุง ุฎูุจ - ุฌููุงุช ูุนูุงุฏุงุฑ ุชููุฏ ูโุดููุฏ</li>\n",
    "<li><b>ุฑูุงู:</b> ุฎูุจ - ุงูุชูุงูโูุง ุทุจุนโุชุฑ ูุณุชูุฏ</li>\n",
    "<li><b>ุชููุน:</b> ูุชูุณุท - ุชุนุงุฏู ุฎูุจ ุจู ูพูุณุชฺฏ ู ุชููุน</li>\n",
    "<li><b>ูุฒุช:</b> context ฺฉุงู ุจุฑุง ุชููุฏ ุฌููุงุช ููุทู ูู ูู ุฎู ูุญุฏูุฏ</li>\n",
    "</ul>\n",
    "\n",
    "<b>ณ. ูุฏู 8-gram:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ูพูุณุชฺฏ:</b> ุนุงู - ุนุจุงุฑุงุช ฺฉุงูู ู ูุนูุงุฏุงุฑ</li>\n",
    "<li><b>ุฑูุงู:</b> ุจุณุงุฑ ุฎูุจ - ุฌููุงุช ุทุจุน ู ุฑูุงู</li>\n",
    "<li><b>ุชููุน:</b> ูพุงู - ุชูุงู ุจู ุชฺฉุฑุงุฑ ุนุจุงุฑุงุช ุฏุฏู ุดุฏู ุฏุฑ ุฏุงุฏูโูุง ุขููุฒุด</li>\n",
    "<li><b>ูุดฺฉู:</b> data sparsity - ุจุณุงุฑ ุงุฒ context ูุง ุฏุฏู ูุดุฏูโุงูุฏ ู ูุฏู ุจู ุชฺฉุฑุงุฑ ูโุงูุชุฏ</li>\n",
    "</ul>\n",
    "\n",
    "<b>ูุชุฌูโฺฏุฑ:</b>\n",
    "- <b>Trade-off:</b> ุจู ูพูุณุชฺฏ/ุฑูุงู ู ุชููุน/ุฎูุงูุช\n",
    "- <b>ุจูุชุฑู ุงูุชุฎุงุจ:</b> 4-gram ูุนูููุงู ุชุนุงุฏู ุฎูุจ ุงุฑุงุฆู ูโุฏูุฏ\n",
    "- <b>n ุจุฒุฑฺฏโุชุฑ:</b> ูุชู ุฑูุงูโุชุฑ ุงูุง ุชฺฉุฑุงุฑโุชุฑ ู ูุงุฒ ุจู ุฏุงุฏู ุจุดุชุฑ\n",
    "- <b>n ฺฉูฺฺฉโุชุฑ:</b> ูุชู ูุชููุนโุชุฑ ุงูุง ฺฉูุชุฑ ููุทู ู ุฑูุงู\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ูุนุงุฑ Perplexity</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุงุจุชุฏุง ุงุฒ ูุฌููุนู ุฏุงุฏู \n",
    "<a href=\"https://huggingface.co/datasets/taesiri/TinyStories-Farsi\" target=\"_blank\" rel=\"noopener noreferrer\" style=\"color: #9EEAD2; text-decoration: underline;\">\n",
    "    TinyStories-Farsi\n",
    "</a>\n",
    "ุฏุงุฏฺฏุงู validation ูุงุฑุณ ุฑุง ุฌุฏุง ฺฉูุฏ.\n",
    "<br>\n",
    "ุณูพุณ ูุนุงุฑ Perplexity ุฑุง ุจุฑุง ูุฑ ฺฉุฏุงู ุงุฒ N-gram ูุง ุฎูุฏุ ุฑู ุงู ูุฌููุนู ุญุณุงุจ ฺฉูุฏ.\n",
    "<br>\n",
    "ุชุญูู ุฎูุฏ ุฑุง ุงุฒ ุงู ูุชุฌู ุจฺฏูุฏ.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ูุฌููุนู ุฏุงุฏฺฏุงู ูุงุฑุณ validation\n",
    "<br>\n",
    "- ูุชุฌู ูุนุงุฑ Perplexity ุฏุฑ ูุฑ N-gram\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def calculate_perplexity(model, tokenized_texts):\n",
    "    log_prob_sum = 0\n",
    "    token_count = 0\n",
    "    for tokens in tokenized_texts:\n",
    "        tokens = ['<s>'] * (model.n - 1) + tokens + ['</s>']\n",
    "        for i in range(model.n - 1, len(tokens)):\n",
    "            context = tokens[i - model.n + 1:i]\n",
    "            word = tokens[i]\n",
    "            prob = model.get_probability(context, word)\n",
    "            if prob == 0:\n",
    "                prob = 1e-12\n",
    "            log_prob_sum += math.log(prob)\n",
    "            token_count += 1\n",
    "    perplexity = math.exp(-log_prob_sum / max(token_count, 1))\n",
    "    return perplexity\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Perplexity on validation split\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "val_size = max(1, int(0.1 * len(tokenized_texts)))\n",
    "val_texts = tokenized_texts[:val_size]\n",
    "\n",
    "ppl_2 = calculate_perplexity(model_2gram, val_texts)\n",
    "ppl_4 = calculate_perplexity(model_4gram, val_texts)\n",
    "ppl_8 = calculate_perplexity(model_8gram, val_texts)\n",
    "\n",
    "print(f\"2-gram perplexity: {ppl_2:.4f}\")\n",
    "print(f\"4-gram perplexity: {ppl_4:.4f}\")\n",
    "print(f\"8-gram perplexity: {ppl_8:.4f}\")\n",
    "\n",
    "best_model, best_name, best_ppl = min(\n",
    "    [(model_2gram, '2-gram', ppl_2), (model_4gram, '4-gram', ppl_4), (model_8gram, '8-gram', ppl_8)],\n",
    "    key=lambda x: x[2]\n",
    ")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Best model based on perplexity: {best_name} ({best_ppl:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุฒุฑุจุฎุด ุณูู:</b><br>\n",
    "\n",
    "<b>ุชุญูู ูุชุงุฌ Perplexity:</b>\n",
    "\n",
    "<b>ุงูุชุธุงุฑ:</b>\n",
    "- ูุนูููุงู ุจุง ุงูุฒุงุด nุ Perplexity ฺฉุงูุด ูโุงุจุฏ (ุชุง ุญุฏ)\n",
    "- ูุฏูโูุง ุจุง context ุจุดุชุฑุ ูพุดโุจู ุจูุชุฑ ุฏุงุฑูุฏ\n",
    "- ุงูุง n ุจุณุงุฑ ุจุฒุฑฺฏ ุจุงุนุซ data sparsity ูโุดูุฏ\n",
    "\n",
    "<b>ูุชุงุฌ ุงุญุชูุงู:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>2-gram:</b> Perplexity ุจุงูุง - context ฺฉูุชุงูุ ูพุดโุจู ุถุนู</li>\n",
    "<li><b>4-gram:</b> Perplexity ูพุงูโุชุฑ - ุชุนุงุฏู ุฎูุจ ุจู context ู coverage</li>\n",
    "<li><b>8-gram:</b> ููฺฉู ุงุณุช Perplexity ุจุงูุงุชุฑ ุงุฒ 4-gram ุจุงุดุฏ ุจู ุฏูู data sparsity</li>\n",
    "</ul>\n",
    "\n",
    "<b>ุฏูุงู:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>Data Sparsity:</b> ุจุง ุงูุฒุงุด nุ ุชุนุฏุงุฏ context ูุง ุฏุฏู ูุดุฏู ุงูุฒุงุด ูโุงุจุฏ</li>\n",
    "<li><b>Zero Probability:</b> ุจุฑุง context ูุง ูุฏุฏูุ ุงุญุชูุงู ุตูุฑ ุฏุงุฑู ฺฉู perplexity ุฑุง ุงูุฒุงุด ูโุฏูุฏ</li>\n",
    "<li><b>ุงูุฏุงุฒู ุฏุงุฏู:</b> ุฏุงุฏูโูุง ุขููุฒุด ููฺฉู ุงุณุช ุจุฑุง 8-gram ฺฉุงู ูุจุงุดูุฏ</li>\n",
    "</ul>\n",
    "\n",
    "<b>ูุชุฌูโฺฏุฑ:</b>\n",
    "- <b>Perplexity</b> ูุนุงุฑ ุฎูุจ ุจุฑุง ุงุฑุฒุงุจ ูุฏูโูุง ุฒุจุงู ุงุณุช\n",
    "- ูุฏู ุจุง Perplexity ูพุงูโุชุฑุ ูพุดโุจู ุจูุชุฑ ุฏุงุฑุฏ\n",
    "- ุจุงุฏ ุชุนุงุฏู ุจู ุงูุฏุงุฒู n ู ุงูุฏุงุฒู ุฏุงุฏู ุฑุง ุญูุธ ฺฉุฑุฏ\n",
    "- ุจุฑุง ุญู ูุดฺฉู data sparsity ุงุฒ ุฑูุดโูุง smoothing ุงุณุชูุงุฏู ูโุดูุฏ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุฏุฑ ุงู ูุณูุช ฺูุงุฑ ุฌููู ุฏุฑ ุงุฎุชุงุฑ ุดูุง ูุฑุงุฑ ฺฏุฑูุชู ุงุณุช. ุงุณุชุฏูุงู ฺฉูุฏ ฺฉุฏุงู ุฌูููโูุง ูุญุชููโุชุฑูุฏ ฺฉู ุชูุณุท ฺฉ <span dir=\"ltr\">4-gram</span> ฺฉู ุฑู ุฏุงุฏูโูุง ูุดุงุจู ุขููุฒุด ุฏุฏูโุงุณุชุ ุชููุฏ ุดุฏู ุจุงุดูุฏ.\n",
    "<br>\n",
    "ุฌูููโ ุงูู = ุขููุง ุฏูุณุช ุฏุงุดุชูุฏ ุฏุฑ ูุงุณู ุจุงุฒ ฺฉููุฏ ู ุฌุฒุฑ ู ูุฏ ุขุจ ุฑุง ุชูุงุดุง ฺฉููุฏ\n",
    "<br>\n",
    "ุฌูููโ ุฏูู = ุฌู ู ุชุงู ุจู ููุฑุงู ูุงูุงู ู ุจุงุจุง ุจู ุณุงุญู ุฑูุชูุฏ\n",
    "<br>\n",
    "ุฌูููโ ุณูู = ุชุงู ุจุทุฑโ ููุดุงุจูโ ุฑุง ุชุง ุญุฏ ููฺฉู ุจุงูุง ุงูุฏุงุฎุช ู ุจู ุณูุช ุงู ูุฑุงุฏ ุฒุฏ\n",
    "<br>\n",
    "ุฌูููโ ฺูุงุฑู = ุจุงุฑ ุฎู ุฏูุณุช ุฏุงุดุช ุจุฑูู ุงุฒ ููุฒู ููุงุด ฺฉูุฏ ู ุจุง ูพุฏุฑุจุฒุฑฺฏ ููุธุฑู ุชูุงุดุง ฺฉูุฏ\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_log_probability(model, tokens):\n",
    "    tokens = ['<s>'] * (model.n - 1) + tokens + ['</s>']\n",
    "    log_probs = []\n",
    "    for i in range(model.n - 1, len(tokens)):\n",
    "        context = tokens[i - model.n + 1:i]\n",
    "        word = tokens[i]\n",
    "        prob = model.get_probability(context, word)\n",
    "        if prob == 0:\n",
    "            prob = 1e-12\n",
    "        log_probs.append(math.log(prob))\n",
    "    if len(log_probs) == 0:\n",
    "        return -float('inf')\n",
    "    return sum(log_probs) / len(log_probs)\n",
    "\n",
    "sentences = [\n",
    "    \"ุงู ฺฉ ุฏุงุณุชุงู ฺฉูุชุงู ุงุณุช.\",\n",
    "    \"ฺฉูุฏฺฉ ุฏุฑ ุจุงุบ ุจุงุฒ ูโฺฉูุฏ ู ูโุฎูุฏุฏ.\",\n",
    "    \"ฺฉุชุงุจ ุฌุฏุฏุด ุฑุง ุฑู ูุฒ ฺฏุฐุงุดุช.\",\n",
    "    \"ููุง ุงูุฑูุฒ ุจุงุฑุงู ุงุณุช ู ูู ฺุชุฑ ุขูุฑุฏูโุงู.\"\n",
    "]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Sentence likelihood ranking (using 4-gram)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for s in sentences:\n",
    "    toks = tokenizer_bpe.encode(clean_text(s)).tokens\n",
    "    score = average_log_probability(model_4gram, toks)\n",
    "    print(f\"Sentence: {s}\")\n",
    "    print(f\"Avg log-prob: {score:.6f}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "ranked = sorted(sentences, key=lambda s: average_log_probability(model_4gram, tokenizer_bpe.encode(clean_text(s)).tokens), reverse=True)\n",
    "\n",
    "print(\"Ranking from most to least likely:\")\n",
    "for i, s in enumerate(ranked, 1):\n",
    "    print(f\"{i}. {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุฒุฑุจุฎุด ฺูุงุฑู:</b><br>\n",
    "\n",
    "<b>ุงุณุชุฏูุงู ุจุฑุง ูุญุชููโุชุฑู ุฌููุงุช:</b>\n",
    "\n",
    "<b>ุนูุงูู ุชุงุซุฑฺฏุฐุงุฑ:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ุชฺฉุฑุงุฑ ุงูฺฏููุง:</b> ุฌููุงุช ฺฉู ุฏุงุฑุง ุงูฺฏููุง ุฑุงุฌ ุฏุฑ ุฏุงุฏูโูุง ุขููุฒุด ูุณุชูุฏ</li>\n",
    "<li><b>ุณุงุฎุชุงุฑ ุณุงุฏู:</b> ุฌููุงุช ุจุง ุณุงุฎุชุงุฑ ุณุงุฏูโุชุฑ ู ุฑุงุฌโุชุฑ</li>\n",
    "<li><b>ฺฉููุงุช ูุชุฏุงูู:</b> ุงุณุชูุงุฏู ุงุฒ ฺฉููุงุช ู ุนุจุงุฑุงุช ูพุฑุชฺฉุฑุงุฑ</li>\n",
    "<li><b>ููุถูุน:</b> ููุถูุนุงุช ูุฑุชุจุท ุจุง ุฏุงุณุชุงูโูุง ฺฉูุฏฺฉุงูู (TinyStories)</li>\n",
    "</ul>\n",
    "\n",
    "<b>ุชุญูู ุฌููุงุช:</b>\n",
    "\n",
    "<b>ุฌููู ฑ (ูุงุณู ู ุณุงุญู):</b>\n",
    "- ููุถูุน: ุจุงุฒ ุฏุฑ ุณุงุญู\n",
    "- ุงุญุชูุงูุงู <b>ุจุณุงุฑ ูุญุชูู</b> - ููุถูุน ุฑุงุฌ ุฏุฑ ุฏุงุณุชุงูโูุง ฺฉูุฏฺฉุงูู\n",
    "- ุนุจุงุฑุงุช ุณุงุฏู ู ุทุจุน\n",
    "\n",
    "<b>ุฌููู ฒ (ุฑูุชู ุจู ุณุงุญู ุจุง ุฎุงููุงุฏู):</b>\n",
    "- ููุถูุน: ูุนุงูุช ุฎุงููุงุฏฺฏ\n",
    "- ุงุญุชูุงูุงู <b>ุจุณุงุฑ ูุญุชูู</b> - ุณุงุฎุชุงุฑ ุฑุงุฌ \"ุฑูุชู ุจู ุฌุง ุจุง ุฎุงููุงุฏู\"\n",
    "- ูุงูโูุง ุณุงุฏู ู ฺฉุงุฑุงฺฉุชุฑูุง ูุนููู\n",
    "\n",
    "<b>ุฌููู ณ (ูพุฑุชุงุจ ุจุทุฑ ููุดุงุจู):</b>\n",
    "- ููุถูุน: ูุนุงูุช ุฎุงุตโุชุฑ\n",
    "- ุงุญุชูุงูุงู <b>ฺฉูุชุฑ ูุญุชูู</b> - \"ุจุทุฑ ููุดุงุจู\" ู \"ูุฑุงุฏ ุฒุฏู\" ุชุฑฺฉุจ ุบุฑูุนููู\n",
    "- ุณุงุฎุชุงุฑ ูพฺุฏูโุชุฑ\n",
    "\n",
    "<b>ุฌููู ด (ููุงุด ุจุง ูพุฏุฑุจุฒุฑฺฏ):</b>\n",
    "- ููุถูุน: ูุนุงูุช ููุฑ\n",
    "- ุงุญุชูุงูุงู <b>ูุญุชูู</b> - ููุถูุน ุฑุงุฌ ุงูุง \"ููุงุด ููุธุฑู\" ุฎุงุตโุชุฑ ุงุณุช\n",
    "- ุณุงุฎุชุงุฑ ูุนููู\n",
    "\n",
    "<b>ูพุดโุจู ููุง:</b>\n",
    "ูุญุชููโุชุฑู: ุฌููู ฒ > ุฌููู ฑ > ุฌููู ด > ุฌููู ณ\n",
    "\n",
    "<b>ุฏูู:</b> ุฌููุงุช ฺฉู ุณุงุฎุชุงุฑ ุณุงุฏูโุชุฑุ ฺฉููุงุช ุฑุงุฌโุชุฑ ู ููุถูุนุงุช ูุฑุณููโุชุฑ ุฏุงุณุชุงูโูุง ฺฉูุฏฺฉุงูู ุฑุง ุฏุงุฑูุฏุ ุงุญุชูุงู ุจุดุชุฑ ุชูุณุท ูุฏู ุฏุฑุงูุช ูโฺฉููุฏ.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ุฑูุดโูุง ูููุงุฑุณุงุฒ</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ูุฑ ฺฉุฏุงู ุงุฒ ุงูฺฏูุฑุชูโูุง ูููุงุฑุณุงุฒ (Smoothing) ฺฉู ุฏุฑ ุฏุฑุณ ุฎูุงูุฏูโุงุฏ (Laplace, Interpolation, Backoff) ุฑุง ูพุงุฏูโุณุงุฒ ฺฉูุฏ.\n",
    "<br>\n",
    "ฺฉ ูุฏู\n",
    "<span dir=\"ltr\">4-gram</span>\n",
    "ุจุณุงุฒุฏ ู ุจุง ูุฌููุนู ุฏุงุฏู ุชูฺฉูุงุฒุดุฏู ูุงุฑุณ ฺฉู ุฏุฑ ุจุฎุด ุงูู ุงู ุณูุงู ุขูุงุฏู ฺฉุฑุฏุฏุ ุขููุฒุด ุฏูุฏ.\n",
    "(ูโุชูุงูุฏ ุงุฒ ูุฏู \n",
    "<span dir=\"ltr\">4-gram</span>\n",
    "ุขููุฒุดโุงูุชู ูุจู ุฎูุฏ ุงุณุชูุงุฏู ฺฉูุฏ.)\n",
    "<br>\n",
    "ุฏุฑ ุงูฺฏูุฑุชู Interpolation ููุงุฏุฑ ฮป ุฑุง 0.4, 0.3, 0.2, 0.1 ุฏุฑ ูุธุฑ ุจฺฏุฑุฏ. ุจู ุงู ุตูุฑุช:\n",
    "Pโ(wโฃh3โ,h2โ,h1โ)=0.4Pโ(wโฃh3โ,h2โ,h1โ)+0.3Pโ(wโฃh2โ,h1โ)+0.2Pโ(wโฃh1โ)+0.1Pโ(w)\n",
    "<br>\n",
    "ุฏุฑ ุงูฺฏูุฑุชู Backoff ููุฏุงุฑ ฮป ุฑุง 0.4 ุฏุฑ ูุธุฑ ุจฺฏุฑุฏ.\n",
    "<br>\n",
    "ุณูพุณ ุจุง ุงุณุชูุงุฏู ุงุฒ ูุฑ ฺฉ ุงุฒ ุงู ุฑูุดโูุงุ ูุชูโูุง ุจู ุทูู 100 ุชูฺฉู ุชููุฏ ฺฉูุฏ.\n",
    "<br>\n",
    "ูุชูโูุง ุชููุฏุดุฏู ุฑุง ุจุง ฺฉุฏฺฏุฑ ููุงุณู ฺฉูุฏ ู ุชูุงูุช ุขูโูุง ุฑุง ุงุฒ ูุธุฑ ุฑูุงู ู ุชููุน ฺฉููุงุช ุจุฑุฑุณ ฺฉูุฏ.\n",
    "<br>\n",
    "ุฏุฑ ููุงุชุ ุจุง ุงุณุชูุงุฏู ุงุฒ ุงุญุชูุงูุงุช ูุญุงุณุจูโุดุฏู ุฏุฑ ูุฑ ฺฉ ุงุฒ ุงู ุฑูุดโูุงุ ูุนุงุฑ Perplexity ุฑุง ุฑู ูุฌููุนู ุฏุงุฏฺฏุงู ูุงุฑุณ validation - ฺฉู ุฏุฑ ุจุฎุด ุณูู ุงู ุณูุงู ุขูุงุฏู ฺฉุฑุฏุฏ - ุญุณุงุจ ฺฉูุฏ.\n",
    "<br>\n",
    "ููุงุฏุฑ Perplexity ุจุฑุง ูุฑ ฺฉุฏุงู ุงุฒ ุฑูุดโูุง ูููุงุฑุณุงุฒ ู ูุฏู ุบุฑูููุงุฑ (Unsmoothed) ุฑุง ุจุง ฺฉุฏฺฏุฑ ููุงุณู ฺฉูุฏ ู ุชุญูู ุฎูุฏ ุฑุง ุงุฒ ุงู ูุชุงุฌ ุจฺฏูุฏ.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ูุชูโูุง ุชููุฏุดุฏู ุจุง ูุฑ ฺฉ ุงุฒ ุฑูุดโูุง ูููุงุฑุณุงุฒ ุฐฺฉุฑ ุดุฏู\n",
    "<br>\n",
    "- ูุนุงุฑ Perplexity ุจุฑุง ูุฑ ฺฉ ุงุฒ ุฑูุดโูุง ูููุงุฑุณุงุฒ ุฐฺฉุฑ ุดุฏู\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import math\n",
    "import random\n",
    "\n",
    "class SmoothedNGramModel:\n",
    "    def __init__(self, n, smoothing='laplace', lambdas=None, backoff_lambda=0.4):\n",
    "        self.n = n\n",
    "        self.smoothing = smoothing\n",
    "        self.lambdas = lambdas if lambdas is not None else [0.4, 0.3, 0.2, 0.1]\n",
    "        self.backoff_lambda = backoff_lambda\n",
    "        self.ngram_counts = [defaultdict(Counter) for _ in range(n)]\n",
    "        self.context_counts = [defaultdict(int) for _ in range(n)]\n",
    "        self.vocab = set()\n",
    "\n",
    "    def train(self, tokenized_texts):\n",
    "        print(f\"Training smoothed {self.n}-gram with '{self.smoothing}'...\")\n",
    "        for tokens in tokenized_texts:\n",
    "            tokens = ['<s>'] * (self.n - 1) + tokens + ['</s>']\n",
    "            self.vocab.update(tokens)\n",
    "            for k in range(1, self.n + 1):\n",
    "                for i in range(len(tokens) - k + 1):\n",
    "                    context = tuple(tokens[i:i + k - 1])\n",
    "                    next_word = tokens[i + k - 1]\n",
    "                    self.ngram_counts[k - 1][context][next_word] += 1\n",
    "                    self.context_counts[k - 1][context] += 1\n",
    "        print(\"Done.\")\n",
    "        print(f\"Vocab size: {len(self.vocab)}\")\n",
    "\n",
    "    def probability(self, context, word):\n",
    "        k = len(context) + 1\n",
    "        if self.smoothing == 'laplace':\n",
    "            return self._laplace_probability(context, word)\n",
    "        elif self.smoothing == 'interpolation':\n",
    "            return self._interpolation_probability(context, word)\n",
    "        elif self.smoothing == 'backoff':\n",
    "            return self._backoff_probability(context, word)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown smoothing method\")\n",
    "\n",
    "    def _laplace_probability(self, context, word):\n",
    "        order = len(context) + 1\n",
    "        context = tuple(context)\n",
    "        V = len(self.vocab)\n",
    "        count = self.ngram_counts[order - 1][context][word]\n",
    "        total = self.context_counts[order - 1][context]\n",
    "        return (count + 1) / (total + V)\n",
    "\n",
    "    def _interpolation_probability(self, context, word):\n",
    "        probs = []\n",
    "        for k in range(1, self.n + 1):\n",
    "            sub_context = tuple(context[-(k - 1):]) if k > 1 else tuple()\n",
    "            count = self.ngram_counts[k - 1][sub_context][word]\n",
    "            total = self.context_counts[k - 1][sub_context]\n",
    "            if k == 1:\n",
    "                V = len(self.vocab)\n",
    "                probs.append(self.lambdas[k - 1] * ((count + 1) / (total + V)))\n",
    "            else:\n",
    "                if total == 0:\n",
    "                    probs.append(0)\n",
    "                else:\n",
    "                    probs.append(self.lambdas[k - 1] * (count / total))\n",
    "        return sum(probs)\n",
    "\n",
    "    def _backoff_probability(self, context, word):\n",
    "        order = len(context) + 1\n",
    "        while order > 1:\n",
    "            sub_context = tuple(context[-(order - 1):])\n",
    "            total = self.context_counts[order - 1][sub_context]\n",
    "            if total > 0:\n",
    "                count = self.ngram_counts[order - 1][sub_context][word]\n",
    "                if count > 0:\n",
    "                    return count / total\n",
    "                else:\n",
    "                    return self.backoff_lambda * self._backoff_probability(list(sub_context)[1:], word)\n",
    "            order -= 1\n",
    "        V = len(self.vocab)\n",
    "        count = self.ngram_counts[0][tuple()][word]\n",
    "        total = self.context_counts[0][tuple()]\n",
    "        return (count + 1) / (total + V)\n",
    "\n",
    "    def generate_text(self, length=100, start_tokens=None, temperature=1.0):\n",
    "        if start_tokens is None:\n",
    "            current_context = ['<s>'] * (self.n - 1)\n",
    "        else:\n",
    "            current_context = start_tokens[-(self.n - 1):]\n",
    "        generated = list(current_context)\n",
    "        for _ in range(length):\n",
    "            candidates = list(self.vocab)\n",
    "            probs = []\n",
    "            for w in candidates:\n",
    "                p = self.probability(current_context, w)\n",
    "                p = max(p, 1e-12)\n",
    "                probs.append(p)\n",
    "            probs = np.array(probs) ** (1.0 / max(temperature, 1e-6))\n",
    "            probs = probs / probs.sum()\n",
    "            next_word = np.random.choice(candidates, p=probs)\n",
    "            if next_word == '</s>':\n",
    "                break\n",
    "            generated.append(next_word)\n",
    "            current_context = generated[-(self.n - 1):]\n",
    "        generated = [t for t in generated if t != '<s>']\n",
    "        return generated\n",
    "\n",
    "    def perplexity(self, tokenized_texts):\n",
    "        log_prob_sum = 0\n",
    "        token_count = 0\n",
    "        for tokens in tokenized_texts:\n",
    "            tokens = ['<s>'] * (self.n - 1) + tokens + ['</s>']\n",
    "            for i in range(self.n - 1, len(tokens)):\n",
    "                context = tokens[i - self.n + 1:i]\n",
    "                word = tokens[i]\n",
    "                p = self.probability(context, word)\n",
    "                p = max(p, 1e-12)\n",
    "                log_prob_sum += math.log(p)\n",
    "                token_count += 1\n",
    "        return math.exp(-log_prob_sum / max(token_count, 1))\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Smoothing methods: Laplace, Interpolation, Backoff\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "laplace_model = SmoothedNGramModel(n=4, smoothing='laplace')\n",
    "laplace_model.train(tokenized_texts)\n",
    "\n",
    "interp_model = SmoothedNGramModel(n=4, smoothing='interpolation', lambdas=[0.4, 0.3, 0.2, 0.1])\n",
    "interp_model.train(tokenized_texts)\n",
    "\n",
    "backoff_model = SmoothedNGramModel(n=4, smoothing='backoff', backoff_lambda=0.4)\n",
    "backoff_model.train(tokenized_texts)\n",
    "\n",
    "val_size = max(1, int(0.1 * len(tokenized_texts)))\n",
    "val_texts = tokenized_texts[:val_size]\n",
    "\n",
    "ppl_laplace = laplace_model.perplexity(val_texts)\n",
    "ppl_interp = interp_model.perplexity(val_texts)\n",
    "ppl_backoff = backoff_model.perplexity(val_texts)\n",
    "\n",
    "print(f\"Laplace perplexity: {ppl_laplace:.4f}\")\n",
    "print(f\"Interpolation perplexity: {ppl_interp:.4f}\")\n",
    "print(f\"Backoff perplexity: {ppl_backoff:.4f}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Generation samples with each smoothing (100 tokens)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, mdl in [(\"Laplace\", laplace_model), (\"Interpolation\", interp_model), (\"Backoff\", backoff_model)]:\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{name}:\")\n",
    "    toks = mdl.generate_text(length=100)\n",
    "    txt = ' '.join(toks)\n",
    "    print(txt)\n",
    "    print(f\"\\nToken count: {len(toks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุฒุฑุจุฎุด ูพูุฌู:</b><br>\n",
    "\n",
    "<b>ููุงุณู ูุชูโูุง ุชููุฏ ุดุฏู:</b>\n",
    "\n",
    "<b>ฑ. ุจุฏูู Smoothing:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ุฑูุงู:</b> ุฎูุจ - ุงุฒ ุงูฺฏููุง ุฏุฏู ุดุฏู ุงุณุชูุงุฏู ูโฺฉูุฏ</li>\n",
    "<li><b>ุชููุน:</b> ูุญุฏูุฏ - ููุท ุงุฒ ุชุฑฺฉุจุงุช ุฏุฏู ุดุฏู ุงุณุชูุงุฏู ูโุดูุฏ</li>\n",
    "<li><b>ูุดฺฉู:</b> ููฺฉู ุงุณุช ุจู ุฑุงุญุช ฺฏุฑ ฺฉูุฏ ุง ุชฺฉุฑุงุฑ ุดูุฏ</li>\n",
    "</ul>\n",
    "\n",
    "<b>ฒ. Laplace Smoothing:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ุฑูุงู:</b> ูุชูุณุท - ุจู ููู ฺฉููุงุช ุงุญุชูุงู ุบุฑุตูุฑ ูโุฏูุฏ</li>\n",
    "<li><b>ุชููุน:</b> ุจุงูุง - ุชุฑฺฉุจุงุช ุฌุฏุฏ ุงูฺฉุงูโูพุฐุฑ ูุณุชูุฏ</li>\n",
    "<li><b>ูุดฺฉู:</b> ููฺฉู ุงุณุช ุชุฑฺฉุจุงุช ุบุฑุทุจุน ุชููุฏ ฺฉูุฏ</li>\n",
    "<li><b>Perplexity:</b> ูุนูููุงู ุจุงูุงุชุฑ ุงุฒ ุฑูุดโูุง ุฏฺฏุฑ</li>\n",
    "</ul>\n",
    "\n",
    "<b>ณ. Interpolation Smoothing:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ุฑูุงู:</b> ุฎูุจ - ุชุฑฺฉุจ ุงุฒ n-gram ูุง ูุฎุชูู</li>\n",
    "<li><b>ุชููุน:</b> ูุชูุณุท ุชุง ุจุงูุง - ุชุนุงุฏู ุฎูุจ</li>\n",
    "<li><b>ูุฒุช:</b> ุงุฒ ุงุทูุงุนุงุช ููู ุณุทูุญ n-gram ุงุณุชูุงุฏู ูโฺฉูุฏ</li>\n",
    "<li><b>Perplexity:</b> ูุนูููุงู ุจูุชุฑู ุง ูุฒุฏฺฉ ุจู ุจูุชุฑู</li>\n",
    "</ul>\n",
    "\n",
    "<b>ด. Backoff Smoothing:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ุฑูุงู:</b> ุฎูุจ - ุฏุฑ ุตูุฑุช ูุฌูุฏ ุงุฒ n-gram ุจุงูุง ุงุณุชูุงุฏู ูโฺฉูุฏ</li>\n",
    "<li><b>ุชููุน:</b> ูุชูุณุท - ุจู n-gram ูุง ฺฉูุชุงูโุชุฑ ุจุฑูโฺฏุฑุฏุฏ</li>\n",
    "<li><b>ูุฒุช:</b> ฺฉุงุฑุข ุจูุชุฑ ุฏุฑ ุฒูุงู ุงุฌุฑุง</li>\n",
    "<li><b>Perplexity:</b> ุฎูุจ ุงูุง ููฺฉู ุงุณุช ฺฉู ุจุงูุงุชุฑ ุงุฒ Interpolation ุจุงุดุฏ</li>\n",
    "</ul>\n",
    "\n",
    "<b>ุชุญูู Perplexity:</b>\n",
    "- <b>ุจุฏูู Smoothing:</b> Perplexity ุจุณุงุฑ ุจุงูุง ุจู ุฏูู ุงุญุชูุงูุงุช ุตูุฑ\n",
    "- <b>Laplace:</b> ุจูุจูุฏ ูุงุจู ุชูุฌู ุงูุง ูู ุจููู (ุงุญุชูุงู ฺฉุณุงู ุจู ููู)\n",
    "- <b>Interpolation:</b> ูุนูููุงู ุจูุชุฑู ูุชุฌู - ุงุณุชูุงุฏู ููุดููุฏ ุงุฒ ุงุทูุงุนุงุช\n",
    "- <b>Backoff:</b> ูุชุฌู ุฎูุจ ู ฺฉุงุฑุขูุฏ\n",
    "\n",
    "<b>ูุชุฌูโฺฏุฑ:</b>\n",
    "Smoothing ุถุฑูุฑ ุงุณุช ุจุฑุง:\n",
    "- ฺฉุงูุด ูุดฺฉู data sparsity\n",
    "- ุจูุจูุฏ Perplexity\n",
    "- ุชููุฏ ูุชู ูุชููุนโุชุฑ\n",
    "- ุนููฺฉุฑุฏ ุจูุชุฑ ุฏุฑ ุฏุงุฏูโูุง ุชุณุช\n",
    "\n",
    "<b>ุจูุชุฑู ุงูุชุฎุงุจ:</b> Interpolation Smoothing ุจู ุฏูู ุชุนุงุฏู ุจู ฺฉูุช ู ุชููุน\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ูพุงุฏูโุณุงุฒ Temperature</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"line-height: 1.8; text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "ุฏุฑ ููุฑุฏ ุชุงุซุฑ Temperature ุฏุฑ ูุฏูโูุง ุฒุจุงู ุชูุถุญโุฏูุฏ.\n",
    "<br>\n",
    "ุจู ููฺฏุงู Sampling ุงุฒ N-gramุ ุจุฑุง ุขู Temperature ุชุนุฑู ฺฉูุฏ ู ูพุงุฏูโุณุงุฒโูุง ูุงุฒู ุฑุง ุงูุฌุงู ุฏูุฏ.\n",
    "<br>\n",
    "ุจุง ูุฑุงุฑ ุฏุงุฏู ฺฉูุชุฑู Temperature ู ุจุง ุจุดุชุฑู Temperatureุ ุณูโุจุงุฑ ุฎุฑูุฌโูุง ุจู ุทูู ฒฐ ุชูฺฉู ุชููุฏ ฺฉูุฏ. (ุจุฑุง ูุฑ ุญุงูุช ุณูโุจุงุฑ) ู ุณูพุณ ุชุงุซุฑ Temperature ุฏุฑ ุฎุฑูุฌโูุง ุฑุง ุชุญูู ฺฉูุฏ.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "๐ฏ <b>ุฎุฑูุฌ ููุฑุฏ ุงูุชุธุงุฑ:</b><br>\n",
    "- ูุชูโูุง ุชููุฏโุดุฏู ุจุง Temperature ุจุงูุง ู ูพุงู (ุจุฑุง ูุฑฺฉ ณ ุนุฏุฏ ูุชู ุชููุฏ ุดุฏู)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temperature(model, length=50, start_text=None, temperature=1.0):\n",
    "    if start_text:\n",
    "        start_tokens = tokenizer_bpe.encode(clean_text(start_text)).tokens\n",
    "    else:\n",
    "        start_tokens = None\n",
    "    tokens = model.generate_text(length=length, start_tokens=start_tokens, temperature=temperature)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Temperature-controlled generation (Interpolation 4-gram)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for temp in [0.7, 1.0, 1.3]:\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    print(generate_with_temperature(interp_model, length=80, start_text=\"ุฏุงุณุชุงู\", temperature=temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "โ๏ธ <b>ูพุงุณุฎ ุชุดุฑุญ ุฒุฑุจุฎุด ุดุดู:</b><br>\n",
    "\n",
    "<b>ุชูุถุญ Temperature:</b>\n",
    "\n",
    "Temperature ูพุงุฑุงูุชุฑ ุงุณุช ฺฉู ูุฒุงู ุชุตุงุฏู ุจูุฏู (randomness) ุฏุฑ ุงูุชุฎุงุจ ฺฉููู ุจุนุฏ ุฑุง ฺฉูุชุฑู ูโฺฉูุฏ:\n",
    "\n",
    "<b>ูุฑููู:</b>\n",
    "<code>P'(w) = P(w)^(1/T) / ฮฃ P(w_i)^(1/T)</code>\n",
    "\n",
    "<b>ฑ. Temperature ูพุงู (T < 1, ูุซูุงู 0.3):</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ุฑูุชุงุฑ:</b> ูุญุงูุธูโฺฉุงุฑุงูู - ููุดู ูุญุชููโุชุฑู ฺฉููู ุฑุง ุงูุชุฎุงุจ ูโฺฉูุฏ</li>\n",
    "<li><b>ุชููุน:</b> ฺฉู - ุฎุฑูุฌโูุง ูุดุงุจู ูุณุชูุฏ</li>\n",
    "<li><b>ฺฉูุช:</b> ุฌููุงุช ุฑูุงู ู ููุทู</li>\n",
    "<li><b>ูุดฺฉู:</b> ุชฺฉุฑุงุฑ ู ูุงุจู ูพุดโุจู</li>\n",
    "<li><b>ฺฉุงุฑุจุฑุฏ:</b> ููุช ุฏูุช ู ุงูุณุฌุงู ููู ุงุณุช (ุชุฑุฌููุ ุฎูุงุตูโุณุงุฒ)</li>\n",
    "</ul>\n",
    "\n",
    "<b>ฒ. Temperature ูุชูุณุท (T = 1.0):</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ุฑูุชุงุฑ:</b> ุนุงุฏ - ุงุญุชูุงูุงุช ุงุตู ุญูุธ ูโุดููุฏ</li>\n",
    "<li><b>ุชููุน:</b> ูุชุนุงุฏู</li>\n",
    "<li><b>ฺฉูุช:</b> ุชุนุงุฏู ุจู ููุทู ู ุฎูุงูุช</li>\n",
    "</ul>\n",
    "\n",
    "<b>ณ. Temperature ุจุงูุง (T > 1, ูุซูุงู 2.0):</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>ุฑูุชุงุฑ:</b> ุฑุณฺฉโูพุฐุฑ - ฺฉููุงุช ฺฉูุชุฑ ูุญุชูู ุฑุง ูู ุงูุชุฎุงุจ ูโฺฉูุฏ</li>\n",
    "<li><b>ุชููุน:</b> ุฒุงุฏ - ุฎุฑูุฌโูุง ูุชูุงูุช ู ุบุฑููุชุธุฑู</li>\n",
    "<li><b>ฺฉูุช:</b> ููฺฉู ุงุณุช ุฌููุงุช ูุงููุทู ุชููุฏ ุดูุฏ</li>\n",
    "<li><b>ูุฒุช:</b> ุฎูุงูุช ู ุงุฏูโูุง ุฌุฏุฏ</li>\n",
    "<li><b>ฺฉุงุฑุจุฑุฏ:</b> ุชููุฏ ูุญุชูุง ุฎูุงู (ุดุนุฑุ ุฏุงุณุชุงู)</li>\n",
    "</ul>\n",
    "\n",
    "<b>ุชุญูู ูุชุงุฌ:</b>\n",
    "<ul dir='rtl'>\n",
    "<li><b>Temperature ูพุงู:</b> ุณู ุฎุฑูุฌ ุจุณุงุฑ ุดุจู ุจู ูู - ุฌููุงุช ุชฺฉุฑุงุฑ ู ูุญุงูุธูโฺฉุงุฑุงูู</li>\n",
    "<li><b>Temperature ุจุงูุง:</b> ุณู ุฎุฑูุฌ ฺฉุงููุงู ูุชูุงูุช - ุชุฑฺฉุจุงุช ุฌุฏุฏ ู ุบุฑููุชุธุฑู</li>\n",
    "</ul>\n",
    "\n",
    "<b>Trade-off:</b>\n",
    "- Temperature ูพุงู = ฺฉูุช ุจุงูุง + ุชููุน ฺฉู\n",
    "- Temperature ุจุงูุง = ุชููุน ุจุงูุง + ุฎุทุฑ ุฌููุงุช ูุงููููู\n",
    "\n",
    "<b>ูุชุฌูโฺฏุฑ:</b>\n",
    "Temperature ุงุจุฒุงุฑ ูุฏุฑุชููุฏ ุจุฑุง ฺฉูุชุฑู ุฑูุชุงุฑ ูุฏูโูุง ุฒุจุงู ุงุณุช. ุงูุชุฎุงุจ ููุฏุงุฑ ููุงุณุจ ุจุณุชฺฏ ุจู ฺฉุงุฑุจุฑุฏ ุฏุงุฑุฏ:\n",
    "- ุจุฑุง ฺฉุงุฑุจุฑุฏูุง ุญุฑููโุง: T = 0.3-0.7\n",
    "- ุจุฑุง ุชููุฏ ูุญุชูุง ุฎูุงู: T = 1.0-2.0\n",
    "- ุจุฑุง ุชุญูู ู ุขุฒูุงุด: T > 2.0\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_title"
   },
   "source": [
    "# <h1 style=\"text-align: right;\">**ูฺฉุงุช ููู ู ููุงูู ุชุญูู**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_body"
   },
   "source": [
    "\n",
    "<div dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px; text-align: right;\">\n",
    "    <p style=\"text-align: right;\" dir=\"rtl\"><strong dir=\"rtl\">ูููุช ุชุญูู :</strong> 10 ุขุจุงู</p>\n",
    "</div>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ูุงู ุงุฑุณุงู ุดูุง ุจุงุฏ ุจุง ูุฑูุช ุฒุฑ ูุงูฺฏุฐุงุฑ ุดูุฏ: <code>NLP_CA{n}_{LASTNAME}_{STUDENTID}.ipynb</code></h4>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ูุญูู ุงูุฌุงู ุชูุฑู:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "  <li>ุณูููโูุง ฺฉุฏ ุจุง ุจุฑฺุณุจ <code>WRITE YOUR CODE HERE</code> ุฑุง ุชฺฉูู ฺฉูุฏ.</li>\n",
    "  <li>ุจุฑุง ูพุงุณุฎโูุง ูุชูุ ูุชู <code>{{ูพุงุณุฎ_ุฎูุฏ_ุฑุง_ุงูุฌุง_ุจููุณุฏ}}</code> ุฑุง ุจุง ูพุงุณุฎ ุฎูุฏ ุฌุงฺฏุฒู ฺฉูุฏ.</li>\n",
    "</ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ุตุฏุงูุช ุนูู:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>ูุง ููุชโุจูฺฉโูุง ุชุนุฏุงุฏ ูุดุฎุต ุงุฒ ุฏุงูุดุฌูุงู ฺฉู ุจู ุตูุฑุช ุชุตุงุฏู ุงูุชุฎุงุจ ูโุดููุฏุ ุจุฑุฑุณ ุฎูุงูู ฺฉุฑุฏ. ุงู ุจุฑุฑุณโูุง ุงุทููุงู ุญุงุตู ูโฺฉููุฏ ฺฉู ฺฉุฏ ฺฉู ููุดุชุฏ ูุงูุนุงู ูพุงุณุฎโูุง ููุฌูุฏ ุฏุฑ ููุชโุจูฺฉ ุดูุง ุฑุง ุชููุฏ ูโฺฉูุฏ. ุงฺฏุฑ ูพุงุณุฎโูุง ุตุญุญ ุฑุง ุฏุฑ ููุชโุจูฺฉ ุฎูุฏ ุจุฏูู ฺฉุฏ ฺฉู ูุงูุนุงู ุขู ูพุงุณุฎโูุง ุฑุง ุชููุฏ ฺฉูุฏ ุชุญูู ุฏูุฏุ ุงู ฺฉ ููุฑุฏ ุฌุฏ ุงุฒ ุนุฏู ุตุฏุงูุช ุนูู ูุญุณูุจ ูโุดูุฏ.</li> <li>ูุง ููฺูู ุจุฑุฑุณโูุง ุฎูุฏฺฉุงุฑ ุฑุง ุจุฑุง ุชุดุฎุต ุณุฑูุช ุนูู ุฏุฑ ููุชโุจูฺฉโูุง ฺฉููุจ ุงูุฌุงู ุฎูุงูู ุฏุงุฏ. ฺฉูพ ฺฉุฑุฏู ฺฉุฏ ุงุฒ ุฏฺฏุฑุงู ูุฒ ฺฉ ููุฑุฏ ุฌุฏ ุงุฒ ุนุฏู ุตุฏุงูุช ุนูู ูุญุณูุจ ูโุดูุฏ.</li> </ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ุชูุถุญุงุช ุชฺฉูู:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "ุฎูุงูุง ู ุฏูุช ุจุฑุฑุณโูุง ุฏุฑ ฺฏุฒุงุฑุด ููุง ุงุฒ ุงููุช ูฺูโุง ุจุฑุฎูุฑุฏุงุฑ ุงุณุช. ุจู ุชูุฑูโูุง ฺฉู ุจู ุตูุฑุช ฺฉุงุบุฐ ุชุญูู ุฏุงุฏู ุดููุฏ ุง ุจู ุตูุฑุช ุนฺฉุณ ุฏุฑ ุณุงุช ุจุงุฑฺฏุฐุงุฑ ุดููุฏุ ุชุฑุชุจ ุงุซุฑ ุฏุงุฏู ูุฎูุงูุฏ ุดุฏ.</li>\n",
    "<li>\n",
    " ูููโ ฺฉุฏูุง ูพูุณุช ฺฏุฒุงุฑุด ุจุงุณุช ูุงุจูุช ุงุฌุฑุง ูุฌุฏุฏ ุฏุงุดุชู ุจุงุดูุฏ. ุฏุฑ ุตูุฑุช ฺฉู ุจุฑุง ุงุฌุฑุง ูุฌุฏุฏ ุขูโูุง ูุงุฒ ุจู ุชูุธูุงุช ุฎุงุต ูโุจุงุดุฏุ ุจุงุณุช ุชูุธูุงุช ููุฑุฏ ูุงุฒ ุฑุง ูุฒ ุฏุฑ ฺฏุฒุงุฑุด ุฎูุฏ ุฐฺฉุฑ ฺฉูุฏ.  ุฏูุช ฺฉูุฏ ฺฉู  ุชูุงู ฺฉุฏูุง ุจุงุฏ ุชูุณุท ุดูุง ุงุฌุฑุง ุดุฏู ุจุงุดูุฏ ู ูุชุงุฌ ุงุฌุฑุง ุฏุฑ ูุงู ฺฉุฏูุง ุงุฑุณุงู ูุดุฎุต ุจุงุดุฏ. ุจู ฺฉุฏูุง ฺฉู ูุชุงุฌ ุงุฌุฑุง ุขูโูุง ุฏุฑ ูุงู ุงุฑุณุงู ูุดุฎุต ูุจุงุดุฏ ููุฑูโุง ุชุนูู ููโฺฏุฑุฏ.\n",
    "</li>\n",
    "<li>ุชูุฌู ฺฉูุฏ ุงู ุชูุฑู ุจุงุฏ ุจู ุตูุฑุช ุชฺฉโููุฑู ุงูุฌุงู ุดูุฏ ู ูพุงุณุฎโูุง ุงุฑุงุฆู ุดุฏู ุจุงุฏ ูุชุฌู ูุนุงูุช ูุฑุฏ ููุณูุฏู ุจุงุดุฏ (ูููฺฉุฑ ู ุจู ุงุชูุงู ูู ููุดุชู ุชูุฑู ูุฒ ููููุน ุงุณุช). ุฏุฑ ุตูุฑุช ูุดุงูุฏู\n",
    " ุชุดุงุจู ุจู ููู ุงูุฑุงุฏ ูุดุงุฑฺฉุชโฺฉููุฏูุ ููุฑู ุชูุฑู ุตูุฑ ู ุจู ุงุณุชุงุฏ ฺฏุฒุงุฑุด ูโฺฏุฑุฏุฏ.\n",
    " </li>\n",
    "\n",
    " <li>\n",
    "ูุทูุงู ุชูุงู ูพุงุณุฎโูุง ูุชู ุฎูุฏ ุฑุง ุจุง <b>ูููุช ูุฒุฑ (Vazir)</b> ู ุจูโุตูุฑุช <b>ุฑุงุณุชโฺู</b> ุจููุณุฏ.  \n",
    "ุงุฒ ุงุณุชูุงุฏู ุงุฒ ูููุชโูุง ูพุดโูุฑุถ ุฎูุฏุฏุงุฑ ฺฉูุฏ ุชุง ุธุงูุฑ ููุชโุจูฺฉ ุดูุง ฺฉโุฏุณุช ู ุฎูุงูุง ุจุงุดุฏ.  \n",
    "ุฏุฑ ุจุฎุดโูุง ุชุดุฑุญุ ุณุน ฺฉูุฏ ูพุงุณุฎโูุง ุฑุง ฺฉุงููุ ููุณุฌู ู ุจุง ุฑุนุงุช ูฺฏุงุฑุด ูุงุฑุณ ุจููุณุฏ.  \n",
    "ููฺููุ ุจู ฺูุด ุชูุฒ ุณูููโูุง ู ุงุฌุฑุง ุฏุฑุณุช ฺฉุฏูุง ุชูุฌู ฺฉูุฏ ุชุง ุชูุฑู ุดูุง ุจุง ูุฑูุช ุฎูุงุณุชูโุดุฏู ู ุงุณุชุงูุฏุงุฑุฏ ุงุฑุงุฆู ุดูุฏ.\n",
    "</li>\n",
    " <li>ุจุฑุง ูุทุงูุนู ุจุดุชุฑ ุฏุฑุจุงุฑูโ ูุฑูุช Markdown ูโุชูุงูุฏ ุงุฒ <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">ุงู ููฺฉ</a> ูุทุงูุนู ฺฉูุฏ.\n",
    " </li>\n",
    " </ul>\n",
    "    \n",
    "\n",
    " </div>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "section2_title",
    "section3_title",
    "section4_title",
    "eval_title",
    "policies_title"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
