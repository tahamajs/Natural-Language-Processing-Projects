{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cover_header",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"text-align: center; padding: 20px; font-family: Vazir;\">\n",
    "<h1 align=\"center\" style=\"font-size: 28px; color:rgb(64, 244, 202); width: 100%;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>ØªÙ…Ø±ÛŒÙ† Ø³ÙˆÙ…<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1>\n",
    "<h2 style=\"color:rgb(90, 255, 184); font-size: 20px;\">Word2Vec & MLP</h2>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">Ø¹Ù„ÛŒ ÙØ±ØªÙˆØª</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">ali.fartout@ut.ac.ir</p>\n",
    "\n",
    "    \n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø¢Ø®ÙˆÙ†Ø¯ÛŒ</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">a.akhoundi79@gmail.com</p>\n",
    "\n",
    "<div dir=\"rtl\" style=\"border: 2px dashed rgb(90, 255, 184); border-radius: 8px; padding: 20px; margin: 20px auto; max-width: 500px; text-align: right;\">\n",
    "<p style=\"color: rgb(64, 244, 202); font-size: 18px; margin-bottom: 15px;\">ğŸ“ Ù…Ø´Ø®ØµØ§Øª Ø¯Ø§Ù†Ø´Ø¬Ùˆ:</p>\n",
    "<p style=\"color: #666; margin: 5px;\">Ù†Ø§Ù… Ùˆ Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ: {{Ù†Ø§Ù…_Ø¯Ø§Ù†Ø´Ø¬Ùˆ}}</p>\n",
    "<p style=\"color: #666; margin: 5px;\">Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒÛŒ: {{Ø´Ù…Ø§Ø±Ù‡_Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒÛŒ}}</p>\n",
    "<p style=\"color: #666; margin: 5px;\">ØªØ§Ø±ÛŒØ® Ø§Ø±Ø³Ø§Ù„: {{ØªØ§Ø±ÛŒØ®_Ø§Ø±Ø³Ø§Ù„}}</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; padding: 25px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<div style=\"line-height: 2.0; font-size: 17px; color: black; font-family: Vazir;\">\n",
    "\n",
    "<br>\n",
    "<div style=\"padding-right:100px\">\n",
    "ğŸ“‹ <b>Ø³Ø§Ø®ØªØ§Ø± ØªÙ…Ø±ÛŒÙ†:</b>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ - <span dir=\"rtl\">Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ : Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ CBOW Ùˆ Skip-Gram</span> </b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ </li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ </li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¨Ú©Ù‡ Ùˆâ€Œ Ø¢Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…:Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§</li>\n",
    "</ul>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… - <span dir=\"rtl\"> Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ Ú©Ù…Ú© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ùˆ Ù…Ø¯Ù„ Fasttext</span> </b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ </li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Embedding Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ </li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¨Ú©Ù‡ Ùˆâ€Œ Ø§Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "<div dir='rtl' style=\"line-height: 1.8; font-family: Vazir; font-size: 16px; margin-top: 20px; background-color: #e8eaf6; padding: 15px; border-radius: 8px; color:black\">\n",
    "ğŸ’¡ <b>Ù†Ú©Ø§Øª Ù…Ù‡Ù…:.</b>\n",
    "<br>\n",
    " ğŸ’¡Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ Ø¨Ø§ÛŒØ¯ ØªÙ…Ø§Ù…ÛŒ Ø¨Ø®Ø´ Ù‡Ø§ Ø±Ùˆ Ø®ÙˆØ¯ØªØ§Ù† Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ . Ø­Ù‚ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù…Ø§Ø¯Ù‡ Ø±Ø§ Ù†Ø¯Ø§Ø±ÛŒØ¯.</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_title"
   },
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ : Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ CBOW Ùˆ Skip-Gram<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ø¨Ø§ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø¯Ùˆ Ù…Ø¯Ù„ CBOW Ùˆ Skipgram Ø¢Ø´Ù†Ø§ Ù…ÛŒØ´ÙˆÛŒØ¯.\n",
    "<p dir=\"rtl\" style=\"padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "</p>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "\n",
    "Ø§Ø¨ØªØ¯Ø§ Ø¯ÛŒØªØ§Ø³Øª Ø²ÛŒØ± Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.\n",
    "<a>https://docs.pytorch.org/text/0.8.1/datasets.html#wikitext-2</a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Û³ ÙØ§ÛŒÙ„ txt Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Train, Valid, Test\n",
    "Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading wikitext-2 zip...\n",
      "Zip download failed: Downloaded content not a zip (missing PK header)\n",
      "Falling back to raw file downloads from GitHub...\n",
      "Zip download failed: Downloaded content not a zip (missing PK header)\n",
      "Falling back to raw file downloads from GitHub...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to fetch wiki.train.tokens from https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/wiki.train.tokens: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/wiki.train.tokens",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 41\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mBadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded content not a zip (missing PK header)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m local_zip\u001b[38;5;241m.\u001b[39mwrite_bytes(content)\n",
      "\u001b[0;31mBadZipFile\u001b[0m: Downloaded content not a zip (missing PK header)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 74\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m re:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/wiki.train.tokens",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m         r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m re:\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mre\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     out_path\u001b[38;5;241m.\u001b[39mwrite_bytes(r\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetched token files from GitHub raw.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to fetch wiki.train.tokens from https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/wiki.train.tokens: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/wiki.train.tokens"
     ]
    }
   ],
   "source": [
    "# Download and prepare WikiText-2 token files (robust with fallbacks)\n",
    "base_url = \"https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\"\n",
    "name = \"wikitext-2\"\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "base_dir = data_dir / name\n",
    "zip_filename = \"wikitext-2-v1.zip\"\n",
    "local_zip = base_dir / zip_filename\n",
    "\n",
    "train_path = base_dir / \"wiki.train.tokens\"\n",
    "valid_path = base_dir / \"wiki.valid.tokens\"\n",
    "test_path  = base_dir / \"wiki.test.tokens\"\n",
    "\n",
    "base_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "files = {\n",
    "    \"train\": train_path,\n",
    "    \"valid\": valid_path,\n",
    "    \"test\":  test_path,\n",
    "}\n",
    "\n",
    "def files_present():\n",
    "    return all(p.exists() for p in files.values())\n",
    "\n",
    "def print_counts():\n",
    "    for split, p in files.items():\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            print(t('counts', split=split, n=len(f.readlines())) if 't' in globals() else f\"{split} has {len(f.readlines()):,} lines\")\n",
    "\n",
    "if files_present():\n",
    "    print(t('all_present') if 't' in globals() else \"All files present. Skipping download.\")\n",
    "    print_counts()\n",
    "else:\n",
    "    try:\n",
    "        print(t('downloading') if 't' in globals() else \"Downloading wikitext-2 zip...\")\n",
    "        resp = requests.get(base_url, timeout=60)\n",
    "        resp.raise_for_status()\n",
    "        content = resp.content\n",
    "        # Quick signature check for zip (should start with 'PK')\n",
    "        if not content.startswith(b\"PK\"):\n",
    "            raise zipfile.BadZipFile(\"Downloaded content not a zip (missing PK header)\")\n",
    "        local_zip.write_bytes(content)\n",
    "        print((t('saved_zip', path=local_zip) if 't' in globals() else f\"Saved zip to {local_zip}\"))\n",
    "        with zipfile.ZipFile(BytesIO(content)) as zf:\n",
    "            members = {\n",
    "                \"wikitext-2/wiki.train.tokens\": train_path,\n",
    "                \"wikitext-2/wiki.valid.tokens\": valid_path,\n",
    "                \"wikitext-2/wiki.test.tokens\":  test_path,\n",
    "            }\n",
    "            for member_name, out_path in members.items():\n",
    "                with zf.open(member_name) as src, open(out_path, \"wb\") as dst:\n",
    "                    dst.write(src.read())\n",
    "        print(t('extracted') if 't' in globals() else \"Extracted token files from zip.\")\n",
    "    except Exception as e:\n",
    "        # Clean up invalid zip if any\n",
    "        if local_zip.exists():\n",
    "            try:\n",
    "                local_zip.unlink()\n",
    "            except Exception:\n",
    "                pass\n",
    "        print((t('zip_failed', err=e) if 't' in globals() else f\"Zip download failed: {e}\"))\n",
    "        # Fallback 1: GitHub raw\n",
    "        print(t('fallback_raw') if 't' in globals() else \"Falling back to GitHub raw token files...\")\n",
    "        ok = True\n",
    "        raw_base = \"https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/\"\n",
    "        for fname, out_path in {\n",
    "            \"wiki.train.tokens\": train_path,\n",
    "            \"wiki.valid.tokens\": valid_path,\n",
    "            \"wiki.test.tokens\":  test_path,\n",
    "        }.items():\n",
    "            if out_path.exists():\n",
    "                continue\n",
    "            url = raw_base + fname\n",
    "            r = requests.get(url, timeout=60)\n",
    "            try:\n",
    "                r.raise_for_status()\n",
    "                out_path.write_bytes(r.content)\n",
    "            except Exception:\n",
    "                ok = False\n",
    "                break\n",
    "        if ok and files_present():\n",
    "            print(t('fetched_raw') if 't' in globals() else \"Fetched token files from GitHub raw.\")\n",
    "        else:\n",
    "            # Fallback 2: Hugging Face datasets\n",
    "            print(t('fallback_hf') if 't' in globals() else \"Falling back to Hugging Face dataset...\")\n",
    "            try:\n",
    "                from datasets import load_dataset\n",
    "                ds = load_dataset('wikitext', 'wikitext-2-v1')\n",
    "                mapping = {\n",
    "                    'train': (ds['train'], train_path),\n",
    "                    'valid': (ds['validation'], valid_path),\n",
    "                    'test':  (ds['test'], test_path),\n",
    "                }\n",
    "                for split, (d, out_path) in mapping.items():\n",
    "                    if out_path.exists():\n",
    "                        continue\n",
    "                    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "                        for ex in d['text']:\n",
    "                            f.write((ex or '') + '\\n')\n",
    "            except Exception as ee:\n",
    "                raise RuntimeError((t('failed_all') if 't' in globals() else \"Failed to obtain wikitext-2 token files; please check internet access and try again.\")) from ee\n",
    "\n",
    "    if files_present():\n",
    "        print_counts()\n",
    "    else:\n",
    "        raise RuntimeError(t('failed_all') if 't' in globals() else \"Failed to obtain wikitext-2 token files; please check internet access and try again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<p style=\"line-height: 1.8; text-align: right;\">\n",
    "Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ù†Ú©Ù‡ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø±Ø¯ÛŒØ¯. ØªÙ…Ø§Ù…ÛŒ Ù…Ø±Ø§Ø­Ù„ Ø²ÛŒØ± Ø±Ø§ Ø®ÙˆØ¯ØªØ§Ù† Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø§ÛŒ Ú¯ÙØªÙ‡ Ø´Ø¯Ù‡ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯\n",
    "<br>\n",
    "\n",
    "Ø¨Ø®Ø´ Ø§ÙˆÙ„) ÛŒÚ© ØªØ§Ø¨Ø¹ÛŒ Ú©Ù‡ Ù…Ø±Ø§Ø­Ù„ Ø²ÛŒØ± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ø¯:\n",
    "â€Œ<br>\n",
    "â€Û±) ØªÙ…Ø§Ù…ÛŒ Ú©Ù„Ù…Ø§Øª Ø±Ø§ lowercase Ø´ÙˆØ¯\n",
    "â€Œ<br>\n",
    "Û²)Special Characters Ø­Ø°Ù Ø´ÙˆÙ†Ø¯\n",
    "â€Œ<br>\n",
    "Û³)Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ù‡Ø± space (ÙØ§ØµÙ„Ù‡) Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ùˆ ØªÙˆÚ©Ù† Ø´ÙˆÙ†Ø¯\n",
    "â€Œ<br>\n",
    "\n",
    "\n",
    "Ø¨Ø®Ø´ Ø¯ÙˆÙ…)  \n",
    "Û±)Ø´Ù…Ø§Ø±Ø´ ÙØ±Ú©Ø§Ù†Ø³ (Frequency Counting): ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø± Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø¯Ø± ØªÙ…Ø§Ù… Ù…ØªÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ø­Ø³Ø§Ø¨ Ú©Ù†ÛŒØ¯\n",
    "â€Œ<br>\n",
    "Û²)ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø¯Ø± (Min Frequency Filtering): ÙÙ‚Ø· Ú©Ù„Ù…Ø§ØªÛŒ Ú©Ù‡ Ø¨ÛŒØ´ØªØ± Ø§Ø² min_freq Ø¨Ø§Ø± ØªÚ©Ø±Ø§Ø± Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒØ¯\n",
    "â€Œ<br>\n",
    "Û³)Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªÙˆÚ©Ù† Ø®Ø§Øµ <unk>: Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ ÛŒØ§ Ù†Ø§Ø¯Ø±\n",
    "â€Œ<br>\n",
    "Û´)Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¯ÙˆØ·Ø±ÙÙ‡:\n",
    "â€Œ<br>\n",
    "word â†’ index (string to index)\n",
    "â€Œ<br>\n",
    "index â†’ word (index to string)\n",
    "\n",
    "\n",
    "Ø¨Ø®Ø´ Ø³ÙˆÙ…)\n",
    "<br>\n",
    "Û±) ØªØ§Ø¨Ø¹ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ú©Ù‡ Ø§Ø² ÛŒÚ© Ø¬Ù…Ù„Ù‡ØŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ CBOW ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯\n",
    "<br>\n",
    "Û²)ØªØ§Ø¨Ø¹ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ú©Ù‡ Ø§Ø² ÛŒÚ© Ø¬Ù…Ù„Ù‡ØŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Skip-gram ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯\n",
    "<br>\n",
    "<b>Ø¨Ø®Ø´ Ø³ÙˆÙ… Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡ÛŒØ¯</b>\n",
    "\n",
    "<b>Ù†Ú©ØªÙ‡: Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ø¯Ø± Ú©Ø¯ Ø¨Ù‡ Ø¬Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø¯Ùˆ ØªØ§Ø¨Ø¹ØŒ  Ù…ÛŒØªÙˆØ§ÛŒÙ†Ø¯ Ø¯Ùˆ ØªØ§Ø¨Ø¹ Ú¯ÙØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§Ù„Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ collate_fn Ù¾Ø§ÛŒØªÙˆØ±Ú† Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.</b>\n",
    "\n",
    "<a>https://discuss.pytorch.org/t/custom-collate-function/145823</a>\n",
    "<br>\n",
    "Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…)\n",
    "<br>\n",
    "Ø¯Ø± Ø¢Ø®Ø± Ø¯Ø§Ø¯Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø±Ø§ Ø¯Ø± dataloader Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø«Ø§Ù„\n",
    "<br>\n",
    "Ø¨Ø±Ø§ÛŒ Ø¨Ø®Ø´ Ø§ÙˆÙ„ \n",
    "\"Hello World! This is a TEST sentence, with 123 numbers.\"\n",
    "Ø¨Ù‡ Ø¹ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ\n",
    "[\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\", \"sentence\", \"with\", \"123\", \"numbers\"]\n",
    "Ø´ÙˆØ¯\n",
    "\n",
    "<br>\n",
    "Ø¨Ø±Ø§ÛŒ Ø¨Ø®Ø´ Ø¯ÙˆÙ…\n",
    "<br>\n",
    "ÙˆØ±ÙˆØ¯ÛŒ:\n",
    "texts = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs\"\n",
    "]\n",
    "min_freq = 2\n",
    "<br>\n",
    "Ø®Ø±ÙˆØ¬ÛŒ:\n",
    "<br>\n",
    "Vocabulary:\n",
    "{\n",
    "    \"<unk>\": 0,\n",
    "    \"the\": 1,\n",
    "    \"sat\": 2,\n",
    "    \"on\": 3,\n",
    "    \"cat\": 4, \n",
    "    \"dog\": 5\n",
    "}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ:</b><br>\n",
    "\n",
    "<b>ØªÙˆØ¶ÛŒØ­ Ø¨Ø®Ø´ Ø³ÙˆÙ… - ØªÙˆÙ„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ:</b>\n",
    "\n",
    "<b>CBOW (Continuous Bag of Words):</b>\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ´ØŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø§Ø·Ø±Ø§Ù (context) Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ù‡ Ù…Ø±Ú©Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ Ø¨Ø§ window_size=2ØŒ Ø§Ú¯Ø± Ø¬Ù…Ù„Ù‡ \"the cat sat on the\" Ø¨Ø§Ø´Ø¯:\n",
    "- Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ù‡ \"sat\" (Ù…Ø±Ú©Ø²)ØŒ Ú©Ù„Ù…Ø§Øª context Ø¹Ø¨Ø§Ø±ØªÙ†Ø¯ Ø§Ø²: [\"the\", \"cat\", \"on\", \"the\"]\n",
    "- ÙˆØ±ÙˆØ¯ÛŒ: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÛŒØ§ Ù…Ø¬Ù…ÙˆØ¹ embedding Ú©Ù„Ù…Ø§Øª context\n",
    "- Ø®Ø±ÙˆØ¬ÛŒ: Ú©Ù„Ù…Ù‡ Ù…Ø±Ú©Ø²ÛŒ \"sat\"\n",
    "\n",
    "<b>Skip-gram:</b>\n",
    "Ø¨Ø±Ø¹Ú©Ø³ CBOWØŒ Ø§Ø² Ú©Ù„Ù…Ù‡ Ù…Ø±Ú©Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ø§Øª Ø§Ø·Ø±Ø§Ù Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ø§Ù† Ù…Ø«Ø§Ù„:\n",
    "- ÙˆØ±ÙˆØ¯ÛŒ: Ú©Ù„Ù…Ù‡ \"sat\"\n",
    "- Ø®Ø±ÙˆØ¬ÛŒ: Ù‡Ø± ÛŒÚ© Ø§Ø² Ú©Ù„Ù…Ø§Øª context Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ [\"the\", \"cat\", \"on\", \"the\"]\n",
    "- Ù¾Ø³ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ù…Ù‡ Ù…Ø±Ú©Ø²ÛŒØŒ Ú†Ù†Ø¯ÛŒÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
    "\n",
    "<b>Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² collate_fn:</b>\n",
    "Ø§Ø² collate_fn Ø¯Ø± DataLoader Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ batchâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ ÙØ±Ù…Øª Ù…Ù†Ø§Ø³Ø¨ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ…. Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ùˆ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ tensorâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "\n",
    "test_sentence = \"Hello World! This is a TEST sentence, with 123 numbers.\"\n",
    "print(\"Input:\", test_sentence)\n",
    "print(\"Output:\", preprocess_text(test_sentence))\n",
    "print()\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_freq = Counter()\n",
    "\n",
    "    def build_vocab(self, texts):\n",
    "        for text in texts:\n",
    "            tokens = preprocess_text(text)\n",
    "            self.word_freq.update(tokens)\n",
    "        self.word2idx = {\"<unk>\": 0}\n",
    "        idx = 1\n",
    "        for word, freq in self.word_freq.items():\n",
    "            if freq >= self.min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "        self.idx2word[0] = \"<unk>\"\n",
    "        print(f\"âœ… Vocabulary built with {len(self.word2idx)} words\")\n",
    "        return self\n",
    "\n",
    "    def encode(self, word):\n",
    "        return self.word2idx.get(word, 0)\n",
    "\n",
    "    def decode(self, idx):\n",
    "        return self.idx2word.get(idx, \"<unk>\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "texts = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs\"\n",
    "]\n",
    "\n",
    "vocab = Vocabulary(min_freq=2)\n",
    "vocab.build_vocab(texts)\n",
    "print(\"Vocabulary:\", {word: idx for word, idx in list(vocab.word2idx.items())[:10]})\n",
    "print()\n",
    "\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, file_path, vocab, window_size=2, model_type='cbow'):\n",
    "        self.vocab = vocab\n",
    "        self.window_size = window_size\n",
    "        self.model_type = model_type\n",
    "        self.samples = []\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                tokens = preprocess_text(line.strip())\n",
    "                if len(tokens) < window_size * 2 + 1:\n",
    "                    continue\n",
    "                token_ids = [vocab.encode(token) for token in tokens]\n",
    "                if model_type == 'cbow':\n",
    "                    self.samples.extend(self._generate_cbow_samples(token_ids))\n",
    "                else:\n",
    "                    self.samples.extend(self._generate_skipgram_samples(token_ids))\n",
    "\n",
    "    def _generate_cbow_samples(self, token_ids):\n",
    "        samples = []\n",
    "        for i in range(self.window_size, len(token_ids) - self.window_size):\n",
    "            context = []\n",
    "            for j in range(i - self.window_size, i + self.window_size + 1):\n",
    "                if j != i:\n",
    "                    context.append(token_ids[j])\n",
    "            target = token_ids[i]\n",
    "            samples.append((context, target))\n",
    "        return samples\n",
    "\n",
    "    def _generate_skipgram_samples(self, token_ids):\n",
    "        samples = []\n",
    "        for i in range(self.window_size, len(token_ids) - self.window_size):\n",
    "            center = token_ids[i]\n",
    "            for j in range(i - self.window_size, i + self.window_size + 1):\n",
    "                if j != i:\n",
    "                    context = token_ids[j]\n",
    "                    samples.append((center, context))\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "\n",
    "def collate_cbow(batch):\n",
    "    contexts = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    contexts = torch.LongTensor(contexts)\n",
    "    targets = torch.LongTensor(targets)\n",
    "    return contexts, targets\n",
    "\n",
    "\n",
    "def collate_skipgram(batch):\n",
    "    centers = [item[0] for item in batch]\n",
    "    contexts = [item[1] for item in batch]\n",
    "    centers = torch.LongTensor(centers)\n",
    "    contexts = torch.LongTensor(contexts)\n",
    "    return centers, contexts\n",
    "\n",
    "\n",
    "print(\"ğŸ“š Loading data...\")\n",
    "\n",
    "all_texts = []\n",
    "for file_path in [train_path, valid_path, test_path]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        all_texts.extend(f.readlines())\n",
    "\n",
    "vocab = Vocabulary(min_freq=5)\n",
    "vocab.build_vocab(all_texts)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Sample words: {list(vocab.word2idx.keys())[:20]}\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“Š Building CBOW Dataset...\")\n",
    "train_dataset_cbow = Word2VecDataset(train_path, vocab, window_size=2, model_type='cbow')\n",
    "valid_dataset_cbow = Word2VecDataset(valid_path, vocab, window_size=2, model_type='cbow')\n",
    "test_dataset_cbow = Word2VecDataset(test_path, vocab, window_size=2, model_type='cbow')\n",
    "\n",
    "train_loader_cbow = DataLoader(train_dataset_cbow, batch_size=512, shuffle=True, collate_fn=collate_cbow)\n",
    "valid_loader_cbow = DataLoader(valid_dataset_cbow, batch_size=512, shuffle=False, collate_fn=collate_cbow)\n",
    "test_loader_cbow = DataLoader(test_dataset_cbow, batch_size=512, shuffle=False, collate_fn=collate_cbow)\n",
    "\n",
    "print(f\"CBOW samples - Train: {len(train_dataset_cbow)}, Valid: {len(valid_dataset_cbow)}, Test: {len(test_dataset_cbow)}\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“Š Building Skip-gram Dataset...\")\n",
    "train_dataset_sg = Word2VecDataset(train_path, vocab, window_size=2, model_type='skipgram')\n",
    "valid_dataset_sg = Word2VecDataset(valid_path, vocab, window_size=2, model_type='skipgram')\n",
    "test_dataset_sg = Word2VecDataset(test_path, vocab, window_size=2, model_type='skipgram')\n",
    "\n",
    "train_loader_sg = DataLoader(train_dataset_sg, batch_size=512, shuffle=True, collate_fn=collate_skipgram)\n",
    "valid_loader_sg = DataLoader(valid_dataset_sg, batch_size=512, shuffle=False, collate_fn=collate_skipgram)\n",
    "test_loader_sg = DataLoader(test_dataset_sg, batch_size=512, shuffle=False, collate_fn=collate_skipgram)\n",
    "\n",
    "print(f\"Skip-gram samples - Train: {len(train_dataset_sg)}, Valid: {len(valid_dataset_sg)}, Test: {len(test_dataset_sg)}\")\n",
    "print(\"âœ… DataLoaders are ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¨Ú©Ù‡ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<p style=\"line-height: 1.8; text-align: right;\">\n",
    "Ø­Ø§Ù„ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ SkipGram Ùˆ CBOW Ø±Ø§ Ù…Ø§Ù†Ù†Ø¯ Ù…Ù‚Ø§Ù„Ù‡ (ÛŒØ§ Ú©ØªØ§Ø¨ Ø¯Ø±Ø³ÛŒ) Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ </p>\n",
    "Ø´Ø¨Ú©Ù‡ Ø±Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ú¯Ø§Ù† Train, Validation & Test Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ø·Ø§ ØªØ±Ø³ÛŒÙ… Ú©Ù†ÛŒØ¯.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    def forward(self, context):\n",
    "        embeds = self.embeddings(context)\n",
    "        embeds = torch.mean(embeds, dim=1)\n",
    "        out = self.linear(embeds)\n",
    "        return out\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "    def forward(self, center):\n",
    "        embeds = self.embeddings(center)\n",
    "        out = self.linear(embeds)\n",
    "        return out\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, test_loader, epochs=30, lr=0.01, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    test_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_batches = 0\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", leave=False):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        train_losses.append(avg_train_loss)\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        valid_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                valid_loss += loss.item()\n",
    "                valid_batches += 1\n",
    "        avg_valid_loss = valid_loss / valid_batches\n",
    "        valid_losses.append(avg_valid_loss)\n",
    "        test_loss = 0\n",
    "        test_batches = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "                test_batches += 1\n",
    "        avg_test_loss = test_loss / test_batches\n",
    "        test_losses.append(avg_test_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_valid_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "    return train_losses, valid_losses, test_losses\n",
    "\n",
    "embedding_dim = 100\n",
    "epochs = 30\n",
    "lr = 0.001\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸš€ Starting CBOW model training...\")\n",
    "cbow_model = CBOW(len(vocab), embedding_dim)\n",
    "cbow_train_losses, cbow_valid_losses, cbow_test_losses = train_model(\n",
    "    cbow_model, train_loader_cbow, valid_loader_cbow, test_loader_cbow, \n",
    "    epochs=epochs, lr=lr, device=device\n",
    ")\n",
    "print(\"âœ… CBOW training completed!\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸš€ Starting Skip-gram model training...\")\n",
    "skipgram_model = SkipGram(len(vocab), embedding_dim)\n",
    "sg_train_losses, sg_valid_losses, sg_test_losses = train_model(\n",
    "    skipgram_model, train_loader_sg, valid_loader_sg, test_loader_sg,\n",
    "    epochs=epochs, lr=lr, device=device\n",
    ")\n",
    "print(\"âœ… Skip-gram training completed!\")\n",
    "print()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "axes[0].plot(cbow_train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(cbow_valid_losses, label='Validation Loss', linewidth=2)\n",
    "axes[0].plot(cbow_test_losses, label='Test Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('Loss', fontsize=12)\n",
    "axes[0].set_title('CBOW Model - Loss Curves', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[1].plot(sg_train_losses, label='Train Loss', linewidth=2)\n",
    "axes[1].plot(sg_valid_losses, label='Validation Loss', linewidth=2)\n",
    "axes[1].plot(sg_test_losses, label='Test Loss', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1].set_ylabel('Loss', fontsize=12)\n",
    "axes[1].set_title('Skip-gram Model - Loss Curves', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('word2vec_loss_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"âœ… Plots saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding: 15px; background-color: #f5f5f5; border-radius: 12px; border: 2px solid #022216; font-family: Vazir; line-height: 1.8; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
    "<h3 style=\"color: #022216; margin-top: 0;\">Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ ØªØ­Ù„ÛŒÙ„ Ø´Ø¨Ø§Ù‡Øª ÙˆØ§Ú˜Ú¯Ø§Ù†</h3>\n",
    "\n",
    "<ol style=\"padding-right: 20px;\">\n",
    "    <li><strong>Ø§Ù†ØªØ®Ø§Ø¨ ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§:</strong> Ûµ ÙˆØ§Ú˜Ù‡ Ø¨Ù‡ Ø¯Ù„Ø®ÙˆØ§Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ù…Ø§ÛŒÛŒØ¯.</li>\n",
    "    <li><strong>ÛŒØ§ÙØªÙ† ÙˆØ§Ú˜Ú¯Ø§Ù† Ù…Ø´Ø§Ø¨Ù‡:</strong> Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆØ§Ú˜Ù‡ Ùˆ Ù‡Ø± Ù…Ø¯Ù„ØŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¹ÛŒØ§Ø± <em>Cosine Similarity</em>ØŒ Ûµ ÙˆØ§Ú˜Ù‡ Ø¨Ø±ØªØ± Ù…Ø´Ø§Ø¨Ù‡ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯.</li>\n",
    "    <li><strong>Ù†Ù…Ø§ÛŒØ´ Ø¨ØµØ±ÛŒ:</strong> ÙˆØ§Ú˜Ú¯Ø§Ù† Ù…Ø´Ø§Ø¨Ù‡ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ <em>t-SNE</em> Ø¨Ù‡ ØµÙˆØ±Øª Ù†Ù…ÙˆØ¯Ø§Ø± Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯ (Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆØ§Ú˜Ù‡ Ùˆ Ù‡Ø± Ù…Ø¯Ù„ ÛŒÚ© Ù†Ù…ÙˆØ¯Ø§Ø± Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡).</li>\n",
    "    <li><strong>ØªØ­Ù„ÛŒÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ:</strong> Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¯Ùˆ Ù…Ø¯Ù„ Ù…Ø®ØªÙ„Ù Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯.</li>\n",
    "</ol>\n",
    "\n",
    "<p style=\"color: #4b5563; font-size: 0.9em; margin-bottom: 0;\">\n",
    "    Ù†Ú©ØªÙ‡: Ø¯Ø± Ù‡Ø± Ù†Ù…ÙˆØ¯Ø§Ø± t-SNE Ù…ÛŒâ€ŒØ¨Ø§ÛŒØ³Øª ÙˆØ§Ú˜Ù‡ Ø§ØµÙ„ÛŒ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ûµ ÙˆØ§Ú˜Ù‡ Ù…Ø´Ø§Ø¨Ù‡ Ø¢Ù† Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def find_similar_words(word, model, vocab, top_k=5):\n",
    "    if word not in vocab.word2idx:\n",
    "        print(f\"Word '{word}' not found in vocabulary\")\n",
    "        return []\n",
    "    \n",
    "    word_idx = vocab.encode(word)\n",
    "    word_embedding = model.embeddings.weight[word_idx].detach().cpu().numpy().reshape(1, -1)\n",
    "    \n",
    "    all_embeddings = model.embeddings.weight.detach().cpu().numpy()\n",
    "    similarities = cosine_similarity(word_embedding, all_embeddings)[0]\n",
    "    \n",
    "    top_indices = similarities.argsort()[-top_k-1:-1][::-1]\n",
    "    \n",
    "    similar_words = []\n",
    "    for idx in top_indices:\n",
    "        if idx != word_idx:\n",
    "            similar_words.append((vocab.decode(idx), similarities[idx]))\n",
    "    \n",
    "    return similar_words[:top_k]\n",
    "\n",
    "\n",
    "def plot_tsne(word, model, vocab, top_k=5, title=\"\"):\n",
    "    if word not in vocab.word2idx:\n",
    "        print(f\"Word '{word}' not found in vocabulary\")\n",
    "        return\n",
    "    \n",
    "    similar_words = find_similar_words(word, model, vocab, top_k)\n",
    "    \n",
    "    word_idx = vocab.encode(word)\n",
    "    indices = [word_idx] + [vocab.encode(w[0]) for w in similar_words]\n",
    "    embeddings = model.embeddings.weight[indices].detach().cpu().numpy()\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(indices)-1))\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    plt.scatter(embeddings_2d[0, 0], embeddings_2d[0, 1], \n",
    "                c='red', s=300, marker='*', edgecolors='black', linewidth=2,\n",
    "                label='Target Word', zorder=3)\n",
    "    plt.annotate(word, (embeddings_2d[0, 0], embeddings_2d[0, 1]),\n",
    "                 fontsize=14, fontweight='bold', ha='center',\n",
    "                 xytext=(0, 10), textcoords='offset points')\n",
    "    \n",
    "    for i in range(1, len(embeddings_2d)):\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],\n",
    "                    c='blue', s=200, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "        plt.annotate(vocab.decode(indices[i]), \n",
    "                     (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                     fontsize=11, ha='center',\n",
    "                     xytext=(0, 8), textcoords='offset points')\n",
    "    \n",
    "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "test_words = ['king', 'computer', 'good', 'time', 'world']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Word Similarity Analysis\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for word in test_words:\n",
    "    if word not in vocab.word2idx:\n",
    "        print(f\"âš ï¸ Word '{word}' not found in vocabulary. Selecting another word...\")\n",
    "        import random\n",
    "        word = random.choice(list(vocab.word2idx.keys())[1:100])\n",
    "    \n",
    "    print(f\"\\nğŸ“Œ Word: {word}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\nğŸ”¹ CBOW model - similar words:\")\n",
    "    cbow_similar = find_similar_words(word, cbow_model, vocab, top_k=5)\n",
    "    for i, (sim_word, score) in enumerate(cbow_similar, 1):\n",
    "        print(f\"  {i}. {sim_word:15s} (similarity: {score:.4f})\")\n",
    "    \n",
    "    print(\"\\nğŸ”¸ Skip-gram model - similar words:\")\n",
    "    sg_similar = find_similar_words(word, skipgram_model, vocab, top_k=5)\n",
    "    for i, (sim_word, score) in enumerate(sg_similar, 1):\n",
    "        print(f\"  {i}. {sim_word:15s} (similarity: {score:.4f})\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_tsne(word, cbow_model, vocab, top_k=5, \n",
    "              title=f'CBOW: Similar Words to \"{word}\"')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plot_tsne(word, skipgram_model, vocab, top_k=5,\n",
    "              title=f'Skip-gram: Similar Words to \"{word}\"')\n",
    "    \n",
    "    plt.savefig(f'tsne_{word}_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ… Word similarity analysis completed!\")\n",
    "print(\"ğŸ“Š Plots saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ:</b><br>\n",
    "\n",
    "<b>ØªØ­Ù„ÛŒÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ CBOW Ùˆ Skip-gram:</b>\n",
    "\n",
    "<b>Û±. ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ø§Ø³Ø§Ø³ÛŒ:</b>\n",
    "- <b>CBOW</b>: Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø§Ø·Ø±Ø§Ù (context) Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ù‡ Ù…Ø±Ú©Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³Øª.\n",
    "- <b>Skip-gram</b>: Ø§Ø² Ú©Ù„Ù…Ù‡ Ù…Ø±Ú©Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ø§Øª Ø§Ø·Ø±Ø§Ù Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø¯Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ø¯Ø§Ø±Ø¯.\n",
    "\n",
    "<b>Û². Ù…Ø´Ø§Ù‡Ø¯Ø§Øª Ø§Ø² Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ t-SNE:</b>\n",
    "- Ø¯Ø± Ù…Ø¯Ù„ Skip-gram Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ú©Ù„Ù…Ø§Øª Ù…Ø´Ø§Ø¨Ù‡â€ŒØªØ± Ø§Ø² Ù†Ø¸Ø± Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨Ù‡ Ù‡Ù… Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ± Ù‡Ø³ØªÙ†Ø¯\n",
    "- Ù…Ø¯Ù„ CBOW ØªÙ…Ø§ÛŒÙ„ Ø¯Ø§Ø±Ø¯ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ context Ù…Ø´Ø§Ø¨Ù‡ Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù†Ø¯\n",
    "- Skip-gram Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¸Ø±ÛŒÙâ€ŒØªØ± Ù‚ÙˆÛŒâ€ŒØªØ± Ø§Ø³Øª\n",
    "\n",
    "<b>Û³. Ú©ÛŒÙÛŒØª Embeddings:</b>\n",
    "- Skip-gram Ø¨Ø±Ø§ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ú©ÙˆÚ†Ú©â€ŒØªØ± Ùˆ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø¯Ø± Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³Øª\n",
    "- CBOW Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³Ø±ÛŒØ¹ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
    "- Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ ØªÙˆØ§Ù†Ø§ÛŒÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯\n",
    "\n",
    "<b>Û´. Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ:</b>\n",
    "Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ùˆ Ø´Ø¨Ø§Ù‡Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ØŒ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ embeddingâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø± ØªÙˆÙ„ÛŒØ¯ Ú©Ø±Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ø§Ù…Ø§ Skip-gram Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ø¯Ø§Ø±Ø¯.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ Ú©Ù…Ú© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ùˆ Ù…Ø¯Ù„ Fasttext<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ø¨Ø§ Ú©Ù…Ú© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ ØªÙ…Ø§Ù… Ù…ØªØµÙ„ \n",
    "(Fully Connected)\n",
    "ÛŒÚ© Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ Ù…ØªÙ† Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ§Ù‡ÛŒØ¯\n",
    "Ú©Ø±Ø¯.\n",
    "Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§Ø² Ù…Ø¯Ù„\n",
    "<a href=\"https://fasttext.cc/\">Fasttext</a>\n",
    "Ø¨Ø±Ø§ÛŒ Embed\n",
    "Ú©Ø±Ø¯Ù† Ù…ØªÙ†â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯.\n",
    "<p dir=\"rtl\" style=\"padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "    <ul>\n",
    "        <li>Ø§Ø¨ØªØ¯Ø§ Ø¯ÛŒØªØ§Ø³Øª Ø²ÛŒØ± Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.\n",
    "    <br>\n",
    "    <a href=\"https://huggingface.co/datasets/SetFit/ag_news\">link</a></li>\n",
    "        <li>\n",
    "            Ù¾Ø³ Ø§Ø² Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ØŒ 5000\n",
    "            Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ùˆ 2000 Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ ØªØ³Øª Ø±Ø§ Ø¨Ù‡ ØªØµØ§Ø¯Ù Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.\n",
    "        </li>\n",
    "        <li>\n",
    "            Ù…Ø¬Ù…ÙˆØ¹Ù‡ 2000 Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¯Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ù‡ 1000 ØªØ§ÛŒÛŒ ØªØ³Øª Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†ÛŒØ¯. Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ 5000 ØªØ§ÛŒÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "        </li>\n",
    "        <li>\n",
    "            Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù… ØªØ´Ú©ÛŒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø­ØªÙ…Ø§ ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡â€ŒØ¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù‡Ø± Ú©Ù„Ø§Ø³ balanced Ø¨Ø§Ø´Ø¯.\n",
    "        </li>\n",
    "        <li>\n",
    "        Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙˆÙ† ØªÙ†Ù‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² lowercasing Ùˆ \n",
    "            Ø­Ø°Ù white space Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ \n",
    "            Ú©Ø§ÙÛŒ Ø§Ø³Øª.\n",
    "        </li>\n",
    "    </ul>\n",
    "    <b>Ø³ÙˆØ§Ù„:</b> Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ fasttext\n",
    "    Ø¨Ø±Ø§ÛŒ embed Ú©Ø±Ø¯Ù† \n",
    "        Ù…ØªÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø²ÛŒØ§Ø¯ÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ…. Ú†Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ù…Ø§ Ø§Ø² Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨ÛŒØ´ØªØ± Ø¨ÛŒâ€ŒÙ†ÛŒØ§Ø² Ø´ÙˆÛŒÙ…ØŸ\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ:</b><br>\n",
    "\n",
    "<b>Ø¯Ù„ÛŒÙ„ Ù†ÛŒØ§Ø² Ú©Ù…ØªØ± Ø¨Ù‡ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø± FastText:</b>\n",
    "\n",
    "Ù…Ø¯Ù„ FastText Ø¨Ø±Ø®Ù„Ø§Ù Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³ÛŒÚ© Word2Vec Ú©Ù‡ Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© ÙˆØ§Ø­Ø¯ atomic Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†Ø¯ØŒ Ø§Ø² <b>subword information</b> Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ Ø±Ø§ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ø¯Ø§Ø±Ø¯:\n",
    "\n",
    "<b>Û±. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Character n-grams:</b>\n",
    "FastText Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø±Ø§ Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² n-gram Ù‡Ø§ÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±ÛŒ ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ Ú©Ù„Ù…Ù‡ \"running\" Ø¨Ù‡: \"&lt;ru\", \"run\", \"unn\", \"nni\", \"nin\", \"ing\", \"ng&gt;\" ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
    "\n",
    "<b>Û². Ù…Ø²Ø§ÛŒØ§ÛŒ Ø§ÛŒÙ† Ø±ÙˆÛŒÚ©Ø±Ø¯:</b>\n",
    "- <b>Ú©Ù„Ù…Ø§Øª Ø®Ø§Ø±Ø¬ Ø§Ø² ÙˆØ§Ú˜Ú¯Ø§Ù† (OOV)</b>: Ø­ØªÛŒ Ø§Ú¯Ø± Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø² n-gram Ù‡Ø§ÛŒ Ø¢Ù† Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª embedding Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯\n",
    "- <b>Ø§Ù…Ù„Ø§ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡</b>: Ú©Ù„Ù…Ø§Øª Ø¨Ø§ Ø§Ù…Ù„Ø§ÛŒ Ù†Ø²Ø¯ÛŒÚ© embedding Ù‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø¯Ø§Ø´Øª\n",
    "- <b>Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø¯Ø±</b>: Ø§Ø² Ø·Ø±ÛŒÙ‚ subword Ù‡Ø§ÛŒ Ù…Ø´ØªØ±Ú©ØŒ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø¯Ø± Ù†ÛŒØ² embedding Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯\n",
    "- <b>Ú©Ù„Ù…Ø§Øª Ù…Ø±Ú©Ø¨</b>: Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø®Ù„ÛŒ Ú©Ù„Ù…Ø§Øª Ù…Ø±Ú©Ø¨ Ø­ÙØ¸ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
    "\n",
    "<b>Û³. Ù†ØªÛŒØ¬Ù‡:</b>\n",
    "Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø­Ø°Ù stopwordsØŒ stemmingØŒ ÛŒØ§ lemmatization Ù†Ø¯Ø§Ø±ÛŒÙ… Ú†ÙˆÙ† Ù…Ø¯Ù„ Ø®ÙˆØ¯Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ø² Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø®Ù„ÛŒ Ú©Ù„Ù…Ø§Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ø¯ Ùˆ Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "print(\"ğŸ“¥ Loading AG News dataset...\")\n",
    "dataset = load_dataset(\"SetFit/ag_news\")\n",
    "\n",
    "print(\"âœ… Dataset loaded!\")\n",
    "print(f\"Number of Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Number of Test samples: {len(dataset['test'])}\")\n",
    "print()\n",
    "\n",
    "print(\"Sample data:\")\n",
    "print(f\"Text: {dataset['train'][0]['text'][:100]}...\")\n",
    "print(f\"Label: {dataset['train'][0]['label']}\")\n",
    "print()\n",
    "\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(\"Available classes:\")\n",
    "print(train_df['label'].value_counts().sort_index())\n",
    "print()\n",
    "\n",
    "def simple_preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"ğŸ“Š Selecting balanced samples...\")\n",
    "\n",
    "samples_per_class_train = 5000 // 4\n",
    "samples_per_class_test = 2000 // 4\n",
    "\n",
    "train_balanced = []\n",
    "for label in range(4):\n",
    "    class_samples = train_df[train_df['label'] == label].sample(n=samples_per_class_train, random_state=42)\n",
    "    train_balanced.append(class_samples)\n",
    "\n",
    "train_data = pd.concat(train_balanced, ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "test_balanced = []\n",
    "for label in range(4):\n",
    "    class_samples = test_df[test_df['label'] == label].sample(n=samples_per_class_test, random_state=42)\n",
    "    test_balanced.append(class_samples)\n",
    "\n",
    "test_data = pd.concat(test_balanced, ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "test_data, valid_data = train_test_split(test_data, test_size=0.5, random_state=42, stratify=test_data['label'])\n",
    "\n",
    "print(f\"âœ… Number of Train samples: {len(train_data)}\")\n",
    "print(f\"âœ… Number of Validation samples: {len(valid_data)}\")\n",
    "print(f\"âœ… Number of Test samples: {len(test_data)}\")\n",
    "print()\n",
    "\n",
    "print(\"Class distribution in Train:\")\n",
    "print(train_data['label'].value_counts().sort_index())\n",
    "print()\n",
    "\n",
    "print(\"Class distribution in Validation:\")\n",
    "print(valid_data['label'].value_counts().sort_index())\n",
    "print()\n",
    "\n",
    "print(\"Class distribution in Test:\")\n",
    "print(test_data['label'].value_counts().sort_index())\n",
    "print()\n",
    "\n",
    "print(\"ğŸ”„ Preprocessing texts...\")\n",
    "train_data['text'] = train_data['text'].apply(simple_preprocess)\n",
    "valid_data['text'] = valid_data['text'].apply(simple_preprocess)\n",
    "test_data['text'] = test_data['text'].apply(simple_preprocess)\n",
    "\n",
    "print(\"âœ… Preprocessing completed!\")\n",
    "print()\n",
    "\n",
    "print(\"Sample preprocessed text:\")\n",
    "print(f\"Text: {train_data.iloc[0]['text'][:150]}...\")\n",
    "print(f\"Label: {train_data.iloc[0]['label']}\")\n",
    "\n",
    "class_names = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\", \n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\"\n",
    "}\n",
    "print()\n",
    "print(\"Class names:\")\n",
    "for label, name in class_names.items():\n",
    "    print(f\"  {label}: {name}\")}```}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ TrainØŒ Test Ùˆ Validation\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Embedding Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ù…ØªÙˆÙ† Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù†ÛŒÙ….\n",
    "    <ol>\n",
    "        <li>\n",
    "            Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ù…Ø¯Ù„ pre-train Ø´Ø¯Ù‡ \n",
    "            <a href=\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.bin.zip\">wiki-news-300d-1M-subword</a>\n",
    "            Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯ Ùˆ Ø¨Ø§ Ú©Ù…Ú© Ú©ØªØ§Ø¨Ø®ÙˆØ§Ù†Ù‡ fasttext \n",
    "            Ø¢Ù†Ø±Ø§ load Ú©Ù†ÛŒØ¯.\n",
    "        </li>\n",
    "        <li>\n",
    "            Ø¯Ø± Ø§Ø³Ù„Ø§ÛŒØ¯ Ø´Ø´Ù… Ø¯Ø±Ø³ Ø¨Ø§ Ù…ÙÙ‡ÙˆÙ… sentence embedding Ø¢Ø´Ù†Ø§ Ø´Ø¯Ù‡â€ŒØ§ÛŒØ¯.\n",
    "            Ø¨Ø§ Ú©Ù…Ú© Ù…Ø¯Ù„ fasttext \n",
    "            ØªÙ…Ø§Ù…ÛŒ Ù…ØªÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ trainØŒ test Ùˆ validation\n",
    "            Ø±Ø§ embed Ú©Ù†ÛŒØ¯.\n",
    "        </li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "model_path = 'wiki-news-300d-1M-subword.bin'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ FastText (Ø§ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú†Ù†Ø¯ Ø¯Ù‚ÛŒÙ‚Ù‡ Ø·ÙˆÙ„ Ø¨Ú©Ø´Ø¯)...\")\n",
    "    url = 'https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.bin.zip'\n",
    "    zip_path = 'wiki-news-300d-1M-subword.bin.zip'\n",
    "    \n",
    "    urllib.request.urlretrieve(url, zip_path)\n",
    "    print(\"âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ø§Ù…Ù„ Ø´Ø¯!\")\n",
    "    \n",
    "    print(\"ğŸ“¦ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§ÛŒÙ„...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('.')\n",
    "    \n",
    "    os.remove(zip_path)\n",
    "    print(\"âœ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ø§Ù…Ù„ Ø´Ø¯!\")\n",
    "else:\n",
    "    print(\"âœ… Ù…Ø¯Ù„ FastText Ø§Ø² Ù‚Ø¨Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡ Ø§Ø³Øª.\")\n",
    "\n",
    "print(\"ğŸ“š Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ FastText...\")\n",
    "ft_model = fasttext.load_model(model_path)\n",
    "print(\"âœ… Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯!\")\n",
    "print(f\"Ø¨Ø¹Ø¯ embedding: {ft_model.get_dimension()}\")\n",
    "print()\n",
    "\n",
    "\n",
    "def get_sentence_embedding(text, model):\n",
    "    words = text.split()\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(model.get_dimension())\n",
    "    \n",
    "    word_vectors = [model.get_word_vector(word) for word in words]\n",
    "    sentence_vector = np.mean(word_vectors, axis=0)\n",
    "    return sentence_vector\n",
    "\n",
    "print(\"ğŸ”„ Embedding Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Train...\")\n",
    "train_embeddings = np.array([get_sentence_embedding(text, ft_model) for text in train_data['text']])\n",
    "train_labels = train_data['label'].values\n",
    "\n",
    "print(\"ğŸ”„ Embedding Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Validation...\")\n",
    "valid_embeddings = np.array([get_sentence_embedding(text, ft_model) for text in valid_data['text']])\n",
    "valid_labels = valid_data['label'].values\n",
    "\n",
    "print(\"ğŸ”„ Embedding Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Test...\")\n",
    "test_embeddings = np.array([get_sentence_embedding(text, ft_model) for text in test_data['text']])\n",
    "test_labels = test_data['label'].values\n",
    "\n",
    "print(\"âœ… ØªÙ…Ø§Ù… embedding Ù‡Ø§ Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯!\")\n",
    "print()\n",
    "\n",
    "print(f\"Ø´Ú©Ù„ Train embeddings: {train_embeddings.shape}\")\n",
    "print(f\"Ø´Ú©Ù„ Validation embeddings: {valid_embeddings.shape}\")\n",
    "print(f\"Ø´Ú©Ù„ Test embeddings: {test_embeddings.shape}\")\n",
    "print()\n",
    "\n",
    "print(\"Ù†Ù…ÙˆÙ†Ù‡ embedding:\")\n",
    "print(f\"Ù…ØªÙ†: {train_data.iloc[0]['text'][:100]}...\")\n",
    "print(f\"Embedding (10 Ù…Ù‚Ø¯Ø§Ø± Ø§ÙˆÙ„): {train_embeddings[0][:10]}\")\n",
    "print(f\"Ø¨Ø±Ú†Ø³Ø¨: {train_labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Embedding\n",
    "Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Train, Test, Validation\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¨Ú©Ù‡ Ùˆâ€Œ Ø§Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø­Ø§Ù„ Ú©Ù‡ Ù…ØªÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ÙØ¶Ø§ÛŒ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ù†Ú¯Ø§Ø´Øª Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒÙ…ØŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ Ú©Ù…Ú© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù‡Ø³ØªÙ†Ø¯.\n",
    "<ol>\n",
    "    <li>Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ù‡Ø§ Ø±Ø§ Ø¨Ø§ Ú©Ù…Ú© Dataloader pytorch\n",
    "    ÛŒØ§ Ù‡Ø± framework Ø¯ÛŒÚ¯Ø±ÛŒ Ú©Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¨Ø¯ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "    </li>\n",
    "    <li>\n",
    "    ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Fully Connected Ø¨Ø§ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø¯Ù„Ø®ÙˆØ§Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø·Ø±Ø§Ø­ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "    </li>\n",
    "    <li>\n",
    "    Ø´Ø¨Ú©Ù‡ Ø±Ø§ Ø­Ø¯Ø§Ù‚Ù„ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ 30 Ø§ÛŒÙ¾Ø§Ú© Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯.\n",
    "    </li>\n",
    "</ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_embeddings_tensor = torch.FloatTensor(train_embeddings)\n",
    "train_labels_tensor = torch.LongTensor(train_labels)\n",
    "\n",
    "valid_embeddings_tensor = torch.FloatTensor(valid_embeddings)\n",
    "valid_labels_tensor = torch.LongTensor(valid_labels)\n",
    "\n",
    "test_embeddings_tensor = torch.FloatTensor(test_embeddings)\n",
    "test_labels_tensor = torch.LongTensor(test_labels)\n",
    "\n",
    "train_dataset = TensorDataset(train_embeddings_tensor, train_labels_tensor)\n",
    "valid_dataset = TensorDataset(valid_embeddings_tensor, valid_labels_tensor)\n",
    "test_dataset = TensorDataset(test_embeddings_tensor, test_labels_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"âœ… DataLoaders Ø¢Ù…Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯!\")\n",
    "print(f\"ØªØ¹Ø¯Ø§Ø¯ batch Ù‡Ø§ÛŒ Train: {len(train_loader)}\")\n",
    "print(f\"ØªØ¹Ø¯Ø§Ø¯ batch Ù‡Ø§ÛŒ Validation: {len(valid_loader)}\")\n",
    "print(f\"ØªØ¹Ø¯Ø§Ø¯ batch Ù‡Ø§ÛŒ Test: {len(test_loader)}\")\n",
    "print()\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.3):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "input_dim = 300 \n",
    "hidden_dims = [256, 128, 64]\n",
    "num_classes = 4\n",
    "dropout = 0.3\n",
    "\n",
    "model = MLPClassifier(input_dim, hidden_dims, num_classes, dropout)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"ğŸ—ï¸ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù…Ø¯Ù„:\")\n",
    "print(model)\n",
    "print()\n",
    "print(f\"Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø³ØªÚ¯Ø§Ù‡: {device}\")\n",
    "print()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "\n",
    "def calculate_accuracy(loader, model, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "<ul>\n",
    "    <li>\n",
    "    Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡\n",
    "    </li>\n",
    "    <li>\n",
    "    Ù†Ù…ÙˆØ¯Ø§Ø± ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ù‚Øª Ùˆ loss\n",
    "        Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù… Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø±Ø±ÙˆÛŒ Ø¯Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡\n",
    "        Train Ùˆ Validation\n",
    "    </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù¾Ø³ Ø§Ø² Ø§ØªÙ…Ø§Ù… Ø¢Ù…ÙˆØ²Ø´ØŒ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ test Ø¨Ø¯Ø³Øª Ø¢ÙˆØ±ÛŒØ¯:\n",
    "    <ul>\n",
    "        <li>\n",
    "            Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ\n",
    "            (Confusion Matrix)\n",
    "        </li>\n",
    "        <li>\n",
    "            Ø¯Ù‚Øª\n",
    "        </li>\n",
    "        <li>\n",
    "            Macro-F1\n",
    "        </li>\n",
    "    </ul>\n",
    "    Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ØŒ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ø²ÛŒØ± Ø¬ÙˆØ§Ø¨ Ø¯Ù‡ÛŒØ¯:\n",
    "    <ul>\n",
    "        <li>Ù…Ø¯Ù„ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ú©Ø¯Ø§Ù… Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¨Ù‡ØªØ±ÛŒÙ† Ùˆ Ø¨Ø¯ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø¯Ø§Ø´ØªÙ‡â€ŒØ§Ø³ØªØŸ</li>\n",
    "        <li>Ù…Ø¯Ù„ Ú©Ø¯Ø§Ù… Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø±Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø¯ÙØ¹Ù‡ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø´ØªØ¨Ø§Ù‡ Ú¯Ø±ÙØªÙ‡â€ŒØ§Ø³ØªØŸ</li>\n",
    "        <li>Ø³Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù…ØªÙˆÙ†ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ú†Ø³Ø¨ Ø²Ø¯Ù‡â€ŒØ§Ø³Øª Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒØ¯ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ:</b><br>\n",
    "\n",
    "<b>ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø®Ø¨Ø§Ø±:</b>\n",
    "\n",
    "<b>Û±. Ø¨Ù‡ØªØ±ÛŒÙ† Ùˆ Ø¨Ø¯ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯:</b>\n",
    "Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªØ±ÛŒÚ© F1-Score Ùˆ Confusion Matrix:\n",
    "- <b>Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯</b>: Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ú©Ù„Ø§Ø³ Sports Ø¯Ø§Ø±Ø§ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø§Ø³Øª Ú†ÙˆÙ† Ø§Ø®Ø¨Ø§Ø± ÙˆØ±Ø²Ø´ÛŒ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø§Ø®ØªØµØ§ØµÛŒ Ùˆ Ù…ØªÙ…Ø§ÛŒØ²ØªØ±ÛŒ Ø¯Ø§Ø±Ù†Ø¯ (Ù†Ø§Ù… Ø¨Ø§Ø²ÛŒÚ©Ù†Ø§Ù†ØŒ ØªÛŒÙ…â€ŒÙ‡Ø§ØŒ Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§)\n",
    "- <b>Ø¨Ø¯ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯</b>: Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ World Ùˆ Business Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙâ€ŒØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯ Ú†ÙˆÙ† Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¢Ù†Ù‡Ø§ Ú¯Ø§Ù‡ÛŒ Ø¨Ø§ Ù‡Ù… overlap Ø¯Ø§Ø±Ù†Ø¯ (Ù…Ø«Ù„Ø§Ù‹ Ø§Ø®Ø¨Ø§Ø± Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø¬Ù‡Ø§Ù†ÛŒ)\n",
    "\n",
    "<b>Û². Ø§Ø´ØªØ¨Ø§Ù‡Ø§Øª Ø±Ø§ÛŒØ¬:</b>\n",
    "Ø¨ÛŒØ´ØªØ±ÛŒÙ† confusion Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨ÛŒÙ† Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø±Ø® Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:\n",
    "- <b>World Ùˆ Business</b>: Ø§Ø®Ø¨Ø§Ø± Ø¬Ù‡Ø§Ù†ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ù‡Ø± Ø¯Ùˆ Ú©Ù„Ø§Ø³ Ù‚Ø±Ø§Ø± Ú¯ÛŒØ±Ø¯\n",
    "- <b>World Ùˆ Sci/Tech</b>: Ø§Ø®Ø¨Ø§Ø± ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ Ø¬Ù‡Ø§Ù†ÛŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù…Ø¨Ù‡Ù… Ø¨Ø§Ø´Ù†Ø¯\n",
    "- <b>Business Ùˆ Sci/Tech</b>: Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ Ø¯Ø± Ù‡Ø± Ø¯Ùˆ Ø¯Ø³ØªÙ‡ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†Ø¯\n",
    "\n",
    "<b>Û³. ØªØ­Ù„ÛŒÙ„ Ù…ØªÙˆÙ† Ø§Ø´ØªØ¨Ø§Ù‡:</b>\n",
    "Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯:\n",
    "- Ù…ØªÙˆÙ† Ú©ÙˆØªØ§Ù‡ Ú©Ù‡ context Ú©Ø§ÙÛŒ Ù†Ø¯Ø§Ø±Ù†Ø¯\n",
    "- Ø§Ø®Ø¨Ø§Ø±ÛŒ Ú©Ù‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡ Ø¯Ø§Ø±Ù†Ø¯ (Ù…Ø«Ù„Ø§Ù‹ Ø´Ø±Ú©Øª ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ø§Ø± Ø¬Ù‡Ø§Ù†ÛŒ)\n",
    "- Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø±Ø²ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¯Ø± Ú†Ù†Ø¯ Ú©Ù„Ø§Ø³ Ù‚Ø±Ø§Ø± Ú¯ÛŒØ±Ù†Ø¯\n",
    "- Ú©Ù…Ø¨ÙˆØ¯ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø´Ø®ØµÙ‡ Ø¯Ø± Ù…ØªÙ†\n",
    "\n",
    "<b>Û´. Ø±Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯:</b>\n",
    "- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ù…Ø§Ù†Ù†Ø¯ LSTM ÛŒØ§ Transformer\n",
    "- Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ\n",
    "- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ensemble methods\n",
    "- Fine-tuning Ø±ÙˆÛŒ embedding Ù‡Ø§ÛŒ pre-trained\n",
    "- Ø§ÙØ²ÙˆØ¯Ù† features Ø¨ÛŒØ´ØªØ± Ù…Ø§Ù†Ù†Ø¯ TF-IDF Ø¯Ø± Ú©Ù†Ø§Ø± FastText\n",
    "\n",
    "<b>Ûµ. Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ:</b>\n",
    "Ù…Ø¯Ù„ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø¯Ø± Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø§Ø±Ø¯ Ø§Ù…Ø§ Ø¯Ø± Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø±Ø²ÛŒ Ùˆ overlap Ø¯Ø§Ø± Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² FastText Ø¨Ù‡ Ù…Ø¯Ù„ Ú©Ù…Ú© Ú©Ø±Ø¯Ù‡ Ú©Ù‡ Ø­ØªÛŒ Ø¨Ø§ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ù†ÛŒØ² Ø¨Ø±Ø®ÙˆØ±Ø¯ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ§ª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Test Set...\")\n",
    "print()\n",
    "\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "macro_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ¯ Ø¯Ù‚Øª (Accuracy): {accuracy * 100:.2f}%\")\n",
    "print(f\"ğŸ“ˆ Macro-F1 Score: {macro_f1:.4f}\")\n",
    "print()\n",
    "\n",
    "class_names = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "print(\"Ú¯Ø²Ø§Ø±Ø´ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:\")\n",
    "print(classification_report(all_labels, all_predictions, target_names=class_names))\n",
    "print()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names, yticklabels=class_names,\n",
    "            cbar_kws={'label': 'Count'}, linewidths=0.5)\n",
    "plt.title('Confusion Matrix', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ú©Ù„Ø§Ø³:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    all_labels, all_predictions, average=None\n",
    ")\n",
    "\n",
    "best_class = np.argmax(f1)\n",
    "worst_class = np.argmin(f1)\n",
    "\n",
    "print(\"ğŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯:\")\n",
    "print(f\"   Ú©Ù„Ø§Ø³: {class_names[best_class]}\")\n",
    "print(f\"   F1-Score: {f1[best_class]:.4f}\")\n",
    "print(f\"   Precision: {precision[best_class]:.4f}\")\n",
    "print(f\"   Recall: {recall[best_class]:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"âš ï¸ Ø¨Ø¯ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯:\")\n",
    "print(f\"   Ú©Ù„Ø§Ø³: {class_names[worst_class]}\")\n",
    "print(f\"   F1-Score: {f1[worst_class]:.4f}\")\n",
    "print(f\"   Precision: {precision[worst_class]:.4f}\")\n",
    "print(f\"   Recall: {recall[worst_class]:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ” ØªØ­Ù„ÛŒÙ„ Ø§Ø´ØªØ¨Ø§Ù‡Ø§Øª Ø±Ø§ÛŒØ¬:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "cm_normalized = cm.copy()\n",
    "np.fill_diagonal(cm_normalized, 0)\n",
    "\n",
    "most_confused = np.unravel_index(cm_normalized.argmax(), cm_normalized.shape)\n",
    "true_class, pred_class = most_confused\n",
    "\n",
    "print(f\"Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø§Ø´ØªØ¨Ø§Ù‡: Ú©Ù„Ø§Ø³ '{class_names[true_class]}' Ø¨Ø§ '{class_names[pred_class]}' Ø§Ø´ØªØ¨Ø§Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡\")\n",
    "print(f\"ØªØ¹Ø¯Ø§Ø¯: {cm[true_class, pred_class]} Ù…ÙˆØ±Ø¯\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“ Ø³Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù…ØªÙˆÙ† Ø§Ø´ØªØ¨Ø§Ù‡ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø´Ø¯Ù‡:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "misclassified_indices = np.where(all_predictions != all_labels)[0]\n",
    "sample_indices = np.random.choice(misclassified_indices, size=min(3, len(misclassified_indices)), replace=False)\n",
    "\n",
    "for i, idx in enumerate(sample_indices, 1):\n",
    "    original_idx = test_data.index[idx]\n",
    "    text = test_data.loc[original_idx, 'text']\n",
    "    true_label = all_labels[idx]\n",
    "    pred_label = all_predictions[idx]\n",
    "    \n",
    "    print(f\"Ù†Ù…ÙˆÙ†Ù‡ {i}:\")\n",
    "    print(f\"  ğŸ“„ Ù…ØªÙ†: {text[:200]}...\")\n",
    "    print(f\"  âœ… Ø¨Ø±Ú†Ø³Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ: {class_names[true_label]}\")\n",
    "    print(f\"  âŒ Ø¨Ø±Ú†Ø³Ø¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø´Ø¯Ù‡: {class_names[pred_label]}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š ØªÙˆØ²ÛŒØ¹ Ø§Ø´ØªØ¨Ø§Ù‡Ø§Øª Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ú©Ù„Ø§Ø³:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    total_samples = cm[i].sum()\n",
    "    correct = cm[i, i]\n",
    "    errors = total_samples - correct\n",
    "    \n",
    "    print(f\"{class_name}:\")\n",
    "    print(f\"  Ú©Ù„ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§: {total_samples}\")\n",
    "    print(f\"  Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØµØ­ÛŒØ­: {correct} ({100*correct/total_samples:.1f}%)\")\n",
    "    print(f\"  Ø§Ø´ØªØ¨Ø§Ù‡Ø§Øª: {errors} ({100*errors/total_samples:.1f}%)\")\n",
    "    \n",
    "    if errors > 0:\n",
    "        print(f\"  ØªÙˆØ²ÛŒØ¹ Ø§Ø´ØªØ¨Ø§Ù‡Ø§Øª:\")\n",
    "        for j, other_class in enumerate(class_names):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                print(f\"    â†’ {other_class}: {cm[i, j]} ({100*cm[i, j]/errors:.1f}% Ø§Ø² Ø§Ø´ØªØ¨Ø§Ù‡Ø§Øª)\")\n",
    "    print()\n",
    "\n",
    "print(\"âœ… ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ø´Ø¯!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "<ul>\n",
    "    <li>\n",
    "    Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ú¯ÙØªÙ‡â€ŒØ´Ø¯Ù‡\n",
    "    </li>\n",
    "    <li>\n",
    "    ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬\n",
    "    </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_title"
   },
   "source": [
    "# <h1 style=\"text-align: right;\">**Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ùˆ Ù‚ÙˆØ§Ù†ÛŒÙ† ØªØ­ÙˆÛŒÙ„**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_body"
   },
   "source": [
    "\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ÙØ±Ù…Øª Ø²ÛŒØ± Ù†Ø§Ù…Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆØ¯: <code>NLP_CA{n}_{LASTNAME}_{STUDENTID}.ipynb</code></h4>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">Ù†Ø­ÙˆÙ‡ Ø§Ù†Ø¬Ø§Ù… ØªÙ…Ø±ÛŒÙ†:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "  <li>Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø¯ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ <code>WRITE YOUR CODE HERE</code> Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ Ú©Ù†ÛŒØ¯.</li>\n",
    "  <li>Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒØŒ Ù…ØªÙ† <code>{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}</code> Ø±Ø§ Ø¨Ø§ Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯.</li>\n",
    "</ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>Ù…Ø§ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´Ø®ØµÛŒ Ø§Ø² Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ø®ÙˆØ§Ù‡ÛŒÙ… Ú©Ø±Ø¯. Ø§ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ú©Ù‡ Ú©Ø¯ÛŒ Ú©Ù‡ Ù†ÙˆØ´ØªÛŒØ¯ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø± Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ ØµØ­ÛŒØ­ Ø±Ø§ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø®ÙˆØ¯ Ø¨Ø¯ÙˆÙ† Ú©Ø¯ÛŒ Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¢Ù† Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯ ØªØ­ÙˆÛŒÙ„ Ø¯Ù‡ÛŒØ¯ØŒ Ø§ÛŒÙ† ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> <li>Ù…Ø§ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³Ø±Ù‚Øª Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÙ„Ø¨ Ø§Ù†Ø¬Ø§Ù… Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø¯. Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ø¯ Ø§Ø² Ø¯ÛŒÚ¯Ø±Ø§Ù† Ù†ÛŒØ² ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> </ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØªÙˆØ¶ÛŒØ­Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ùˆ Ø¯Ù‚Øª Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø± Ø§Ø³Øª. Ø¨Ù‡ ØªÙ…Ø±ÛŒÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§ØºØ°ÛŒ ØªØ­ÙˆÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ ÛŒØ§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ú©Ø³ Ø¯Ø± Ø³Ø§ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆÙ†Ø¯ØŒ ØªØ±ØªÛŒØ¨ Ø§Ø«Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.</li>\n",
    "<li>\n",
    " Ù‡Ù…Ù‡â€ŒÛŒ Ú©Ø¯Ù‡Ø§ÛŒ Ù¾ÛŒÙˆØ³Øª Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø§ÛŒØ³ØªÛŒ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯. Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ Ù…Ø¬Ø¯Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø®Ø§ØµÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ Ø¨Ø§ÛŒØ³ØªÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ù†ÛŒØ² Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ø®ÙˆØ¯ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯.  Ø¯Ù‚Øª Ú©Ù†ÛŒØ¯ Ú©Ù‡  ØªÙ…Ø§Ù…ÛŒ Ú©Ø¯Ù‡Ø§ Ø¨Ø§ÛŒØ¯ ØªÙˆØ³Ø· Ø´Ù…Ø§ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ú©Ø¯Ù‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ø¨Ø§Ø´Ø¯. Ø¨Ù‡ Ú©Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ù†Ø¨Ø§Ø´Ø¯ Ù†Ù…Ø±Ù‡â€ŒØ§ÛŒ ØªØ¹Ù„Ù‚ Ù†Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
    "</li>\n",
    "<li>ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª ØªÚ©â€ŒÙ†ÙØ±Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ Ùˆ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ÛŒØ¯ Ù†ØªÛŒØ¬Ù‡ ÙØ¹Ø§Ù„ÛŒØª ÙØ±Ø¯ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ (Ù‡Ù…ÙÚ©Ø±ÛŒ Ùˆ Ø¨Ù‡ Ø§ØªÙØ§Ù‚ Ù‡Ù… Ù†ÙˆØ´ØªÙ† ØªÙ…Ø±ÛŒÙ† Ù†ÛŒØ² Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³Øª). Ø¯Ø± ØµÙˆØ±Øª Ù…Ø´Ø§Ù‡Ø¯Ù‡\n",
    " ØªØ´Ø§Ø¨Ù‡ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø§ÙØ±Ø§Ø¯ Ù…Ø´Ø§Ø±Ú©Øªâ€ŒÚ©Ù†Ù†Ø¯Ù‡ØŒ Ù†Ù…Ø±Ù‡ ØªÙ…Ø±ÛŒÙ† ØµÙØ± Ùˆ Ø¨Ù‡ Ø§Ø³ØªØ§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.\n",
    " </li>\n",
    "\n",
    " <li>\n",
    "Ù„Ø·ÙØ§Ù‹ ØªÙ…Ø§Ù…ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ <b>ÙÙˆÙ†Øª ÙˆØ²ÛŒØ± (Vazir)</b> Ùˆ Ø¨Ù‡â€ŒØµÙˆØ±Øª <b>Ø±Ø§Ø³Øªâ€ŒÚ†ÛŒÙ†</b> Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙÙˆÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§ Ø¸Ø§Ù‡Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ ÛŒÚ©â€ŒØ¯Ø³Øª Ùˆ Ø®ÙˆØ§Ù†Ø§ Ø¨Ø§Ø´Ø¯.  \n",
    "Ø¯Ø± Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ ØªØ´Ø±ÛŒØ­ÛŒØŒ Ø³Ø¹ÛŒ Ú©Ù†ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ Ú©Ø§Ù…Ù„ØŒ Ù…Ù†Ø³Ø¬Ù… Ùˆ Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª Ù†Ú¯Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ù‡ Ú†ÛŒÙ†Ø´ ØªÙ…ÛŒØ² Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø±Ø³Øª Ú©Ø¯Ù‡Ø§ ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ ØªÙ…Ø±ÛŒÙ† Ø´Ù…Ø§ Ø¨Ø§ ÙØ±Ù…Øª Ø®ÙˆØ§Ø³ØªÙ‡â€ŒØ´Ø¯Ù‡ Ùˆ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø§Ø±Ø§Ø¦Ù‡ Ø´ÙˆØ¯.\n",
    "</li>\n",
    " <li>Ø¨Ø±Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø¨ÛŒØ´ØªØ± Ø¯Ø±Ø¨Ø§Ø±Ù‡â€ŒÛŒ ÙØ±Ù…Øª Markdown Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">Ø§ÛŒÙ† Ù„ÛŒÙ†Ú©</a> Ù…Ø·Ø§Ù„Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n",
    " </li>\n",
    " </ul>\n",
    "    \n",
    "\n",
    " </div>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "section2_title",
    "section3_title",
    "section4_title",
    "eval_title",
    "policies_title"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
