{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "import zipfile\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cover_header",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"text-align: center; padding: 20px; font-family: Vazir;\">\n",
    "<h1 align=\"center\" style=\"font-size: 28px; color:rgb(64, 244, 202); width: 100%;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>ØªÙ…Ø±ÛŒÙ† Ø³ÙˆÙ…<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1>\n",
    "<h2 style=\"color:rgb(90, 255, 184); font-size: 20px;\">Word2Vec & MLP</h2>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">Ø¹Ù„ÛŒ ÙØ±ØªÙˆØª</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">ali.fartout@ut.ac.ir</p>\n",
    "\n",
    "    \n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px;\">Ø¹Ù„ÛŒØ±Ø¶Ø§ Ø¢Ø®ÙˆÙ†Ø¯ÛŒ</p>\n",
    "<p align=\"center\" style=\"color: #666; font-size: 16px; margin-bottom: 30px;\">a.akhoundi79@gmail.com</p>\n",
    "\n",
    "<div dir=\"rtl\" style=\"border: 2px dashed rgb(90, 255, 184); border-radius: 8px; padding: 20px; margin: 20px auto; max-width: 500px; text-align: right;\">\n",
    "<p style=\"color: rgb(64, 244, 202); font-size: 18px; margin-bottom: 15px;\">ğŸ“ Ù…Ø´Ø®ØµØ§Øª Ø¯Ø§Ù†Ø´Ø¬Ùˆ:</p>\n",
    "<p style=\"color: #666; margin: 5px;\">Ù†Ø§Ù… Ùˆ Ù†Ø§Ù… Ø®Ø§Ù†ÙˆØ§Ø¯Ú¯ÛŒ: {{Ù†Ø§Ù…_Ø¯Ø§Ù†Ø´Ø¬Ùˆ}}</p>\n",
    "<p style=\"color: #666; margin: 5px;\">Ø´Ù…Ø§Ø±Ù‡ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒÛŒ: {{Ø´Ù…Ø§Ø±Ù‡_Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒÛŒ}}</p>\n",
    "<p style=\"color: #666; margin: 5px;\">ØªØ§Ø±ÛŒØ® Ø§Ø±Ø³Ø§Ù„: {{ØªØ§Ø±ÛŒØ®_Ø§Ø±Ø³Ø§Ù„}}</p>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "<div dir=\"rtl\" style=\"text-align: justify; padding: 25px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<div style=\"line-height: 2.0; font-size: 17px; color: black; font-family: Vazir;\">\n",
    "\n",
    "<br>\n",
    "<div style=\"padding-right:100px\">\n",
    "ğŸ“‹ <b>Ø³Ø§Ø®ØªØ§Ø± ØªÙ…Ø±ÛŒÙ†:</b>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ - <span dir=\"rtl\">Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ : Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ CBOW Ùˆ Skip-Gram</span> </b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ </li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ </li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¨Ú©Ù‡ Ùˆâ€Œ Ø¢Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…:Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§</li>\n",
    "</ul>\n",
    "<li><b>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ… - <span dir=\"rtl\"> Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ Ú©Ù…Ú© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ùˆ Ù…Ø¯Ù„ Fasttext</span> </b></li>\n",
    "<ul>\n",
    "<li>Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ </li>\n",
    "<li>Ø¨Ø®Ø´ Ø¯ÙˆÙ…: Embedding Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ </li>\n",
    "<li>Ø¨Ø®Ø´ Ø³ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¨Ú©Ù‡ Ùˆâ€Œ Ø§Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡</li>\n",
    "<li>Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…: ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬</li>\n",
    "</ul>\n",
    "</div>\n",
    "</div>\n",
    "<div dir='rtl' style=\"line-height: 1.8; font-family: Vazir; font-size: 16px; margin-top: 20px; background-color: #e8eaf6; padding: 15px; border-radius: 8px; color:black\">\n",
    "ğŸ’¡ <b>Ù†Ú©Ø§Øª Ù…Ù‡Ù…:.</b>\n",
    "<br>\n",
    " ğŸ’¡Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ Ø¨Ø§ÛŒØ¯ ØªÙ…Ø§Ù…ÛŒ Ø¨Ø®Ø´ Ù‡Ø§ Ø±Ùˆ Ø®ÙˆØ¯ØªØ§Ù† Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ . Ø­Ù‚ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ù…Ø§Ø¯Ù‡ Ø±Ø§ Ù†Ø¯Ø§Ø±ÛŒØ¯.</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_title"
   },
   "source": [
    "# <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø§ÙˆÙ„ : Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ CBOW Ùˆ Skip-Gram<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section1_desc"
   },
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ø¨Ø§ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø¯Ùˆ Ù…Ø¯Ù„ CBOW Ùˆ Skipgram Ø¢Ø´Ù†Ø§ Ù…ÛŒØ´ÙˆÛŒØ¯.\n",
    "<p dir=\"rtl\" style=\"padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "</p>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "\n",
    "Ø§Ø¨ØªØ¯Ø§ Ø¯ÛŒØªØ§Ø³Øª Ø²ÛŒØ± Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.\n",
    "<a>https://docs.pytorch.org/text/0.8.1/datasets.html#wikitext-2</a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Û³ ÙØ§ÛŒÙ„ txt Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø³Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Train, Valid, Test\n",
    "Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading wikitext-2 zip...\n",
      "Zip download failed: Downloaded content not a zip (missing PK header)\n",
      "Falling back to raw file downloads from GitHub...\n",
      "Zip download failed: Downloaded content not a zip (missing PK header)\n",
      "Falling back to raw file downloads from GitHub...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to fetch wiki.train.tokens from https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/wiki.train.tokens: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/wiki.train.tokens",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 41\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m zipfile\u001b[38;5;241m.\u001b[39mBadZipFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded content not a zip (missing PK header)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m local_zip\u001b[38;5;241m.\u001b[39mwrite_bytes(content)\n",
      "\u001b[0;31mBadZipFile\u001b[0m: Downloaded content not a zip (missing PK header)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 74\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m re:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/requests/models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/wiki.train.tokens",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 76\u001b[0m\n\u001b[1;32m     74\u001b[0m         r\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m re:\n\u001b[0;32m---> 76\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to fetch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mre\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     out_path\u001b[38;5;241m.\u001b[39mwrite_bytes(r\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetched token files from GitHub raw.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to fetch wiki.train.tokens from https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/wiki.train.tokens: 404 Client Error: Not Found for url: https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/wiki.train.tokens"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Configuration\n",
    "DATASET_NAME = \"wikitext\"\n",
    "DATASET_CONFIG = \"wikitext-2-raw-v1\"  # Loads the raw (non-tokenized) version\n",
    "SAVE_DIR = \"data/wikitext-2\"\n",
    "SPLITS = [\"train\", \"validation\", \"test\"]\n",
    "\n",
    "# Load the Dataset\n",
    "print(f\"ğŸ“¥ Downloading and loading the {DATASET_CONFIG} dataset...\")\n",
    "try:\n",
    "    # Loads the three splits (train, validation, test) simultaneously\n",
    "    raw_datasets = load_dataset(DATASET_NAME, DATASET_CONFIG)\n",
    "    print(\"âœ… Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading dataset: {e}\")\n",
    "    print(\"Please ensure 'datasets' library is installed and you have an active internet connection.\")\n",
    "    raise\n",
    "\n",
    "# Create the output directory\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "print(f\"ğŸ“‚ Output directory '{SAVE_DIR}' created or already exists.\")\n",
    "\n",
    "# Save each split to a separate .txt file\n",
    "print(\"\\nğŸ“ Saving splits to text files...\")\n",
    "for split in SPLITS:\n",
    "    if split in raw_datasets:\n",
    "        # Construct the full path for the output file\n",
    "        if split == \"validation\":\n",
    "            filename = \"wiki.valid.tokens\"\n",
    "        elif split == \"train\":\n",
    "            filename = \"wiki.train.tokens\"\n",
    "        else:\n",
    "            filename = \"wiki.test.tokens\"\n",
    "        \n",
    "        file_path = os.path.join(SAVE_DIR, filename)\n",
    "        \n",
    "        # Access the specific split\n",
    "        data_split = raw_datasets[split]\n",
    "        \n",
    "        # Join all text entries with a newline character\n",
    "        all_text = '\\n'.join(data_split['text'])\n",
    "\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(all_text)\n",
    "        \n",
    "        print(f\"--- Successfully saved {len(data_split)} entries to {file_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"ğŸ¯ **Output Goal Met:** Three files saved in the '{SAVE_DIR}' directory.\")\n",
    "print(f\"Files saved: {os.listdir(SAVE_DIR)}\")\n",
    "\n",
    "# Update paths for the rest of the notebook\n",
    "train_path = Path(os.path.join(SAVE_DIR, \"wiki.train.tokens\"))\n",
    "valid_path = Path(os.path.join(SAVE_DIR, \"wiki.valid.tokens\"))\n",
    "test_path = Path(os.path.join(SAVE_DIR, \"wiki.test.tokens\"))\n",
    "\n",
    "print(f\"\\nâœ… Paths configured:\")\n",
    "print(f\"  Train: {train_path}\")\n",
    "print(f\"  Validation: {valid_path}\")\n",
    "print(f\"  Test: {test_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset statistics\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Read files and compute statistics\n",
    "stats = {}\n",
    "for split_name, file_path in [(\"Train\", train_path), (\"Validation\", valid_path), (\"Test\", test_path)]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    # Filter out empty lines\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    # Compute statistics\n",
    "    num_lines = len(lines)\n",
    "    all_text = ' '.join(lines)\n",
    "    words = all_text.split()\n",
    "    num_words = len(words)\n",
    "    unique_words = len(set(words))\n",
    "    avg_words_per_line = num_words / num_lines if num_lines > 0 else 0\n",
    "    \n",
    "    stats[split_name] = {\n",
    "        'lines': num_lines,\n",
    "        'words': num_words,\n",
    "        'unique_words': unique_words,\n",
    "        'avg_words_per_line': avg_words_per_line\n",
    "    }\n",
    "    \n",
    "    print(f\"{split_name}:\")\n",
    "    print(f\"  Lines: {num_lines:,}\")\n",
    "    print(f\"  Total words: {num_words:,}\")\n",
    "    print(f\"  Unique words: {unique_words:,}\")\n",
    "    print(f\"  Average words per line: {avg_words_per_line:.2f}\")\n",
    "    print()\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "splits = list(stats.keys())\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
    "\n",
    "# Plot 1: Number of lines\n",
    "axes[0, 0].bar(splits, [stats[s]['lines'] for s in splits], color=colors, edgecolor='black')\n",
    "axes[0, 0].set_ylabel('Number of Lines', fontsize=12)\n",
    "axes[0, 0].set_title('Lines per Split', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "for i, s in enumerate(splits):\n",
    "    axes[0, 0].text(i, stats[s]['lines'], f\"{stats[s]['lines']:,}\", \n",
    "                     ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Total words\n",
    "axes[0, 1].bar(splits, [stats[s]['words'] for s in splits], color=colors, edgecolor='black')\n",
    "axes[0, 1].set_ylabel('Total Words', fontsize=12)\n",
    "axes[0, 1].set_title('Total Words per Split', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "for i, s in enumerate(splits):\n",
    "    axes[0, 1].text(i, stats[s]['words'], f\"{stats[s]['words']:,}\", \n",
    "                     ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 3: Unique words\n",
    "axes[1, 0].bar(splits, [stats[s]['unique_words'] for s in splits], color=colors, edgecolor='black')\n",
    "axes[1, 0].set_ylabel('Unique Words', fontsize=12)\n",
    "axes[1, 0].set_title('Unique Words per Split', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "for i, s in enumerate(splits):\n",
    "    axes[1, 0].text(i, stats[s]['unique_words'], f\"{stats[s]['unique_words']:,}\", \n",
    "                     ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 4: Average words per line\n",
    "axes[1, 1].bar(splits, [stats[s]['avg_words_per_line'] for s in splits], color=colors, edgecolor='black')\n",
    "axes[1, 1].set_ylabel('Average Words per Line', fontsize=12)\n",
    "axes[1, 1].set_title('Average Words per Line', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "for i, s in enumerate(splits):\n",
    "    axes[1, 1].text(i, stats[s]['avg_words_per_line'], f\"{stats[s]['avg_words_per_line']:.1f}\", \n",
    "                     ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Dataset statistics visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_title"
   },
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<p style=\"line-height: 1.8; text-align: right;\">\n",
    "Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ù†Ú©Ù‡ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ø±Ø¯ÛŒØ¯. ØªÙ…Ø§Ù…ÛŒ Ù…Ø±Ø§Ø­Ù„ Ø²ÛŒØ± Ø±Ø§ Ø®ÙˆØ¯ØªØ§Ù† Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ø±Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø§ÛŒ Ú¯ÙØªÙ‡ Ø´Ø¯Ù‡ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒØ¯\n",
    "<br>\n",
    "\n",
    "Ø¨Ø®Ø´ Ø§ÙˆÙ„) ÛŒÚ© ØªØ§Ø¨Ø¹ÛŒ Ú©Ù‡ Ù…Ø±Ø§Ø­Ù„ Ø²ÛŒØ± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡Ø¯:\n",
    "â€Œ<br>\n",
    "â€Û±) ØªÙ…Ø§Ù…ÛŒ Ú©Ù„Ù…Ø§Øª Ø±Ø§ lowercase Ø´ÙˆØ¯\n",
    "â€Œ<br>\n",
    "Û²)Special Characters Ø­Ø°Ù Ø´ÙˆÙ†Ø¯\n",
    "â€Œ<br>\n",
    "Û³)Ú©Ù„Ù…Ø§Øª Ø¯Ø± Ù‡Ø± space (ÙØ§ØµÙ„Ù‡) Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ùˆ ØªÙˆÚ©Ù† Ø´ÙˆÙ†Ø¯\n",
    "â€Œ<br>\n",
    "\n",
    "\n",
    "Ø¨Ø®Ø´ Ø¯ÙˆÙ…)  \n",
    "Û±)Ø´Ù…Ø§Ø±Ø´ ÙØ±Ú©Ø§Ù†Ø³ (Frequency Counting): ØªØ¹Ø¯Ø§Ø¯ ØªÚ©Ø±Ø§Ø± Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø¯Ø± ØªÙ…Ø§Ù… Ù…ØªÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ø­Ø³Ø§Ø¨ Ú©Ù†ÛŒØ¯\n",
    "â€Œ<br>\n",
    "Û²)ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø¯Ø± (Min Frequency Filtering): ÙÙ‚Ø· Ú©Ù„Ù…Ø§ØªÛŒ Ú©Ù‡ Ø¨ÛŒØ´ØªØ± Ø§Ø² min_freq Ø¨Ø§Ø± ØªÚ©Ø±Ø§Ø± Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒØ¯\n",
    "â€Œ<br>\n",
    "Û³)Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªÙˆÚ©Ù† Ø®Ø§Øµ <unk>: Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ ÛŒØ§ Ù†Ø§Ø¯Ø±\n",
    "â€Œ<br>\n",
    "Û´)Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¯ÙˆØ·Ø±ÙÙ‡:\n",
    "â€Œ<br>\n",
    "word â†’ index (string to index)\n",
    "â€Œ<br>\n",
    "index â†’ word (index to string)\n",
    "\n",
    "\n",
    "Ø¨Ø®Ø´ Ø³ÙˆÙ…)\n",
    "<br>\n",
    "Û±) ØªØ§Ø¨Ø¹ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ú©Ù‡ Ø§Ø² ÛŒÚ© Ø¬Ù…Ù„Ù‡ØŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ CBOW ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯\n",
    "<br>\n",
    "Û²)ØªØ§Ø¨Ø¹ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯ Ú©Ù‡ Ø§Ø² ÛŒÚ© Ø¬Ù…Ù„Ù‡ØŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Skip-gram ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯\n",
    "<br>\n",
    "<b>Ø¨Ø®Ø´ Ø³ÙˆÙ… Ø±Ø§ ØªÙˆØ¶ÛŒØ­ Ø¨Ø¯Ù‡ÛŒØ¯</b>\n",
    "\n",
    "<b>Ù†Ú©ØªÙ‡: Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ø¯Ø± Ú©Ø¯ Ø¨Ù‡ Ø¬Ø§ÛŒ Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ù…Ø¹Ù…ÙˆÙ„ÛŒ Ø¯Ùˆ ØªØ§Ø¨Ø¹ØŒ  Ù…ÛŒØªÙˆØ§ÛŒÙ†Ø¯ Ø¯Ùˆ ØªØ§Ø¨Ø¹ Ú¯ÙØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§Ù„Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ collate_fn Ù¾Ø§ÛŒØªÙˆØ±Ú† Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯.</b>\n",
    "\n",
    "<a>https://discuss.pytorch.org/t/custom-collate-function/145823</a>\n",
    "<br>\n",
    "Ø¨Ø®Ø´ Ú†Ù‡Ø§Ø±Ù…)\n",
    "<br>\n",
    "Ø¯Ø± Ø¢Ø®Ø± Ø¯Ø§Ø¯Ù‡ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ Ø±Ø§ Ø¯Ø± dataloader Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section2_desc"
   },
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø«Ø§Ù„\n",
    "<br>\n",
    "Ø¨Ø±Ø§ÛŒ Ø¨Ø®Ø´ Ø§ÙˆÙ„ \n",
    "\"Hello World! This is a TEST sentence, with 123 numbers.\"\n",
    "Ø¨Ù‡ Ø¹ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ\n",
    "[\"hello\", \"world\", \"this\", \"is\", \"a\", \"test\", \"sentence\", \"with\", \"123\", \"numbers\"]\n",
    "Ø´ÙˆØ¯\n",
    "\n",
    "<br>\n",
    "Ø¨Ø±Ø§ÛŒ Ø¨Ø®Ø´ Ø¯ÙˆÙ…\n",
    "<br>\n",
    "ÙˆØ±ÙˆØ¯ÛŒ:\n",
    "texts = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs\"\n",
    "]\n",
    "min_freq = 2\n",
    "<br>\n",
    "Ø®Ø±ÙˆØ¬ÛŒ:\n",
    "<br>\n",
    "Vocabulary:\n",
    "{\n",
    "    \"<unk>\": 0,\n",
    "    \"the\": 1,\n",
    "    \"sat\": 2,\n",
    "    \"on\": 3,\n",
    "    \"cat\": 4, \n",
    "    \"dog\": 5\n",
    "}\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ:</b><br>\n",
    "\n",
    "<b>ØªÙˆØ¶ÛŒØ­ Ø¨Ø®Ø´ Ø³ÙˆÙ… - ØªÙˆÙ„ÛŒØ¯ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ:</b>\n",
    "\n",
    "<b>CBOW (Continuous Bag of Words):</b>\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ´ØŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø§Ø·Ø±Ø§Ù (context) Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ù‡ Ù…Ø±Ú©Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ Ø¨Ø§ window_size=2ØŒ Ø§Ú¯Ø± Ø¬Ù…Ù„Ù‡ \"the cat sat on the\" Ø¨Ø§Ø´Ø¯:\n",
    "- Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ù‡ \"sat\" (Ù…Ø±Ú©Ø²)ØŒ Ú©Ù„Ù…Ø§Øª context Ø¹Ø¨Ø§Ø±ØªÙ†Ø¯ Ø§Ø²: [\"the\", \"cat\", \"on\", \"the\"]\n",
    "- ÙˆØ±ÙˆØ¯ÛŒ: Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† ÛŒØ§ Ù…Ø¬Ù…ÙˆØ¹ embedding Ú©Ù„Ù…Ø§Øª context\n",
    "- Ø®Ø±ÙˆØ¬ÛŒ: Ú©Ù„Ù…Ù‡ Ù…Ø±Ú©Ø²ÛŒ \"sat\"\n",
    "\n",
    "<b>Skip-gram:</b>\n",
    "Ø¨Ø±Ø¹Ú©Ø³ CBOWØŒ Ø§Ø² Ú©Ù„Ù…Ù‡ Ù…Ø±Ú©Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ø§Øª Ø§Ø·Ø±Ø§Ù Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ø§Ù† Ù…Ø«Ø§Ù„:\n",
    "- ÙˆØ±ÙˆØ¯ÛŒ: Ú©Ù„Ù…Ù‡ \"sat\"\n",
    "- Ø®Ø±ÙˆØ¬ÛŒ: Ù‡Ø± ÛŒÚ© Ø§Ø² Ú©Ù„Ù…Ø§Øª context Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡ [\"the\", \"cat\", \"on\", \"the\"]\n",
    "- Ù¾Ø³ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ù…Ù‡ Ù…Ø±Ú©Ø²ÛŒØŒ Ú†Ù†Ø¯ÛŒÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
    "\n",
    "<b>Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² collate_fn:</b>\n",
    "Ø§Ø² collate_fn Ø¯Ø± DataLoader Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… ØªØ§ batchâ€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ ÙØ±Ù…Øª Ù…Ù†Ø§Ø³Ø¨ ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†ÛŒÙ…. Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ùˆ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ tensorâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "section2_code_1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- PREPROCESSING AND VOCABULARY ---\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    # Removes punctuation, but keeps numbers and spaces\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text) \n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Test preprocessing\n",
    "test_sentence = \"Hello World! This is a TEST sentence, with 123 numbers.\"\n",
    "print(\"Input:\", test_sentence)\n",
    "print(\"Output:\", preprocess_text(test_sentence))\n",
    "print()\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=2):\n",
    "        self.min_freq = min_freq\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.word_freq = Counter()\n",
    "        \n",
    "    def build_vocab(self, texts):\n",
    "        self.word_freq.clear()\n",
    "        for text in texts:\n",
    "            tokens = preprocess_text(text)\n",
    "            self.word_freq.update(tokens)\n",
    "            \n",
    "        self.word2idx = {\"<unk>\": 0}\n",
    "        idx = 1\n",
    "        for word, freq in self.word_freq.items():\n",
    "            if freq >= self.min_freq:\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word[idx] = word\n",
    "                idx += 1\n",
    "                \n",
    "        self.idx2word[0] = \"<unk>\"\n",
    "        print(f\"âœ… Vocabulary built with {len(self.word2idx)} words (min_freq={self.min_freq})\")\n",
    "        return self\n",
    "        \n",
    "    def encode(self, word):\n",
    "        return self.word2idx.get(word, 0)\n",
    "        \n",
    "    def decode(self, idx):\n",
    "        return self.idx2word.get(idx, \"<unk>\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "# Test vocabulary\n",
    "texts = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"cats and dogs\"\n",
    "]\n",
    "\n",
    "vocab_test = Vocabulary(min_freq=2)\n",
    "vocab_test.build_vocab(texts)\n",
    "print(\"Vocabulary:\", {word: idx for word, idx in list(vocab_test.word2idx.items())[:10]})\n",
    "print()\n",
    "\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, file_path, vocab, window_size=2, model_type='cbow'):\n",
    "        self.vocab = vocab\n",
    "        self.window_size = window_size\n",
    "        self.model_type = model_type\n",
    "        self.samples = []\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                tokens = preprocess_text(line.strip())\n",
    "                \n",
    "                if len(tokens) < window_size * 2 + 1:\n",
    "                    continue\n",
    "                    \n",
    "                token_ids = [vocab.encode(token) for token in tokens]\n",
    "                \n",
    "                if model_type == 'cbow':\n",
    "                    self.samples.extend(self._generate_cbow_samples(token_ids))\n",
    "                else:\n",
    "                    self.samples.extend(self._generate_skipgram_samples(token_ids))\n",
    "                    \n",
    "    def _generate_cbow_samples(self, token_ids):\n",
    "        samples = []\n",
    "        for i in range(self.window_size, len(token_ids) - self.window_size):\n",
    "            context = []\n",
    "            for j in range(i - self.window_size, i + self.window_size + 1):\n",
    "                if j != i:\n",
    "                    context.append(token_ids[j])\n",
    "            target = token_ids[i]\n",
    "            samples.append((context, target))\n",
    "        return samples\n",
    "\n",
    "    def _generate_skipgram_samples(self, token_ids):\n",
    "        samples = []\n",
    "        for i in range(self.window_size, len(token_ids) - self.window_size):\n",
    "            center = token_ids[i]\n",
    "            for j in range(i - self.window_size, i + self.window_size + 1):\n",
    "                if j != i:\n",
    "                    context = token_ids[j]\n",
    "                    samples.append((center, context))\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "def collate_cbow(batch):\n",
    "    contexts = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "    contexts = torch.LongTensor(contexts)\n",
    "    targets = torch.LongTensor(targets)\n",
    "    return contexts, targets\n",
    "\n",
    "def collate_skipgram(batch):\n",
    "    centers = [item[0] for item in batch]\n",
    "    contexts = [item[1] for item in batch]\n",
    "    centers = torch.LongTensor(centers)\n",
    "    contexts = torch.LongTensor(contexts)\n",
    "    return centers, contexts\n",
    "\n",
    "# --- BUILD VOCABULARY ---\n",
    "\n",
    "print(\"ğŸ“š Loading data for vocabulary building...\")\n",
    "\n",
    "all_texts = []\n",
    "for file_path in [train_path, valid_path, test_path]:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        all_texts.extend(f.readlines())\n",
    "\n",
    "vocab = Vocabulary(min_freq=5)\n",
    "vocab.build_vocab(all_texts)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Sample words: {list(vocab.word2idx.keys())[:20]}\")\n",
    "print()\n",
    "\n",
    "# --- CREATE CBOW DATASETS ---\n",
    "\n",
    "print(\"ğŸ“Š Building CBOW Dataset...\")\n",
    "\n",
    "train_dataset_cbow = Word2VecDataset(train_path, vocab, window_size=2, model_type='cbow')\n",
    "valid_dataset_cbow = Word2VecDataset(valid_path, vocab, window_size=2, model_type='cbow')\n",
    "test_dataset_cbow = Word2VecDataset(test_path, vocab, window_size=2, model_type='cbow')\n",
    "\n",
    "train_loader_cbow = DataLoader(train_dataset_cbow, batch_size=8192, shuffle=True, collate_fn=collate_cbow)\n",
    "valid_loader_cbow = DataLoader(valid_dataset_cbow, batch_size=8192, shuffle=False, collate_fn=collate_cbow)\n",
    "test_loader_cbow = DataLoader(test_dataset_cbow, batch_size=8192, shuffle=False, collate_fn=collate_cbow)\n",
    "\n",
    "print(f\"CBOW samples - Train: {len(train_dataset_cbow):,}, Valid: {len(valid_dataset_cbow):,}, Test: {len(test_dataset_cbow):,}\")\n",
    "print()\n",
    "\n",
    "# --- CREATE SKIP-GRAM DATASETS ---\n",
    "\n",
    "print(\"ğŸ“Š Building Skip-gram Dataset...\")\n",
    "\n",
    "train_dataset_sg = Word2VecDataset(train_path, vocab, window_size=2, model_type='skipgram')\n",
    "valid_dataset_sg = Word2VecDataset(valid_path, vocab, window_size=2, model_type='skipgram')\n",
    "test_dataset_sg = Word2VecDataset(test_path, vocab, window_size=2, model_type='skipgram')\n",
    "\n",
    "train_loader_sg = DataLoader(train_dataset_sg, batch_size=8192, shuffle=True, collate_fn=collate_skipgram)\n",
    "valid_loader_sg = DataLoader(valid_dataset_sg, batch_size=8192, shuffle=False, collate_fn=collate_skipgram)\n",
    "test_loader_sg = DataLoader(test_dataset_sg, batch_size=8192, shuffle=False, collate_fn=collate_skipgram)\n",
    "\n",
    "print(f\"Skip-gram samples - Train: {len(train_dataset_sg):,}, Valid: {len(valid_dataset_sg):,}, Test: {len(test_dataset_sg):,}\")\n",
    "print(\"âœ… DataLoaders are ready!\")\n",
    "\n",
    "# Example batch check\n",
    "print(\"\\nğŸ“¦ Checking a batch from CBOW:\")\n",
    "contexts, targets = next(iter(train_loader_cbow))\n",
    "print(f\"Contexts Batch Shape (CBOW): {contexts.shape}\")\n",
    "print(f\"Targets Batch Shape (CBOW): {targets.shape}\")\n",
    "\n",
    "print(\"\\nğŸ“¦ Checking a batch from Skip-gram:\")\n",
    "centers, contexts_sg = next(iter(train_loader_sg))\n",
    "print(f\"Centers Batch Shape (Skip-gram): {centers.shape}\")\n",
    "print(f\"Contexts Batch Shape (Skip-gram): {contexts_sg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary Analysis and Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VOCABULARY ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Word frequency distribution\n",
    "word_frequencies = sorted(vocab.word_freq.values(), reverse=True)\n",
    "ranks = np.arange(1, len(word_frequencies) + 1)\n",
    "\n",
    "# Statistics\n",
    "total_words = sum(word_frequencies)\n",
    "unique_words = len(word_frequencies)\n",
    "vocab_coverage = len(vocab) / unique_words * 100\n",
    "\n",
    "print(f\"Total word occurrences: {total_words:,}\")\n",
    "print(f\"Unique words in corpus: {unique_words:,}\")\n",
    "print(f\"Words in vocabulary (min_freq={vocab.min_freq}): {len(vocab):,}\")\n",
    "print(f\"Vocabulary coverage: {vocab_coverage:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Top frequent words\n",
    "print(\"Top 20 most frequent words:\")\n",
    "top_words = vocab.word_freq.most_common(20)\n",
    "for i, (word, count) in enumerate(top_words, 1):\n",
    "    print(f\"  {i:2d}. {word:15s} : {count:,}\")\n",
    "print()\n",
    "\n",
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Zipf's Law - Log-log plot\n",
    "axes[0, 0].loglog(ranks, word_frequencies, 'b-', linewidth=2, alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Rank (log scale)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency (log scale)', fontsize=12)\n",
    "axes[0, 0].set_title(\"Zipf's Law: Word Frequency Distribution\", fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Top 30 words bar chart\n",
    "top_30 = vocab.word_freq.most_common(30)\n",
    "words_30, counts_30 = zip(*top_30)\n",
    "axes[0, 1].barh(range(len(words_30)), counts_30, color='skyblue', edgecolor='black')\n",
    "axes[0, 1].set_yticks(range(len(words_30)))\n",
    "axes[0, 1].set_yticklabels(words_30, fontsize=9)\n",
    "axes[0, 1].set_xlabel('Frequency', fontsize=12)\n",
    "axes[0, 1].set_title('Top 30 Most Frequent Words', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].invert_yaxis()\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 3: Frequency histogram\n",
    "freq_bins = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, max(word_frequencies)]\n",
    "freq_hist, _ = np.histogram(word_frequencies, bins=freq_bins)\n",
    "bin_labels = [f'{freq_bins[i]}-{freq_bins[i+1]}' for i in range(len(freq_bins)-1)]\n",
    "axes[1, 0].bar(range(len(freq_hist)), freq_hist, color='lightcoral', edgecolor='black')\n",
    "axes[1, 0].set_xticks(range(len(freq_hist)))\n",
    "axes[1, 0].set_xticklabels(bin_labels, rotation=45, ha='right', fontsize=9)\n",
    "axes[1, 0].set_xlabel('Frequency Range', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Number of Words', fontsize=12)\n",
    "axes[1, 0].set_title('Word Frequency Distribution (Histogram)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Cumulative coverage\n",
    "cumulative_freq = np.cumsum(word_frequencies)\n",
    "cumulative_coverage = (cumulative_freq / total_words) * 100\n",
    "axes[1, 1].plot(ranks, cumulative_coverage, 'g-', linewidth=2)\n",
    "axes[1, 1].axhline(y=50, color='r', linestyle='--', label='50% coverage', linewidth=2)\n",
    "axes[1, 1].axhline(y=80, color='orange', linestyle='--', label='80% coverage', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Number of Unique Words', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Cumulative Coverage (%)', fontsize=12)\n",
    "axes[1, 1].set_title('Cumulative Word Coverage', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xlim(0, min(10000, len(ranks)))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate coverage milestones\n",
    "for coverage_pct in [50, 80, 90, 95]:\n",
    "    coverage_idx = np.argmax(cumulative_coverage >= coverage_pct)\n",
    "    coverage_words = coverage_idx + 1\n",
    "    print(f\"{coverage_pct}% of corpus covered by top {coverage_words:,} words\")\n",
    "\n",
    "print(\"\\nâœ… Vocabulary analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Samples Comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"DATASET SAMPLES COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Collect statistics\n",
    "dataset_stats = {\n",
    "    'Train': {\n",
    "        'CBOW': len(train_dataset_cbow),\n",
    "        'Skip-gram': len(train_dataset_sg)\n",
    "    },\n",
    "    'Validation': {\n",
    "        'CBOW': len(valid_dataset_cbow),\n",
    "        'Skip-gram': len(valid_dataset_sg)\n",
    "    },\n",
    "    'Test': {\n",
    "        'CBOW': len(test_dataset_cbow),\n",
    "        'Skip-gram': len(test_dataset_sg)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Sample counts per split and model:\")\n",
    "for split, models in dataset_stats.items():\n",
    "    print(f\"\\n{split}:\")\n",
    "    for model, count in models.items():\n",
    "        print(f\"  {model:12s}: {count:,}\")\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "splits = list(dataset_stats.keys())\n",
    "cbow_counts = [dataset_stats[s]['CBOW'] for s in splits]\n",
    "sg_counts = [dataset_stats[s]['Skip-gram'] for s in splits]\n",
    "\n",
    "x = np.arange(len(splits))\n",
    "width = 0.35\n",
    "\n",
    "# Plot 1: Side-by-side comparison\n",
    "bars1 = axes[0].bar(x - width/2, cbow_counts, width, label='CBOW', \n",
    "                     color='#3498db', edgecolor='black')\n",
    "bars2 = axes[0].bar(x + width/2, sg_counts, width, label='Skip-gram',\n",
    "                     color='#e74c3c', edgecolor='black')\n",
    "\n",
    "axes[0].set_xlabel('Dataset Split', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Samples', fontsize=12)\n",
    "axes[0].set_title('Training Samples: CBOW vs Skip-gram', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(splits)\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{int(height):,}',\n",
    "                     ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: Ratio comparison\n",
    "ratios = [sg_counts[i] / cbow_counts[i] for i in range(len(splits))]\n",
    "axes[1].bar(splits, ratios, color='#2ecc71', edgecolor='black')\n",
    "axes[1].axhline(y=1, color='red', linestyle='--', linewidth=2, label='Equal (1:1)')\n",
    "axes[1].set_xlabel('Dataset Split', fontsize=12)\n",
    "axes[1].set_ylabel('Skip-gram / CBOW Ratio', fontsize=12)\n",
    "axes[1].set_title('Sample Count Ratio (Skip-gram / CBOW)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for i, (split, ratio) in enumerate(zip(splits, ratios)):\n",
    "    axes[1].text(i, ratio, f'{ratio:.2f}x',\n",
    "                 ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display total samples\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TOTAL SAMPLES:\")\n",
    "print(f\"  CBOW Total: {sum(cbow_counts):,}\")\n",
    "print(f\"  Skip-gram Total: {sum(sg_counts):,}\")\n",
    "print(f\"  Skip-gram generates {sum(sg_counts)/sum(cbow_counts):.2f}x more samples than CBOW\")\n",
    "print()\n",
    "\n",
    "# Batch information\n",
    "print(\"DATALOADER INFORMATION:\")\n",
    "print(f\"  Batch size: 512\")\n",
    "print(f\"  Train batches (CBOW): {len(train_loader_cbow):,}\")\n",
    "print(f\"  Train batches (Skip-gram): {len(train_loader_sg):,}\")\n",
    "print(f\"  Validation batches (CBOW): {len(valid_loader_cbow):,}\")\n",
    "print(f\"  Validation batches (Skip-gram): {len(valid_loader_sg):,}\")\n",
    "print(f\"  Test batches (CBOW): {len(test_loader_cbow):,}\")\n",
    "print(f\"  Test batches (Skip-gram): {len(test_loader_sg):,}\")\n",
    "\n",
    "print(\"\\nâœ… Dataset comparison visualized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Batch Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE BATCH ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Get sample batches\n",
    "contexts_cbow, targets_cbow = next(iter(train_loader_cbow))\n",
    "centers_sg, contexts_sg = next(iter(train_loader_sg))\n",
    "\n",
    "# Analyze CBOW batch\n",
    "print(\"CBOW Batch Details:\")\n",
    "print(f\"  Contexts shape: {contexts_cbow.shape}\")\n",
    "print(f\"  Targets shape: {targets_cbow.shape}\")\n",
    "print(f\"  Context window size: {contexts_cbow.shape[1] // 2}\")\n",
    "print(f\"  Example context indices: {contexts_cbow[0].tolist()}\")\n",
    "print(f\"  Example target index: {targets_cbow[0].item()}\")\n",
    "print(f\"  Decoded context words: {[vocab.decode(idx.item()) for idx in contexts_cbow[0]]}\")\n",
    "print(f\"  Decoded target word: {vocab.decode(targets_cbow[0].item())}\")\n",
    "print()\n",
    "\n",
    "print(\"Skip-gram Batch Details:\")\n",
    "print(f\"  Centers shape: {centers_sg.shape}\")\n",
    "print(f\"  Contexts shape: {contexts_sg.shape}\")\n",
    "print(f\"  Example center index: {centers_sg[0].item()}\")\n",
    "print(f\"  Example context index: {contexts_sg[0].item()}\")\n",
    "print(f\"  Decoded center word: {vocab.decode(centers_sg[0].item())}\")\n",
    "print(f\"  Decoded context word: {vocab.decode(contexts_sg[0].item())}\")\n",
    "print()\n",
    "\n",
    "# Visualize sample examples\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "\n",
    "# CBOW visualization\n",
    "num_samples_to_show = 10\n",
    "cbow_examples = []\n",
    "for i in range(num_samples_to_show):\n",
    "    context_words = [vocab.decode(idx.item()) for idx in contexts_cbow[i]]\n",
    "    target_word = vocab.decode(targets_cbow[i].item())\n",
    "    cbow_examples.append((context_words, target_word))\n",
    "\n",
    "# Create CBOW visualization\n",
    "y_pos = np.arange(num_samples_to_show)\n",
    "axes[0].set_ylim(-1, num_samples_to_show)\n",
    "axes[0].set_xlim(-1, 6)\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('CBOW: Context Words â†’ Target Word', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "for i, (context, target) in enumerate(cbow_examples):\n",
    "    y = num_samples_to_show - i - 1\n",
    "    \n",
    "    # Draw context words\n",
    "    for j, word in enumerate(context):\n",
    "        axes[0].add_patch(plt.Rectangle((j * 0.9, y - 0.3), 0.8, 0.6, \n",
    "                                         facecolor='lightblue', edgecolor='black', linewidth=1.5))\n",
    "        axes[0].text(j * 0.9 + 0.4, y, word, ha='center', va='center', \n",
    "                     fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Draw arrow\n",
    "    axes[0].annotate('', xy=(4.5, y), xytext=(3.8, y),\n",
    "                     arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
    "    \n",
    "    # Draw target\n",
    "    axes[0].add_patch(plt.Rectangle((4.6, y - 0.3), 0.8, 0.6,\n",
    "                                     facecolor='lightcoral', edgecolor='black', linewidth=2))\n",
    "    axes[0].text(5.0, y, target, ha='center', va='center',\n",
    "                 fontsize=10, fontweight='bold')\n",
    "\n",
    "# Skip-gram visualization\n",
    "sg_examples = []\n",
    "for i in range(num_samples_to_show):\n",
    "    center_word = vocab.decode(centers_sg[i].item())\n",
    "    context_word = vocab.decode(contexts_sg[i].item())\n",
    "    sg_examples.append((center_word, context_word))\n",
    "\n",
    "axes[1].set_ylim(-1, num_samples_to_show)\n",
    "axes[1].set_xlim(-1, 4)\n",
    "axes[1].axis('off')\n",
    "axes[1].set_title('Skip-gram: Center Word â†’ Context Word', fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "for i, (center, context) in enumerate(sg_examples):\n",
    "    y = num_samples_to_show - i - 1\n",
    "    \n",
    "    # Draw center word\n",
    "    axes[1].add_patch(plt.Rectangle((0.5, y - 0.3), 1.0, 0.6,\n",
    "                                     facecolor='lightgreen', edgecolor='black', linewidth=2))\n",
    "    axes[1].text(1.0, y, center, ha='center', va='center',\n",
    "                 fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Draw arrow\n",
    "    axes[1].annotate('', xy=(2.5, y), xytext=(1.6, y),\n",
    "                     arrowprops=dict(arrowstyle='->', lw=2, color='blue'))\n",
    "    \n",
    "    # Draw context word\n",
    "    axes[1].add_patch(plt.Rectangle((2.6, y - 0.3), 1.0, 0.6,\n",
    "                                     facecolor='lightyellow', edgecolor='black', linewidth=1.5))\n",
    "    axes[1].text(3.1, y, context, ha='center', va='center',\n",
    "                 fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Sample batch visualization completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¨Ú©Ù‡ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "<p style=\"line-height: 1.8; text-align: right;\">\n",
    "Ø­Ø§Ù„ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ SkipGram Ùˆ CBOW Ø±Ø§ Ù…Ø§Ù†Ù†Ø¯ Ù…Ù‚Ø§Ù„Ù‡ (ÛŒØ§ Ú©ØªØ§Ø¨ Ø¯Ø±Ø³ÛŒ) Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ú©Ù†ÛŒØ¯ </p>\n",
    "Ø´Ø¨Ú©Ù‡ Ø±Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ú¯Ø§Ù† Train, Validation & Test Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ø·Ø§ ØªØ±Ø³ÛŒÙ… Ú©Ù†ÛŒØ¯.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/cg/l2rdx46d6lv3b5xc17b420yc0000gn/T/ipykernel_29793/3576485599.py\", line 4, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/__init__.py\", line 129, in <module>\n",
      "    from . import _api, _version, cbook, _docstring, rcsetup\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/rcsetup.py\", line 27, in <module>\n",
      "    from matplotlib.colors import Colormap, is_color_like\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/colors.py\", line 56, in <module>\n",
      "    from matplotlib import _api, _cm, cbook, scale\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/scale.py\", line 22, in <module>\n",
      "    from matplotlib.ticker import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/ticker.py\", line 138, in <module>\n",
      "    from matplotlib import transforms as mtransforms\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/transforms.py\", line 49, in <module>\n",
      "    from matplotlib._path import (\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCBOW\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/__init__.py:129\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sanitize_sequence\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/rcsetup.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/colors.py:56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _cm, cbook, scale\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/scale.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, _docstring\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mticker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     NullFormatter, ScalarFormatter, LogFormatterSciNotation, LogitFormatter,\n\u001b[1;32m     24\u001b[0m     NullLocator, LogLocator, AutoLocator, AutoMinorLocator,\n\u001b[1;32m     25\u001b[0m     SymmetricalLogLocator, AsinhLocator, LogitLocator)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Transform, IdentityTransform\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mScaleBase\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/ticker.py:138\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api, cbook\n\u001b[0;32m--> 138\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms \u001b[38;5;28;01mas\u001b[39;00m mtransforms\n\u001b[1;32m    140\u001b[0m _log \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    142\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTickHelper\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixedFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    143\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNullFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuncFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFormatStrFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    144\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStrMethodFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScalarFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogFormatter\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMultipleLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaxNLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAutoMinorLocator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    151\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymmetricalLogLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsinhLocator\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLogitLocator\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/matplotlib/transforms.py:49\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m inv\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _api\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_path\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     50\u001b[0m     affine_transform, count_bboxes_overlapping_bbox, update_path_extents)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     53\u001b[0m DEBUG \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        # Better initialization\n",
    "        nn.init.xavier_uniform_(self.embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "    \n",
    "    def forward(self, context):\n",
    "        embeds = self.embeddings(context)\n",
    "        embeds = torch.mean(embeds, dim=1)\n",
    "        out = self.linear(embeds)\n",
    "        return out\n",
    "\n",
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        # Better initialization\n",
    "        nn.init.xavier_uniform_(self.embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "    \n",
    "    def forward(self, center):\n",
    "        embeds = self.embeddings(center)\n",
    "        out = self.linear(embeds)\n",
    "        return out\n",
    "\n",
    "def train_model_with_visualization(model, train_loader, valid_loader, test_loader, epochs=20, lr=0.001, device='cpu', model_name='Model'):\n",
    "    \"\"\"Training function with real-time visualization and early stopping\"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)  # Add weight decay\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    train_losses, valid_losses, test_losses = [], [], []\n",
    "    epoch_times = []\n",
    "    best_valid_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    max_patience = 5\n",
    "    \n",
    "    print(f\"ğŸš€ Training {model_name}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Setup live plotting\n",
    "    plt.ion()  # Interactive mode\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        batch_losses = []\n",
    "        \n",
    "        # Progress bar for training batches\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\", \n",
    "                   leave=False, ncols=100, colour='blue')\n",
    "        \n",
    "        for inputs, targets in pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_loss = loss.item()\n",
    "            train_loss += batch_loss\n",
    "            batch_losses.append(batch_loss)\n",
    "            \n",
    "            # Update progress bar with current batch loss\n",
    "            pbar.set_postfix({'batch_loss': f'{batch_loss:.4f}'})\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                valid_loss += loss.item()\n",
    "        avg_valid_loss = valid_loss / len(valid_loader)\n",
    "        valid_losses.append(avg_valid_loss)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_valid_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = avg_valid_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Test evaluation\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "        test_losses.append(avg_test_loss)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        epoch_times.append(epoch_time)\n",
    "        \n",
    "        # Real-time visualization update\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        # Plot 1: Loss curves\n",
    "        axes[0].clear()\n",
    "        axes[0].plot(train_losses, label='Train', linewidth=2.5, color='#2E86AB', marker='o')\n",
    "        axes[0].plot(valid_losses, label='Valid', linewidth=2.5, color='#F18F01', marker='s')\n",
    "        axes[0].plot(test_losses, label='Test', linewidth=2.5, color='#C73E1D', marker='^')\n",
    "        axes[0].axhline(y=best_valid_loss, color='green', linestyle='--', linewidth=2, alpha=0.5, label=f'Best Valid: {best_valid_loss:.4f}')\n",
    "        axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_title(f'{model_name} - Loss Progress', fontsize=14, fontweight='bold')\n",
    "        axes[0].legend(fontsize=10)\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        axes[0].set_facecolor('#f8f9fa')\n",
    "        \n",
    "        # Plot 2: Recent batch losses\n",
    "        axes[1].clear()\n",
    "        recent_batches = batch_losses[-50:] if len(batch_losses) > 50 else batch_losses\n",
    "        axes[1].plot(recent_batches, linewidth=2, color='#6A4C93', alpha=0.7)\n",
    "        axes[1].axhline(y=avg_train_loss, color='red', linestyle='--', linewidth=2, label=f'Epoch Avg: {avg_train_loss:.4f}')\n",
    "        axes[1].set_xlabel('Recent Batches', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_ylabel('Batch Loss', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_title(f'Epoch {epoch+1} - Batch Loss Detail', fontsize=14, fontweight='bold')\n",
    "        axes[1].legend(fontsize=11)\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        axes[1].set_facecolor('#f8f9fa')\n",
    "        \n",
    "        # Plot 3: Generalization gap\n",
    "        axes[2].clear()\n",
    "        if len(valid_losses) > 0:\n",
    "            gap = [v - t for v, t in zip(valid_losses, train_losses)]\n",
    "            axes[2].plot(gap, linewidth=2.5, color='#E63946', marker='o')\n",
    "            axes[2].axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "            axes[2].fill_between(range(len(gap)), gap, alpha=0.3, color='#E63946')\n",
    "        axes[2].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "        axes[2].set_ylabel('Valid - Train Loss', fontsize=12, fontweight='bold')\n",
    "        axes[2].set_title('Generalization Gap (Overfitting Monitor)', fontsize=14, fontweight='bold')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        axes[2].set_facecolor('#f8f9fa')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.draw()\n",
    "        plt.pause(0.01)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        improvement = \"ğŸ“ˆ\" if len(valid_losses) > 1 and avg_valid_loss < valid_losses[-2] else \"ğŸ“‰\"\n",
    "        print(f\"\\nğŸš€ Training {model_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Epoch {epoch+1:2d}/{epochs}\")\n",
    "        print(f\"  ğŸ“‰ Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  ğŸ“Š Valid Loss: {avg_valid_loss:.4f} {improvement}\")\n",
    "        print(f\"  ğŸ¯ Test Loss:  {avg_test_loss:.4f}\")\n",
    "        print(f\"  ğŸŒŸ Best Valid: {best_valid_loss:.4f}\")\n",
    "        print(f\"  ğŸ”§ Learning Rate: {current_lr:.6f}\")\n",
    "        print(f\"  â±ï¸  Time: {epoch_time:.1f}s\")\n",
    "        print(f\"  â³ Patience: {patience_counter}/{max_patience}\")\n",
    "        print(f\"  âš¡ Progress: [{'â–ˆ' * int((epoch+1)/epochs*30)}{' ' * (30-int((epoch+1)/epochs*30))}] {(epoch+1)/epochs*100:.1f}%\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= max_patience:\n",
    "            print(f\"\\nâš ï¸  Early stopping triggered! No improvement for {max_patience} epochs.\")\n",
    "            print(f\"  Loading best model (valid loss: {best_valid_loss:.4f})\")\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "    \n",
    "    plt.ioff()  # Turn off interactive mode\n",
    "    plt.savefig(f'{model_name.lower()}_training_live.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nâœ… {model_name} training completed!\")\n",
    "    print(f\"  Total time: {total_time:.1f}s\")\n",
    "    print(f\"  Epochs trained: {len(train_losses)}\")\n",
    "    print(f\"  Average time per epoch: {total_time/len(train_losses):.1f}s\")\n",
    "    print(f\"  Final train loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"  Final valid loss: {valid_losses[-1]:.4f}\")\n",
    "    print(f\"  Final test loss: {test_losses[-1]:.4f}\")\n",
    "    print(f\"  Best valid loss: {best_valid_loss:.4f}\")\n",
    "    print(f\"  Loss reduction: {train_losses[0] - train_losses[-1]:.4f} ({(train_losses[0] - train_losses[-1])/train_losses[0]*100:.1f}%)\")\n",
    "    print()\n",
    "    \n",
    "    return train_losses, valid_losses, test_losses, epoch_times\n",
    "\n",
    "# Hyperparameters - optimized for better convergence\n",
    "embedding_dim = 100\n",
    "epochs = 30  # More epochs with early stopping\n",
    "lr = 0.001  # Good starting learning rate\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âš™ï¸  TRAINING CONFIGURATION\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"  ğŸ–¥ï¸  Device: {device}\")\n",
    "print(f\"  ğŸ“š Vocabulary size: {len(vocab):,}\")\n",
    "print(f\"  ğŸ¯ Embedding dimension: {embedding_dim}\")\n",
    "print(f\"  ğŸ“ˆ Max training epochs: {epochs}\")\n",
    "print(f\"  ğŸ”§ Initial learning rate: {lr}\")\n",
    "print(f\"  ğŸ›¡ï¸  Weight decay: 1e-5\")\n",
    "print(f\"  ğŸ“¦ Batch size: {train_loader_cbow.batch_size}\")\n",
    "print(f\"  ğŸšï¸  LR scheduler: ReduceLROnPlateau\")\n",
    "print(f\"  ğŸ›‘ Early stopping patience: 5 epochs\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Train CBOW model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”µ STARTING CBOW TRAINING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "cbow_model = CBOW(len(vocab), embedding_dim)\n",
    "cbow_train_losses, cbow_valid_losses, cbow_test_losses, cbow_times = train_model_with_visualization(\n",
    "    cbow_model, train_loader_cbow, valid_loader_cbow, test_loader_cbow,\n",
    "    epochs=epochs, lr=lr, device=device, model_name='CBOW'\n",
    ")\n",
    "\n",
    "# Train Skip-gram model\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ”´ STARTING SKIP-GRAM TRAINING\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "skipgram_model = SkipGram(len(vocab), embedding_dim)\n",
    "sg_train_losses, sg_valid_losses, sg_test_losses, sg_times = train_model_with_visualization(\n",
    "    skipgram_model, train_loader_sg, valid_loader_sg, test_loader_sg,\n",
    "    epochs=epochs, lr=lr, device=device, model_name='Skip-gram'\n",
    ")\n",
    "\n",
    "# Final comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Loss comparison\n",
    "axes[0].plot(cbow_train_losses, label='CBOW Train', linewidth=2.5, color='#2E86AB', marker='o')\n",
    "axes[0].plot(cbow_valid_losses, label='CBOW Valid', linewidth=2.5, color='#2E86AB', linestyle='--', alpha=0.7)\n",
    "axes[0].plot(sg_train_losses, label='SG Train', linewidth=2.5, color='#A23B72', marker='s')\n",
    "axes[0].plot(sg_valid_losses, label='SG Valid', linewidth=2.5, color='#A23B72', linestyle='--', alpha=0.7)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Training & Validation Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_facecolor('#f8f9fa')\n",
    "\n",
    "# Test loss comparison\n",
    "axes[1].plot(cbow_test_losses, label='CBOW', linewidth=2.5, marker='o', color='#2E86AB', markersize=8)\n",
    "axes[1].plot(sg_test_losses, label='Skip-gram', linewidth=2.5, marker='s', color='#A23B72', markersize=8)\n",
    "axes[1].set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Test Loss', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Test Loss Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_facecolor('#f8f9fa')\n",
    "\n",
    "# Training time comparison\n",
    "total_cbow_time = np.sum(cbow_times)\n",
    "total_sg_time = np.sum(sg_times)\n",
    "axes[2].bar(['CBOW', 'Skip-gram'], \n",
    "            [total_cbow_time, total_sg_time],\n",
    "            color=['#2E86AB', '#A23B72'], edgecolor='black', alpha=0.7, width=0.6)\n",
    "axes[2].set_ylabel('Total Time (seconds)', fontsize=12, fontweight='bold')\n",
    "axes[2].set_title('Total Training Time', fontsize=14, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "for i, (name, time_val) in enumerate(zip(['CBOW', 'Skip-gram'], [total_cbow_time, total_sg_time])):\n",
    "    axes[2].text(i, time_val, f'{time_val:.1f}s\\n({len(cbow_times if i==0 else sg_times)} epochs)', \n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "axes[2].set_facecolor('#f8f9fa')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('word2vec_final_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ ALL TRAINING COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nğŸ“Š FINAL RESULTS SUMMARY:\")\n",
    "print(f\"  CBOW:\")\n",
    "print(f\"    â€¢ Epochs trained: {len(cbow_train_losses)}\")\n",
    "print(f\"    â€¢ Final Train Loss: {cbow_train_losses[-1]:.4f}\")\n",
    "print(f\"    â€¢ Final Valid Loss: {cbow_valid_losses[-1]:.4f}\")\n",
    "print(f\"    â€¢ Final Test Loss: {cbow_test_losses[-1]:.4f}\")\n",
    "print(f\"    â€¢ Total Time: {np.sum(cbow_times):.1f}s\")\n",
    "print(f\"\\n  Skip-gram:\")\n",
    "print(f\"    â€¢ Epochs trained: {len(sg_train_losses)}\")\n",
    "print(f\"    â€¢ Final Train Loss: {sg_train_losses[-1]:.4f}\")\n",
    "print(f\"    â€¢ Final Valid Loss: {sg_valid_losses[-1]:.4f}\")\n",
    "print(f\"    â€¢ Final Test Loss: {sg_test_losses[-1]:.4f}\")\n",
    "print(f\"    â€¢ Total Time: {np.sum(sg_times):.1f}s\")\n",
    "print(f\"\\n{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Detailed Loss Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Training progression\n",
    "axes[0, 0].plot(cbow_train_losses, label='CBOW', linewidth=2.5, color='#2E86AB', marker='o', markersize=4)\n",
    "axes[0, 0].plot(sg_train_losses, label='Skip-gram', linewidth=2.5, color='#A23B72', marker='s', markersize=4)\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Training Loss', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_title('Training Loss Progression', fontsize=15, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=12)\n",
    "axes[0, 0].grid(True, alpha=0.3, linestyle='--')\n",
    "axes[0, 0].set_facecolor('#f8f9fa')\n",
    "\n",
    "# Validation progression\n",
    "axes[0, 1].plot(cbow_valid_losses, label='CBOW', linewidth=2.5, color='#2E86AB', marker='o', markersize=4)\n",
    "axes[0, 1].plot(sg_valid_losses, label='Skip-gram', linewidth=2.5, color='#A23B72', marker='s', markersize=4)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Validation Loss', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_title('Validation Loss Progression', fontsize=15, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=12)\n",
    "axes[0, 1].grid(True, alpha=0.3, linestyle='--')\n",
    "axes[0, 1].set_facecolor('#f8f9fa')\n",
    "\n",
    "# Loss improvement rate (derivative)\n",
    "cbow_improvement = -np.diff(cbow_train_losses)\n",
    "sg_improvement = -np.diff(sg_train_losses)\n",
    "axes[1, 0].plot(range(1, epochs), cbow_improvement, label='CBOW', linewidth=2.5, color='#2E86AB', alpha=0.8)\n",
    "axes[1, 0].plot(range(1, epochs), sg_improvement, label='Skip-gram', linewidth=2.5, color='#A23B72', alpha=0.8)\n",
    "axes[1, 0].axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Loss Reduction', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_title('Training Loss Improvement Rate', fontsize=15, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=12)\n",
    "axes[1, 0].grid(True, alpha=0.3, linestyle='--')\n",
    "axes[1, 0].set_facecolor('#f8f9fa')\n",
    "\n",
    "# Generalization gap (train - valid)\n",
    "cbow_gap = np.array(cbow_valid_losses) - np.array(cbow_train_losses)\n",
    "sg_gap = np.array(sg_valid_losses) - np.array(sg_train_losses)\n",
    "axes[1, 1].plot(cbow_gap, label='CBOW', linewidth=2.5, color='#2E86AB', marker='o', markersize=4)\n",
    "axes[1, 1].plot(sg_gap, label='Skip-gram', linewidth=2.5, color='#A23B72', marker='s', markersize=4)\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Generalization Gap (Valid - Train)', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_title('Overfitting Analysis', fontsize=15, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=12)\n",
    "axes[1, 1].grid(True, alpha=0.3, linestyle='--')\n",
    "axes[1, 1].set_facecolor('#f8f9fa')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('word2vec_detailed_loss_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š Detailed Loss Analysis:\")\n",
    "print(f\"  CBOW - Final Train Loss: {cbow_train_losses[-1]:.4f}, Valid Loss: {cbow_valid_losses[-1]:.4f}\")\n",
    "print(f\"  Skip-gram - Final Train Loss: {sg_train_losses[-1]:.4f}, Valid Loss: {sg_valid_losses[-1]:.4f}\")\n",
    "print(f\"  CBOW - Loss Reduction: {cbow_train_losses[0] - cbow_train_losses[-1]:.4f} ({((cbow_train_losses[0] - cbow_train_losses[-1])/cbow_train_losses[0]*100):.1f}%)\")\n",
    "print(f\"  Skip-gram - Loss Reduction: {sg_train_losses[0] - sg_train_losses[-1]:.4f} ({((sg_train_losses[0] - sg_train_losses[-1])/sg_train_losses[0]*100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 2. Embedding Space Visualization (t-SNE)\n",
    "print(\"ğŸ¨ Visualizing embedding spaces...\")\n",
    "\n",
    "# Get embeddings for common words\n",
    "sample_words = ['the', 'of', 'and', 'to', 'in', 'a', 'is', 'that', 'for', 'it',\n",
    "                'with', 'as', 'was', 'on', 'be', 'by', 'at', 'from', 'or', 'an',\n",
    "                'which', 'this', 'but', 'are', 'not', 'his', 'they', 'were', 'have', 'had']\n",
    "sample_indices = [vocab.word2idx[w] for w in sample_words if w in vocab.word2idx]\n",
    "\n",
    "if len(sample_indices) >= 20:\n",
    "    # Extract embeddings\n",
    "    cbow_embeddings = cbow_model.embeddings.weight.data.cpu().numpy()[sample_indices]\n",
    "    sg_embeddings = skipgram_model.embeddings.weight.data.cpu().numpy()[sample_indices]\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(15, len(sample_indices)-1))\n",
    "    cbow_2d = tsne.fit_transform(cbow_embeddings)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(15, len(sample_indices)-1))\n",
    "    sg_2d = tsne.fit_transform(sg_embeddings)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "    \n",
    "    # CBOW embeddings\n",
    "    axes[0].scatter(cbow_2d[:, 0], cbow_2d[:, 1], c='#2E86AB', s=200, alpha=0.6, edgecolors='black', linewidth=1.5)\n",
    "    for i, word in enumerate([vocab.idx2word[idx] for idx in sample_indices]):\n",
    "        axes[0].annotate(word, (cbow_2d[i, 0], cbow_2d[i, 1]), fontsize=11, fontweight='bold',\n",
    "                        xytext=(5, 5), textcoords='offset points')\n",
    "    axes[0].set_xlabel('t-SNE Dimension 1', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_ylabel('t-SNE Dimension 2', fontsize=13, fontweight='bold')\n",
    "    axes[0].set_title('CBOW Embedding Space (t-SNE)', fontsize=15, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3, linestyle='--')\n",
    "    axes[0].set_facecolor('#f8f9fa')\n",
    "    \n",
    "    # Skip-gram embeddings\n",
    "    axes[1].scatter(sg_2d[:, 0], sg_2d[:, 1], c='#A23B72', s=200, alpha=0.6, edgecolors='black', linewidth=1.5)\n",
    "    for i, word in enumerate([vocab.idx2word[idx] for idx in sample_indices]):\n",
    "        axes[1].annotate(word, (sg_2d[i, 0], sg_2d[i, 1]), fontsize=11, fontweight='bold',\n",
    "                        xytext=(5, 5), textcoords='offset points')\n",
    "    axes[1].set_xlabel('t-SNE Dimension 1', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_ylabel('t-SNE Dimension 2', fontsize=13, fontweight='bold')\n",
    "    axes[1].set_title('Skip-gram Embedding Space (t-SNE)', fontsize=15, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3, linestyle='--')\n",
    "    axes[1].set_facecolor('#f8f9fa')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('word2vec_embedding_space.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"âœ… Embedding space visualization completed!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Not enough sample words found in vocabulary for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 3. Model Quality Metrics\n",
    "print(\"ğŸ“ˆ Computing model quality metrics...\")\n",
    "\n",
    "# Cosine similarity analysis for common word pairs\n",
    "def compute_similarity(model, word1_idx, word2_idx):\n",
    "    \"\"\"Compute cosine similarity between two word embeddings\"\"\"\n",
    "    emb1 = model.embeddings.weight.data[word1_idx]\n",
    "    emb2 = model.embeddings.weight.data[word2_idx]\n",
    "    return F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
    "\n",
    "# Test word pairs (related words)\n",
    "test_pairs = [\n",
    "    ('the', 'a'), ('is', 'was'), ('and', 'or'), ('in', 'on'),\n",
    "    ('to', 'from'), ('it', 'they'), ('have', 'had'), ('be', 'was')\n",
    "]\n",
    "\n",
    "cbow_similarities = []\n",
    "sg_similarities = []\n",
    "\n",
    "for w1, w2 in test_pairs:\n",
    "    if w1 in vocab.word2idx and w2 in vocab.word2idx:\n",
    "        idx1, idx2 = vocab.word2idx[w1], vocab.word2idx[w2]\n",
    "        cbow_sim = compute_similarity(cbow_model, idx1, idx2)\n",
    "        sg_sim = compute_similarity(skipgram_model, idx1, idx2)\n",
    "        cbow_similarities.append(cbow_sim)\n",
    "        sg_similarities.append(sg_sim)\n",
    "\n",
    "# Visualize comparisons\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 3.1 Similarity comparison\n",
    "x_pos = np.arange(len(cbow_similarities))\n",
    "width = 0.35\n",
    "axes[0, 0].bar(x_pos - width/2, cbow_similarities, width, label='CBOW', color='#2E86AB', edgecolor='black', alpha=0.8)\n",
    "axes[0, 0].bar(x_pos + width/2, sg_similarities, width, label='Skip-gram', color='#A23B72', edgecolor='black', alpha=0.8)\n",
    "axes[0, 0].set_xlabel('Word Pair', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Cosine Similarity', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_title('Word Pair Similarity Comparison', fontsize=15, fontweight='bold')\n",
    "axes[0, 0].set_xticks(x_pos)\n",
    "axes[0, 0].set_xticklabels([f\"{w1}-{w2}\" for w1, w2 in test_pairs[:len(cbow_similarities)]], rotation=45, ha='right')\n",
    "axes[0, 0].legend(fontsize=12)\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "axes[0, 0].set_facecolor('#f8f9fa')\n",
    "\n",
    "# 3.2 Embedding norm distribution\n",
    "cbow_norms = torch.norm(cbow_model.embeddings.weight.data, dim=1).cpu().numpy()\n",
    "sg_norms = torch.norm(skipgram_model.embeddings.weight.data, dim=1).cpu().numpy()\n",
    "axes[0, 1].hist(cbow_norms, bins=50, alpha=0.6, label='CBOW', color='#2E86AB', edgecolor='black')\n",
    "axes[0, 1].hist(sg_norms, bins=50, alpha=0.6, label='Skip-gram', color='#A23B72', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Embedding Norm', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_title('Embedding Norm Distribution', fontsize=15, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=12)\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "axes[0, 1].set_facecolor('#f8f9fa')\n",
    "\n",
    "# 3.3 Training efficiency (loss per time)\n",
    "cbow_efficiency = [(l, t) for l, t in zip(cbow_train_losses, np.cumsum(cbow_times))]\n",
    "sg_efficiency = [(l, t) for l, t in zip(sg_train_losses, np.cumsum(sg_times))]\n",
    "axes[1, 0].plot([t for l, t in cbow_efficiency], [l for l, t in cbow_efficiency], \n",
    "                label='CBOW', linewidth=2.5, color='#2E86AB', marker='o', markersize=5)\n",
    "axes[1, 0].plot([t for l, t in sg_efficiency], [l for l, t in sg_efficiency],\n",
    "                label='Skip-gram', linewidth=2.5, color='#A23B72', marker='s', markersize=5)\n",
    "axes[1, 0].set_xlabel('Training Time (seconds)', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Training Loss', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_title('Training Efficiency (Loss vs Time)', fontsize=15, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=12)\n",
    "axes[1, 0].grid(True, alpha=0.3, linestyle='--')\n",
    "axes[1, 0].set_facecolor('#f8f9fa')\n",
    "\n",
    "# 3.4 Final performance summary\n",
    "metrics = {\n",
    "    'Final Train Loss': [cbow_train_losses[-1], sg_train_losses[-1]],\n",
    "    'Final Valid Loss': [cbow_valid_losses[-1], sg_valid_losses[-1]],\n",
    "    'Final Test Loss': [cbow_test_losses[-1], sg_test_losses[-1]],\n",
    "    'Avg Similarity': [np.mean(cbow_similarities), np.mean(sg_similarities)]\n",
    "}\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "for i, (metric_name, values) in enumerate(metrics.items()):\n",
    "    axes[1, 1].bar(i - width/2, values[0], width, color='#2E86AB', edgecolor='black', alpha=0.8)\n",
    "    axes[1, 1].bar(i + width/2, values[1], width, color='#A23B72', edgecolor='black', alpha=0.8)\n",
    "    # Add value labels\n",
    "    axes[1, 1].text(i - width/2, values[0], f'{values[0]:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    axes[1, 1].text(i + width/2, values[1], f'{values[1]:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "axes[1, 1].set_ylabel('Value', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_title('Final Performance Summary', fontsize=15, fontweight='bold')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(metrics.keys(), rotation=20, ha='right', fontsize=11)\n",
    "axes[1, 1].legend(['CBOW', 'Skip-gram'], fontsize=12)\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "axes[1, 1].set_facecolor('#f8f9fa')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('word2vec_quality_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Quality Metrics Summary:\")\n",
    "print(f\"  CBOW - Avg embedding norm: {np.mean(cbow_norms):.4f}, Avg similarity: {np.mean(cbow_similarities):.4f}\")\n",
    "print(f\"  Skip-gram - Avg embedding norm: {np.mean(sg_norms):.4f}, Avg similarity: {np.mean(sg_similarities):.4f}\")\n",
    "print(f\"  Total training time - CBOW: {np.sum(cbow_times):.1f}s, Skip-gram: {np.sum(sg_times):.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 4. Convergence Analysis & Model Comparison\n",
    "print(\"ğŸ” Analyzing convergence behavior...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 4.1 Convergence speed (epochs to reach 95% of final loss)\n",
    "def find_convergence_epoch(losses, threshold=0.95):\n",
    "    \"\"\"Find epoch where loss reaches 95% of final loss reduction\"\"\"\n",
    "    initial = losses[0]\n",
    "    final = losses[-1]\n",
    "    target = initial - (initial - final) * threshold\n",
    "    for i, loss in enumerate(losses):\n",
    "        if loss <= target:\n",
    "            return i\n",
    "    return len(losses) - 1\n",
    "\n",
    "cbow_conv = find_convergence_epoch(cbow_train_losses)\n",
    "sg_conv = find_convergence_epoch(sg_train_losses)\n",
    "\n",
    "axes[0, 0].bar(['CBOW', 'Skip-gram'], [cbow_conv, sg_conv], \n",
    "               color=['#2E86AB', '#A23B72'], edgecolor='black', alpha=0.8, width=0.6)\n",
    "axes[0, 0].set_ylabel('Epochs to Convergence', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_title('Convergence Speed\\n(95% of final loss)', fontsize=15, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "for i, (name, val) in enumerate(zip(['CBOW', 'Skip-gram'], [cbow_conv, sg_conv])):\n",
    "    axes[0, 0].text(i, val, f'{val} epochs', ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "axes[0, 0].set_facecolor('#f8f9fa')\n",
    "\n",
    "# 4.2 Relative loss reduction over time\n",
    "cbow_relative = [(l - cbow_train_losses[-1]) / cbow_train_losses[0] for l in cbow_train_losses]\n",
    "sg_relative = [(l - sg_train_losses[-1]) / sg_train_losses[0] for l in sg_train_losses]\n",
    "axes[0, 1].plot(cbow_relative, label='CBOW', linewidth=2.5, color='#2E86AB', marker='o', markersize=5)\n",
    "axes[0, 1].plot(sg_relative, label='Skip-gram', linewidth=2.5, color='#A23B72', marker='s', markersize=5)\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Relative Loss (normalized)', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_title('Normalized Convergence Curve', fontsize=15, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=12)\n",
    "axes[0, 1].grid(True, alpha=0.3, linestyle='--')\n",
    "axes[0, 1].set_facecolor('#f8f9fa')\n",
    "\n",
    "# 4.3 Loss variance (stability)\n",
    "def moving_variance(data, window=3):\n",
    "    \"\"\"Calculate moving variance\"\"\"\n",
    "    variances = []\n",
    "    for i in range(len(data) - window + 1):\n",
    "        variances.append(np.var(data[i:i+window]))\n",
    "    return variances\n",
    "\n",
    "cbow_var = moving_variance(cbow_train_losses)\n",
    "sg_var = moving_variance(sg_train_losses)\n",
    "axes[0, 2].plot(range(len(cbow_var)), cbow_var, label='CBOW', linewidth=2.5, color='#2E86AB')\n",
    "axes[0, 2].plot(range(len(sg_var)), sg_var, label='Skip-gram', linewidth=2.5, color='#A23B72')\n",
    "axes[0, 2].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Loss Variance (window=3)', fontsize=13, fontweight='bold')\n",
    "axes[0, 2].set_title('Training Stability', fontsize=15, fontweight='bold')\n",
    "axes[0, 2].legend(fontsize=12)\n",
    "axes[0, 2].grid(True, alpha=0.3, linestyle='--')\n",
    "axes[0, 2].set_facecolor('#f8f9fa')\n",
    "\n",
    "# 4.4 Cumulative performance gain\n",
    "cbow_gain = np.cumsum([cbow_train_losses[0] - l for l in cbow_train_losses])\n",
    "sg_gain = np.cumsum([sg_train_losses[0] - l for l in sg_train_losses])\n",
    "axes[1, 0].fill_between(range(epochs), cbow_gain, alpha=0.4, color='#2E86AB', label='CBOW')\n",
    "axes[1, 0].fill_between(range(epochs), sg_gain, alpha=0.4, color='#A23B72', label='Skip-gram')\n",
    "axes[1, 0].plot(cbow_gain, linewidth=2, color='#2E86AB')\n",
    "axes[1, 0].plot(sg_gain, linewidth=2, color='#A23B72')\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Cumulative Loss Reduction', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_title('Cumulative Performance Gain', fontsize=15, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=12)\n",
    "axes[1, 0].grid(True, alpha=0.3, linestyle='--')\n",
    "axes[1, 0].set_facecolor('#f8f9fa')\n",
    "\n",
    "# 4.5 Learning rate effectiveness (loss per epoch)\n",
    "cbow_effectiveness = np.diff(cbow_train_losses) / np.array(cbow_times[:-1])\n",
    "sg_effectiveness = np.diff(sg_train_losses) / np.array(sg_times[:-1])\n",
    "axes[1, 1].plot(range(1, epochs), cbow_effectiveness, label='CBOW', linewidth=2.5, color='#2E86AB', marker='o', markersize=4)\n",
    "axes[1, 1].plot(range(1, epochs), sg_effectiveness, label='Skip-gram', linewidth=2.5, color='#A23B72', marker='s', markersize=4)\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1, 1].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Loss Change / Time (loss/sec)', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_title('Learning Efficiency', fontsize=15, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=12)\n",
    "axes[1, 1].grid(True, alpha=0.3, linestyle='--')\n",
    "axes[1, 1].set_facecolor('#f8f9fa')\n",
    "\n",
    "# 4.6 Overall comparison summary\n",
    "comparison_metrics = {\n",
    "    'Conv. Speed\\n(epochs)': [cbow_conv, sg_conv],\n",
    "    'Final Loss\\nReduction': [(cbow_train_losses[0] - cbow_train_losses[-1]), \n",
    "                               (sg_train_losses[0] - sg_train_losses[-1])],\n",
    "    'Avg Epoch\\nTime (s)': [np.mean(cbow_times), np.mean(sg_times)],\n",
    "    'Training\\nStability': [1/np.mean(cbow_var) if len(cbow_var) > 0 else 0, \n",
    "                             1/np.mean(sg_var) if len(sg_var) > 0 else 0]\n",
    "}\n",
    "\n",
    "x = np.arange(len(comparison_metrics))\n",
    "width = 0.35\n",
    "for i, (metric_name, values) in enumerate(comparison_metrics.items()):\n",
    "    # Normalize for better visualization\n",
    "    normalized = [v / max(values) for v in values]\n",
    "    axes[1, 2].bar(i - width/2, normalized[0], width, color='#2E86AB', edgecolor='black', alpha=0.8)\n",
    "    axes[1, 2].bar(i + width/2, normalized[1], width, color='#A23B72', edgecolor='black', alpha=0.8)\n",
    "\n",
    "axes[1, 2].set_ylabel('Normalized Score', fontsize=13, fontweight='bold')\n",
    "axes[1, 2].set_title('Model Comparison (Normalized)', fontsize=15, fontweight='bold')\n",
    "axes[1, 2].set_xticks(x)\n",
    "axes[1, 2].set_xticklabels(comparison_metrics.keys(), fontsize=11, fontweight='bold')\n",
    "axes[1, 2].legend(['CBOW', 'Skip-gram'], fontsize=12)\n",
    "axes[1, 2].grid(True, alpha=0.3, axis='y', linestyle='--')\n",
    "axes[1, 2].set_ylim(0, 1.2)\n",
    "axes[1, 2].set_facecolor('#f8f9fa')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('word2vec_convergence_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nğŸ“Š Convergence Analysis Summary:\")\n",
    "print(f\"  CBOW converged in {cbow_conv} epochs\")\n",
    "print(f\"  Skip-gram converged in {sg_conv} epochs\")\n",
    "print(f\"  CBOW average training stability: {1/np.mean(cbow_var) if len(cbow_var) > 0 else 0:.4f}\")\n",
    "print(f\"  Skip-gram average training stability: {1/np.mean(sg_var) if len(sg_var) > 0 else 0:.4f}\")\n",
    "print(f\"\\nâš¡ Performance optimizations applied:\")\n",
    "print(f\"  â€¢ Reduced epochs from 30 to 20 (33% faster)\")\n",
    "print(f\"  â€¢ Test evaluation only every 5 epochs (saves ~60% eval time)\")\n",
    "print(f\"  â€¢ Batch size: 512 for optimal throughput\")\n",
    "print(f\"  â€¢ Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed Training Analysis and Comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Final losses\n",
    "print(\"Final Loss Values:\")\n",
    "print(f\"  CBOW:\")\n",
    "print(f\"    Train Loss: {cbow_train_losses[-1]:.4f}\")\n",
    "print(f\"    Valid Loss: {cbow_valid_losses[-1]:.4f}\")\n",
    "print(f\"    Test Loss:  {cbow_test_losses[-1]:.4f}\")\n",
    "print()\n",
    "print(f\"  Skip-gram:\")\n",
    "print(f\"    Train Loss: {sg_train_losses[-1]:.4f}\")\n",
    "print(f\"    Valid Loss: {sg_valid_losses[-1]:.4f}\")\n",
    "print(f\"    Test Loss:  {sg_test_losses[-1]:.4f}\")\n",
    "print()\n",
    "\n",
    "# Best epoch (lowest validation loss)\n",
    "cbow_best_epoch = np.argmin(cbow_valid_losses) + 1\n",
    "sg_best_epoch = np.argmin(sg_valid_losses) + 1\n",
    "\n",
    "print(\"Best Epochs (lowest validation loss):\")\n",
    "print(f\"  CBOW: Epoch {cbow_best_epoch} (Valid Loss: {min(cbow_valid_losses):.4f})\")\n",
    "print(f\"  Skip-gram: Epoch {sg_best_epoch} (Valid Loss: {min(sg_valid_losses):.4f})\")\n",
    "print()\n",
    "\n",
    "# Loss reduction\n",
    "cbow_train_reduction = ((cbow_train_losses[0] - cbow_train_losses[-1]) / cbow_train_losses[0]) * 100\n",
    "sg_train_reduction = ((sg_train_losses[0] - sg_train_losses[-1]) / sg_train_losses[0]) * 100\n",
    "\n",
    "print(\"Training Loss Reduction:\")\n",
    "print(f\"  CBOW: {cbow_train_reduction:.2f}% (from {cbow_train_losses[0]:.4f} to {cbow_train_losses[-1]:.4f})\")\n",
    "print(f\"  Skip-gram: {sg_train_reduction:.2f}% (from {sg_train_losses[0]:.4f} to {sg_train_losses[-1]:.4f})\")\n",
    "print()\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Plot 1: CBOW all splits\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.plot(cbow_train_losses, label='Train', linewidth=2, color='blue', alpha=0.7)\n",
    "ax1.plot(cbow_valid_losses, label='Valid', linewidth=2, color='orange', alpha=0.7)\n",
    "ax1.plot(cbow_test_losses, label='Test', linewidth=2, color='green', alpha=0.7)\n",
    "ax1.axvline(x=cbow_best_epoch-1, color='red', linestyle='--', alpha=0.5, label=f'Best Epoch ({cbow_best_epoch})')\n",
    "ax1.set_xlabel('Epoch', fontsize=11)\n",
    "ax1.set_ylabel('Loss', fontsize=11)\n",
    "ax1.set_title('CBOW: All Splits', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Skip-gram all splits\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.plot(sg_train_losses, label='Train', linewidth=2, color='blue', alpha=0.7)\n",
    "ax2.plot(sg_valid_losses, label='Valid', linewidth=2, color='orange', alpha=0.7)\n",
    "ax2.plot(sg_test_losses, label='Test', linewidth=2, color='green', alpha=0.7)\n",
    "ax2.axvline(x=sg_best_epoch-1, color='red', linestyle='--', alpha=0.5, label=f'Best Epoch ({sg_best_epoch})')\n",
    "ax2.set_xlabel('Epoch', fontsize=11)\n",
    "ax2.set_ylabel('Loss', fontsize=11)\n",
    "ax2.set_title('Skip-gram: All Splits', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Direct comparison - Train loss\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.plot(cbow_train_losses, label='CBOW', linewidth=2, color='#3498db')\n",
    "ax3.plot(sg_train_losses, label='Skip-gram', linewidth=2, color='#e74c3c')\n",
    "ax3.set_xlabel('Epoch', fontsize=11)\n",
    "ax3.set_ylabel('Loss', fontsize=11)\n",
    "ax3.set_title('Training Loss Comparison', fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Direct comparison - Valid loss\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.plot(cbow_valid_losses, label='CBOW', linewidth=2, color='#3498db')\n",
    "ax4.plot(sg_valid_losses, label='Skip-gram', linewidth=2, color='#e74c3c')\n",
    "ax4.set_xlabel('Epoch', fontsize=11)\n",
    "ax4.set_ylabel('Loss', fontsize=11)\n",
    "ax4.set_title('Validation Loss Comparison', fontsize=12, fontweight='bold')\n",
    "ax4.legend(fontsize=9)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Direct comparison - Test loss\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax5.plot(cbow_test_losses, label='CBOW', linewidth=2, color='#3498db')\n",
    "ax5.plot(sg_test_losses, label='Skip-gram', linewidth=2, color='#e74c3c')\n",
    "ax5.set_xlabel('Epoch', fontsize=11)\n",
    "ax5.set_ylabel('Loss', fontsize=11)\n",
    "ax5.set_title('Test Loss Comparison', fontsize=12, fontweight='bold')\n",
    "ax5.legend(fontsize=9)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Loss reduction over time\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "cbow_reduction = [(cbow_train_losses[0] - loss) / cbow_train_losses[0] * 100 for loss in cbow_train_losses]\n",
    "sg_reduction = [(sg_train_losses[0] - loss) / sg_train_losses[0] * 100 for loss in sg_train_losses]\n",
    "ax6.plot(cbow_reduction, label='CBOW', linewidth=2, color='#3498db')\n",
    "ax6.plot(sg_reduction, label='Skip-gram', linewidth=2, color='#e74c3c')\n",
    "ax6.set_xlabel('Epoch', fontsize=11)\n",
    "ax6.set_ylabel('Loss Reduction (%)', fontsize=11)\n",
    "ax6.set_title('Training Loss Reduction', fontsize=12, fontweight='bold')\n",
    "ax6.legend(fontsize=9)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 7: Final losses comparison (bar chart)\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "splits = ['Train', 'Valid', 'Test']\n",
    "cbow_finals = [cbow_train_losses[-1], cbow_valid_losses[-1], cbow_test_losses[-1]]\n",
    "sg_finals = [sg_train_losses[-1], sg_valid_losses[-1], sg_test_losses[-1]]\n",
    "x = np.arange(len(splits))\n",
    "width = 0.35\n",
    "bars1 = ax7.bar(x - width/2, cbow_finals, width, label='CBOW', color='#3498db', edgecolor='black')\n",
    "bars2 = ax7.bar(x + width/2, sg_finals, width, label='Skip-gram', color='#e74c3c', edgecolor='black')\n",
    "ax7.set_ylabel('Final Loss', fontsize=11)\n",
    "ax7.set_title('Final Loss Comparison', fontsize=12, fontweight='bold')\n",
    "ax7.set_xticks(x)\n",
    "ax7.set_xticklabels(splits)\n",
    "ax7.legend(fontsize=9)\n",
    "ax7.grid(True, alpha=0.3, axis='y')\n",
    "# Add value labels\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax7.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                 f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Plot 8: Overfitting analysis (Train-Valid gap)\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "cbow_gap = [t - v for t, v in zip(cbow_train_losses, cbow_valid_losses)]\n",
    "sg_gap = [t - v for t, v in zip(sg_train_losses, sg_valid_losses)]\n",
    "ax8.plot(cbow_gap, label='CBOW', linewidth=2, color='#3498db')\n",
    "ax8.plot(sg_gap, label='Skip-gram', linewidth=2, color='#e74c3c')\n",
    "ax8.axhline(y=0, color='black', linestyle='-', linewidth=1, alpha=0.3)\n",
    "ax8.set_xlabel('Epoch', fontsize=11)\n",
    "ax8.set_ylabel('Train Loss - Valid Loss', fontsize=11)\n",
    "ax8.set_title('Generalization Gap (Overfitting)', fontsize=12, fontweight='bold')\n",
    "ax8.legend(fontsize=9)\n",
    "ax8.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 9: Learning rate effectiveness (loss change per epoch)\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "cbow_changes = [abs(cbow_train_losses[i] - cbow_train_losses[i-1]) for i in range(1, len(cbow_train_losses))]\n",
    "sg_changes = [abs(sg_train_losses[i] - sg_train_losses[i-1]) for i in range(1, len(sg_train_losses))]\n",
    "ax9.plot(cbow_changes, label='CBOW', linewidth=2, color='#3498db', alpha=0.7)\n",
    "ax9.plot(sg_changes, label='Skip-gram', linewidth=2, color='#e74c3c', alpha=0.7)\n",
    "ax9.set_xlabel('Epoch', fontsize=11)\n",
    "ax9.set_ylabel('Loss Change', fontsize=11)\n",
    "ax9.set_title('Loss Change per Epoch', fontsize=12, fontweight='bold')\n",
    "ax9.legend(fontsize=9)\n",
    "ax9.grid(True, alpha=0.3)\n",
    "ax9.set_yscale('log')\n",
    "\n",
    "plt.savefig('word2vec_detailed_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Detailed training analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding: 15px; background-color: #f5f5f5; border-radius: 12px; border: 2px solid #022216; font-family: Vazir; line-height: 1.8; box-shadow: 0 2px 4px rgba(0,0,0,0.1);\">\n",
    "<h3 style=\"color: #022216; margin-top: 0;\">Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ ØªØ­Ù„ÛŒÙ„ Ø´Ø¨Ø§Ù‡Øª ÙˆØ§Ú˜Ú¯Ø§Ù†</h3>\n",
    "\n",
    "<ol style=\"padding-right: 20px;\">\n",
    "    <li><strong>Ø§Ù†ØªØ®Ø§Ø¨ ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§:</strong> Ûµ ÙˆØ§Ú˜Ù‡ Ø¨Ù‡ Ø¯Ù„Ø®ÙˆØ§Ù‡ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ù…Ø§ÛŒÛŒØ¯.</li>\n",
    "    <li><strong>ÛŒØ§ÙØªÙ† ÙˆØ§Ú˜Ú¯Ø§Ù† Ù…Ø´Ø§Ø¨Ù‡:</strong> Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆØ§Ú˜Ù‡ Ùˆ Ù‡Ø± Ù…Ø¯Ù„ØŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¹ÛŒØ§Ø± <em>Cosine Similarity</em>ØŒ Ûµ ÙˆØ§Ú˜Ù‡ Ø¨Ø±ØªØ± Ù…Ø´Ø§Ø¨Ù‡ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯.</li>\n",
    "    <li><strong>Ù†Ù…Ø§ÛŒØ´ Ø¨ØµØ±ÛŒ:</strong> ÙˆØ§Ú˜Ú¯Ø§Ù† Ù…Ø´Ø§Ø¨Ù‡ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±ÙˆØ´ <em>t-SNE</em> Ø¨Ù‡ ØµÙˆØ±Øª Ù†Ù…ÙˆØ¯Ø§Ø± Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯ (Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆØ§Ú˜Ù‡ Ùˆ Ù‡Ø± Ù…Ø¯Ù„ ÛŒÚ© Ù†Ù…ÙˆØ¯Ø§Ø± Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡).</li>\n",
    "    <li><strong>ØªØ­Ù„ÛŒÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ:</strong> Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¯Ùˆ Ù…Ø¯Ù„ Ù…Ø®ØªÙ„Ù Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯.</li>\n",
    "</ol>\n",
    "\n",
    "<p style=\"color: #4b5563; font-size: 0.9em; margin-bottom: 0;\">\n",
    "    Ù†Ú©ØªÙ‡: Ø¯Ø± Ù‡Ø± Ù†Ù…ÙˆØ¯Ø§Ø± t-SNE Ù…ÛŒâ€ŒØ¨Ø§ÛŒØ³Øª ÙˆØ§Ú˜Ù‡ Ø§ØµÙ„ÛŒ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ûµ ÙˆØ§Ú˜Ù‡ Ù…Ø´Ø§Ø¨Ù‡ Ø¢Ù† Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def find_similar_words(word, model, vocab, top_k=5):\n",
    "    if word not in vocab.word2idx:\n",
    "        print(f\"Word '{word}' not found in vocabulary\")\n",
    "        return []\n",
    "    \n",
    "    word_idx = vocab.encode(word)\n",
    "    word_embedding = model.embeddings.weight[word_idx].detach().cpu().numpy().reshape(1, -1)\n",
    "    \n",
    "    all_embeddings = model.embeddings.weight.detach().cpu().numpy()\n",
    "    similarities = cosine_similarity(word_embedding, all_embeddings)[0]\n",
    "    \n",
    "    top_indices = similarities.argsort()[-top_k-1:-1][::-1]\n",
    "    \n",
    "    similar_words = []\n",
    "    for idx in top_indices:\n",
    "        if idx != word_idx:\n",
    "            similar_words.append((vocab.decode(idx), similarities[idx]))\n",
    "    \n",
    "    return similar_words[:top_k]\n",
    "\n",
    "\n",
    "def plot_tsne(word, model, vocab, top_k=5, title=\"\"):\n",
    "    if word not in vocab.word2idx:\n",
    "        print(f\"Word '{word}' not found in vocabulary\")\n",
    "        return\n",
    "    \n",
    "    similar_words = find_similar_words(word, model, vocab, top_k)\n",
    "    \n",
    "    word_idx = vocab.encode(word)\n",
    "    indices = [word_idx] + [vocab.encode(w[0]) for w in similar_words]\n",
    "    embeddings = model.embeddings.weight[indices].detach().cpu().numpy()\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(5, len(indices)-1))\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    plt.scatter(embeddings_2d[0, 0], embeddings_2d[0, 1], \n",
    "                c='red', s=300, marker='*', edgecolors='black', linewidth=2,\n",
    "                label='Target Word', zorder=3)\n",
    "    plt.annotate(word, (embeddings_2d[0, 0], embeddings_2d[0, 1]),\n",
    "                 fontsize=14, fontweight='bold', ha='center',\n",
    "                 xytext=(0, 10), textcoords='offset points')\n",
    "    \n",
    "    for i in range(1, len(embeddings_2d)):\n",
    "        plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1],\n",
    "                    c='blue', s=200, alpha=0.6, edgecolors='black', linewidth=1)\n",
    "        plt.annotate(vocab.decode(indices[i]), \n",
    "                     (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                     fontsize=11, ha='center',\n",
    "                     xytext=(0, 8), textcoords='offset points')\n",
    "    \n",
    "    plt.title(title, fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.xlabel('t-SNE Dimension 1', fontsize=12)\n",
    "    plt.ylabel('t-SNE Dimension 2', fontsize=12)\n",
    "    plt.legend(fontsize=11)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "test_words = ['king', 'computer', 'good', 'time', 'world']\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Word Similarity Analysis\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "for word in test_words:\n",
    "    if word not in vocab.word2idx:\n",
    "        print(f\"âš ï¸ Word '{word}' not found in vocabulary. Selecting another word...\")\n",
    "        import random\n",
    "        word = random.choice(list(vocab.word2idx.keys())[1:100])\n",
    "    \n",
    "    print(f\"\\nğŸ“Œ Word: {word}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(\"\\nğŸ”¹ CBOW model - similar words:\")\n",
    "    cbow_similar = find_similar_words(word, cbow_model, vocab, top_k=5)\n",
    "    for i, (sim_word, score) in enumerate(cbow_similar, 1):\n",
    "        print(f\"  {i}. {sim_word:15s} (similarity: {score:.4f})\")\n",
    "    \n",
    "    print(\"\\nğŸ”¸ Skip-gram model - similar words:\")\n",
    "    sg_similar = find_similar_words(word, skipgram_model, vocab, top_k=5)\n",
    "    for i, (sim_word, score) in enumerate(sg_similar, 1):\n",
    "        print(f\"  {i}. {sim_word:15s} (similarity: {score:.4f})\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_tsne(word, cbow_model, vocab, top_k=5, \n",
    "              title=f'CBOW: Similar Words to \"{word}\"')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plot_tsne(word, skipgram_model, vocab, top_k=5,\n",
    "              title=f'Skip-gram: Similar Words to \"{word}\"')\n",
    "    \n",
    "    plt.savefig(f'tsne_{word}_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ… Word similarity analysis completed!\")\n",
    "print(\"ğŸ“Š Plots saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ:</b><br>\n",
    "\n",
    "<b>ØªØ­Ù„ÛŒÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ CBOW Ùˆ Skip-gram:</b>\n",
    "\n",
    "<b>Û±. ØªÙØ§ÙˆØªâ€ŒÙ‡Ø§ÛŒ Ø§Ø³Ø§Ø³ÛŒ:</b>\n",
    "- <b>CBOW</b>: Ø§Ø² Ú©Ù„Ù…Ø§Øª Ø§Ø·Ø±Ø§Ù (context) Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ù‡ Ù…Ø±Ú©Ø²ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø³Ø±ÛŒØ¹â€ŒØªØ± Ø¢Ù…ÙˆØ²Ø´ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ø¯ Ùˆ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³Øª.\n",
    "- <b>Skip-gram</b>: Ø§Ø² Ú©Ù„Ù…Ù‡ Ù…Ø±Ú©Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ù„Ù…Ø§Øª Ø§Ø·Ø±Ø§Ù Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø¯Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ø¯Ø§Ø±Ø¯.\n",
    "\n",
    "<b>Û². Ù…Ø´Ø§Ù‡Ø¯Ø§Øª Ø§Ø² Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ t-SNE:</b>\n",
    "- Ø¯Ø± Ù…Ø¯Ù„ Skip-gram Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ú©Ù„Ù…Ø§Øª Ù…Ø´Ø§Ø¨Ù‡â€ŒØªØ± Ø§Ø² Ù†Ø¸Ø± Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨Ù‡ Ù‡Ù… Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ± Ù‡Ø³ØªÙ†Ø¯\n",
    "- Ù…Ø¯Ù„ CBOW ØªÙ…Ø§ÛŒÙ„ Ø¯Ø§Ø±Ø¯ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ context Ù…Ø´Ø§Ø¨Ù‡ Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù†Ø¯\n",
    "- Skip-gram Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¸Ø±ÛŒÙâ€ŒØªØ± Ù‚ÙˆÛŒâ€ŒØªØ± Ø§Ø³Øª\n",
    "\n",
    "<b>Û³. Ú©ÛŒÙÛŒØª Embeddings:</b>\n",
    "- Skip-gram Ø¨Ø±Ø§ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ú©ÙˆÚ†Ú©â€ŒØªØ± Ùˆ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø¯Ø± Ù…Ù†Ø§Ø³Ø¨â€ŒØªØ± Ø§Ø³Øª\n",
    "- CBOW Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø³Ø±ÛŒØ¹ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ø²Ø±Ú¯ Ø¨Ù‡ØªØ± Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯\n",
    "- Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ ØªÙˆØ§Ù†Ø§ÛŒÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¨ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯\n",
    "\n",
    "<b>Û´. Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ:</b>\n",
    "Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ùˆ Ø´Ø¨Ø§Ù‡Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ØŒ Ù‡Ø± Ø¯Ùˆ Ù…Ø¯Ù„ embeddingâ€ŒÙ‡Ø§ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø± ØªÙˆÙ„ÛŒØ¯ Ú©Ø±Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ø§Ù…Ø§ Skip-gram Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ±ÛŒ Ø¯Ø§Ø±Ø¯.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # <div style=\"text-align: center; direction: rtl; font-family: Vazir;\"><h1 align=\"center\" style=\"font-size: 24px; padding: 20px;\">âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸<br>Ø³ÙˆØ§Ù„ Ø¯ÙˆÙ…: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ Ú©Ù…Ú© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ùˆ Ù…Ø¯Ù„ Fasttext<br>âšœï¸â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”âšœï¸</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir=\"rtl\" style=\"text-align: right; padding:30px; background-color:rgb(12, 12, 12); border-radius: 12px; color: white; font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø´Ù…Ø§ Ø¨Ø§ Ú©Ù…Ú© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ ØªÙ…Ø§Ù… Ù…ØªØµÙ„ \n",
    "(Fully Connected)\n",
    "ÛŒÚ© Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ Ù…ØªÙ† Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ§Ù‡ÛŒØ¯\n",
    "Ú©Ø±Ø¯.\n",
    "Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§Ø² Ù…Ø¯Ù„\n",
    "<a href=\"https://fasttext.cc/\">Fasttext</a>\n",
    "Ø¨Ø±Ø§ÛŒ Embed\n",
    "Ú©Ø±Ø¯Ù† Ù…ØªÙ†â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯.\n",
    "<p dir=\"rtl\" style=\"padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "</p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ùˆ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "    <ul>\n",
    "        <li>Ø§Ø¨ØªØ¯Ø§ Ø¯ÛŒØªØ§Ø³Øª Ø²ÛŒØ± Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.\n",
    "    <br>\n",
    "    <a href=\"https://huggingface.co/datasets/SetFit/ag_news\">link</a></li>\n",
    "        <li>\n",
    "            Ù¾Ø³ Ø§Ø² Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ØŒ 5000\n",
    "            Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ùˆ 2000 Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ ØªØ³Øª Ø±Ø§ Ø¨Ù‡ ØªØµØ§Ø¯Ù Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.\n",
    "        </li>\n",
    "        <li>\n",
    "            Ù…Ø¬Ù…ÙˆØ¹Ù‡ 2000 Ù†Ù…ÙˆÙ†Ù‡â€ŒØ§ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¯Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ù‡ 1000 ØªØ§ÛŒÛŒ ØªØ³Øª Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ ØªÙ‚Ø³ÛŒÙ… Ú©Ù†ÛŒØ¯. Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§Ø² Ù…Ø¬Ù…ÙˆØ¹Ù‡ 5000 ØªØ§ÛŒÛŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ø¢Ù…ÙˆØ²Ø´ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "        </li>\n",
    "        <li>\n",
    "            Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù… ØªØ´Ú©ÛŒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø­ØªÙ…Ø§ ØªÙˆØ¬Ù‡ Ø¯Ø§Ø´ØªÙ‡â€ŒØ¨Ø§Ø´ÛŒØ¯ Ú©Ù‡ ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± Ù‡Ø± Ú©Ù„Ø§Ø³ balanced Ø¨Ø§Ø´Ø¯.\n",
    "        </li>\n",
    "        <li>\n",
    "        Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙˆÙ† ØªÙ†Ù‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² lowercasing Ùˆ \n",
    "            Ø­Ø°Ù white space Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÙ‡ \n",
    "            Ú©Ø§ÙÛŒ Ø§Ø³Øª.\n",
    "        </li>\n",
    "    </ul>\n",
    "    <b>Ø³ÙˆØ§Ù„:</b> Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ fasttext\n",
    "    Ø¨Ø±Ø§ÛŒ embed Ú©Ø±Ø¯Ù† \n",
    "        Ù…ØªÙˆÙ† Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø²ÛŒØ§Ø¯ÛŒ Ù†Ø¯Ø§Ø±ÛŒÙ…. Ú†Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒ Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ù…Ø§ Ø§Ø² Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨ÛŒØ´ØªØ± Ø¨ÛŒâ€ŒÙ†ÛŒØ§Ø² Ø´ÙˆÛŒÙ…ØŸ\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ:</b><br>\n",
    "\n",
    "<b>Ø¯Ù„ÛŒÙ„ Ù†ÛŒØ§Ø² Ú©Ù…ØªØ± Ø¨Ù‡ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø± FastText:</b>\n",
    "\n",
    "Ù…Ø¯Ù„ FastText Ø¨Ø±Ø®Ù„Ø§Ù Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³ÛŒÚ© Word2Vec Ú©Ù‡ Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© ÙˆØ§Ø­Ø¯ atomic Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†Ø¯ØŒ Ø§Ø² <b>subword information</b> Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ Ø±Ø§ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ø¯Ø§Ø±Ø¯:\n",
    "\n",
    "<b>Û±. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Character n-grams:</b>\n",
    "FastText Ù‡Ø± Ú©Ù„Ù…Ù‡ Ø±Ø§ Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² n-gram Ù‡Ø§ÛŒ Ú©Ø§Ø±Ø§Ú©ØªØ±ÛŒ ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø¨Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„ Ú©Ù„Ù…Ù‡ \"running\" Ø¨Ù‡: \"&lt;ru\", \"run\", \"unn\", \"nni\", \"nin\", \"ing\", \"ng&gt;\" ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
    "\n",
    "<b>Û². Ù…Ø²Ø§ÛŒØ§ÛŒ Ø§ÛŒÙ† Ø±ÙˆÛŒÚ©Ø±Ø¯:</b>\n",
    "- <b>Ú©Ù„Ù…Ø§Øª Ø®Ø§Ø±Ø¬ Ø§Ø² ÙˆØ§Ú˜Ú¯Ø§Ù† (OOV)</b>: Ø­ØªÛŒ Ø§Ú¯Ø± Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ Ø¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø² n-gram Ù‡Ø§ÛŒ Ø¢Ù† Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª embedding Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯\n",
    "- <b>Ø§Ù…Ù„Ø§ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡</b>: Ú©Ù„Ù…Ø§Øª Ø¨Ø§ Ø§Ù…Ù„Ø§ÛŒ Ù†Ø²Ø¯ÛŒÚ© embedding Ù‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø¯Ø§Ø´Øª\n",
    "- <b>Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø¯Ø±</b>: Ø§Ø² Ø·Ø±ÛŒÙ‚ subword Ù‡Ø§ÛŒ Ù…Ø´ØªØ±Ú©ØŒ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø¯Ø± Ù†ÛŒØ² embedding Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯\n",
    "- <b>Ú©Ù„Ù…Ø§Øª Ù…Ø±Ú©Ø¨</b>: Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø®Ù„ÛŒ Ú©Ù„Ù…Ø§Øª Ù…Ø±Ú©Ø¨ Ø­ÙØ¸ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
    "\n",
    "<b>Û³. Ù†ØªÛŒØ¬Ù‡:</b>\n",
    "Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø­Ø°Ù stopwordsØŒ stemmingØŒ ÛŒØ§ lemmatization Ù†Ø¯Ø§Ø±ÛŒÙ… Ú†ÙˆÙ† Ù…Ø¯Ù„ Ø®ÙˆØ¯Ø´ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø§Ø² Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø®Ù„ÛŒ Ú©Ù„Ù…Ø§Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ø¯ Ùˆ Ø±ÙˆØ§Ø¨Ø· Ù…Ø¹Ù†Ø§ÛŒÛŒ Ø±Ø§ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ø¯.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "print(\"ğŸ“¥ Loading AG News dataset...\")\n",
    "dataset = load_dataset(\"SetFit/ag_news\")\n",
    "\n",
    "print(\"âœ… Dataset loaded!\")\n",
    "print(f\"Number of Train samples: {len(dataset['train'])}\")\n",
    "print(f\"Number of Test samples: {len(dataset['test'])}\")\n",
    "print()\n",
    "\n",
    "print(\"Sample data:\")\n",
    "print(f\"Text: {dataset['train'][0]['text'][:100]}...\")\n",
    "print(f\"Label: {dataset['train'][0]['label']}\")\n",
    "print()\n",
    "\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "test_df = pd.DataFrame(dataset['test'])\n",
    "\n",
    "print(\"Available classes:\")\n",
    "print(train_df['label'].value_counts().sort_index())\n",
    "print()\n",
    "\n",
    "def simple_preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "print(\"ğŸ“Š Selecting balanced samples...\")\n",
    "\n",
    "samples_per_class_train = 5000 // 4\n",
    "samples_per_class_test = 2000 // 4\n",
    "\n",
    "train_balanced = []\n",
    "for label in range(4):\n",
    "    class_samples = train_df[train_df['label'] == label].sample(n=samples_per_class_train, random_state=42)\n",
    "    train_balanced.append(class_samples)\n",
    "\n",
    "train_data = pd.concat(train_balanced, ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "test_balanced = []\n",
    "for label in range(4):\n",
    "    class_samples = test_df[test_df['label'] == label].sample(n=samples_per_class_test, random_state=42)\n",
    "    test_balanced.append(class_samples)\n",
    "\n",
    "test_data = pd.concat(test_balanced, ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "test_data, valid_data = train_test_split(test_data, test_size=0.5, random_state=42, stratify=test_data['label'])\n",
    "\n",
    "print(f\"âœ… Number of Train samples: {len(train_data)}\")\n",
    "print(f\"âœ… Number of Validation samples: {len(valid_data)}\")\n",
    "print(f\"âœ… Number of Test samples: {len(test_data)}\")\n",
    "print()\n",
    "\n",
    "print(\"Class distribution in Train:\")\n",
    "print(train_data['label'].value_counts().sort_index())\n",
    "print()\n",
    "\n",
    "print(\"Class distribution in Validation:\")\n",
    "print(valid_data['label'].value_counts().sort_index())\n",
    "print()\n",
    "\n",
    "print(\"Class distribution in Test:\")\n",
    "print(test_data['label'].value_counts().sort_index())\n",
    "print()\n",
    "\n",
    "print(\"ğŸ”„ Preprocessing texts...\")\n",
    "train_data['text'] = train_data['text'].apply(simple_preprocess)\n",
    "valid_data['text'] = valid_data['text'].apply(simple_preprocess)\n",
    "test_data['text'] = test_data['text'].apply(simple_preprocess)\n",
    "\n",
    "print(\"âœ… Preprocessing completed!\")\n",
    "print()\n",
    "\n",
    "print(\"Sample preprocessed text:\")\n",
    "print(f\"Text: {train_data.iloc[0]['text'][:150]}...\")\n",
    "print(f\"Label: {train_data.iloc[0]['label']}\")\n",
    "\n",
    "class_names = {\n",
    "    0: \"World\",\n",
    "    1: \"Sports\", \n",
    "    2: \"Business\",\n",
    "    3: \"Sci/Tech\"\n",
    "}\n",
    "print()\n",
    "print(\"Class names:\")\n",
    "for label, name in class_names.items():\n",
    "    print(f\"  {label}: {name}\")}```}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ TrainØŒ Test Ùˆ Validation\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Embedding Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ù…ØªÙˆÙ† Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù†ÛŒÙ….\n",
    "    <ol>\n",
    "        <li>\n",
    "            Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ù…Ø¯Ù„ pre-train Ø´Ø¯Ù‡ \n",
    "            <a href=\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.bin.zip\">wiki-news-300d-1M-subword</a>\n",
    "            Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯ Ùˆ Ø¨Ø§ Ú©Ù…Ú© Ú©ØªØ§Ø¨Ø®ÙˆØ§Ù†Ù‡ fasttext \n",
    "            Ø¢Ù†Ø±Ø§ load Ú©Ù†ÛŒØ¯.\n",
    "        </li>\n",
    "        <li>\n",
    "            Ø¯Ø± Ø§Ø³Ù„Ø§ÛŒØ¯ Ø´Ø´Ù… Ø¯Ø±Ø³ Ø¨Ø§ Ù…ÙÙ‡ÙˆÙ… sentence embedding Ø¢Ø´Ù†Ø§ Ø´Ø¯Ù‡â€ŒØ§ÛŒØ¯.\n",
    "            Ø¨Ø§ Ú©Ù…Ú© Ù…Ø¯Ù„ fasttext \n",
    "            ØªÙ…Ø§Ù…ÛŒ Ù…ØªÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ trainØŒ test Ùˆ validation\n",
    "            Ø±Ø§ embed Ú©Ù†ÛŒØ¯.\n",
    "        </li>\n",
    "    </ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "import numpy as np\n",
    "\n",
    "# fasttext.util.download_model('en', if_exists='ignore')\n",
    "print(t('loading_fasttext'))\n",
    "ft = fasttext.load_model('wiki-news-300d-1M-subword.bin')\n",
    "print(t('fasttext_loaded'))\n",
    "\n",
    "def sentence_embedding(sentence, model):\n",
    "    words = sentence.split()\n",
    "    embeddings = [model.get_word_vector(word) for word in words]\n",
    "    if not embeddings:\n",
    "        return np.zeros(model.get_dimension())\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "# Extract texts and labels from the dataframes\n",
    "train_texts = train_data['text'].tolist()\n",
    "train_labels = train_data['label'].tolist()\n",
    "\n",
    "valid_texts = valid_data['text'].tolist()\n",
    "valid_labels = valid_data['label'].tolist()\n",
    "\n",
    "test_texts = test_data['text'].tolist()\n",
    "test_labels = test_data['label'].tolist()\n",
    "\n",
    "# Create embeddings\n",
    "train_embeddings = [sentence_embedding(text, ft) for text in train_texts]\n",
    "valid_embeddings = [sentence_embedding(text, ft) for text in valid_texts]\n",
    "test_embeddings = [sentence_embedding(text, ft) for text in test_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Embedding\n",
    "Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Train, Test, Validation\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">Ù¾ÛŒØ§Ø¯Ù‡ Ø³Ø§Ø²ÛŒ Ø´Ø¨Ú©Ù‡ Ùˆâ€Œ Ø§Ù…ÙˆØ²Ø´ Ø´Ø¨Ú©Ù‡</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ø­Ø§Ù„ Ú©Ù‡ Ù…ØªÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ ÙØ¶Ø§ÛŒ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ù†Ú¯Ø§Ø´Øª Ú©Ø±Ø¯Ù‡â€ŒØ§ÛŒÙ…ØŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø§ Ú©Ù…Ú© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù‡Ø³ØªÙ†Ø¯.\n",
    "<ol>\n",
    "    <li>Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡ Ù‡Ø§ Ø±Ø§ Ø¨Ø§ Ú©Ù…Ú© Dataloader pytorch\n",
    "    ÛŒØ§ Ù‡Ø± framework Ø¯ÛŒÚ¯Ø±ÛŒ Ú©Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¨Ø¯ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n",
    "    </li>\n",
    "    <li>\n",
    "    ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Fully Connected Ø¨Ø§ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø¯Ù„Ø®ÙˆØ§Ù‡ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø·Ø±Ø§Ø­ÛŒ Ú©Ù†ÛŒØ¯.\n",
    "    </li>\n",
    "    <li>\n",
    "    Ø´Ø¨Ú©Ù‡ Ø±Ø§ Ø­Ø¯Ø§Ù‚Ù„ Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ 30 Ø§ÛŒÙ¾Ø§Ú© Ø¢Ù…ÙˆØ²Ø´ Ø¯Ù‡ÛŒØ¯.\n",
    "    </li>\n",
    "</ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_embeddings_tensor = torch.FloatTensor(train_embeddings)\n",
    "train_labels_tensor = torch.LongTensor(train_labels)\n",
    "\n",
    "valid_embeddings_tensor = torch.FloatTensor(valid_embeddings)\n",
    "valid_labels_tensor = torch.LongTensor(valid_labels)\n",
    "\n",
    "test_embeddings_tensor = torch.FloatTensor(test_embeddings)\n",
    "test_labels_tensor = torch.LongTensor(test_labels)\n",
    "\n",
    "train_dataset = TensorDataset(train_embeddings_tensor, train_labels_tensor)\n",
    "valid_dataset = TensorDataset(valid_embeddings_tensor, valid_labels_tensor)\n",
    "test_dataset = TensorDataset(test_embeddings_tensor, test_labels_tensor)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(t('dataloaders_ready'))\n",
    "print(t('train_batches', len(train_loader)))\n",
    "print(t('valid_batches', len(valid_loader)))\n",
    "print(t('test_batches', len(test_loader)))\n",
    "print()\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.3):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "input_dim = 300 \n",
    "hidden_dims = [256, 128, 64]\n",
    "num_classes = 4\n",
    "dropout = 0.3\n",
    "\n",
    "model = MLPClassifier(input_dim, hidden_dims, num_classes, dropout)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(t('model_arch'))\n",
    "print(model)\n",
    "print()\n",
    "print(t('using_device', device))\n",
    "print()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "\n",
    "def calculate_accuracy(loader, model, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total if total > 0 else 0.0\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    epoch_train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    train_accuracy = calculate_accuracy(train_loader, model, device)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    model.eval()\n",
    "    running_valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_valid_loss += loss.item()\n",
    "            \n",
    "    epoch_valid_loss = running_valid_loss / len(valid_loader)\n",
    "    valid_losses.append(epoch_valid_loss)\n",
    "    \n",
    "    valid_accuracy = calculate_accuracy(valid_loader, model, device)\n",
    "    valid_accuracies.append(valid_accuracy)\n",
    "    \n",
    "    print(t('epoch_stat', epoch+1, num_epochs, epoch_train_loss, train_accuracy, epoch_valid_loss, valid_accuracy))\n",
    "\n",
    "print()\n",
    "print(t('training_complete'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "<ul>\n",
    "    <li>\n",
    "    Ù…Ø¯Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡\n",
    "    </li>\n",
    "    <li>\n",
    "    Ù†Ù…ÙˆØ¯Ø§Ø± ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ù‚Øª Ùˆ loss\n",
    "        Ø¯Ø± Ù‡Ù†Ú¯Ø§Ù… Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø±Ø±ÙˆÛŒ Ø¯Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ¯Ø§Ø¯Ù‡\n",
    "        Train Ùˆ Validation\n",
    "    </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center; direction: rtl; font-family: Vazir;\">ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"text-align: right; padding:10px; background-color:#6B7280;  border-radius: 12px; border: 2px solid rgb(2, 34, 22); font-family: Vazir;\">\n",
    "Ù¾Ø³ Ø§Ø² Ø§ØªÙ…Ø§Ù… Ø¢Ù…ÙˆØ²Ø´ØŒ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ test Ø¨Ø¯Ø³Øª Ø¢ÙˆØ±ÛŒØ¯:\n",
    "    <ul>\n",
    "        <li>\n",
    "            Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ\n",
    "            (Confusion Matrix)\n",
    "        </li>\n",
    "        <li>\n",
    "            Ø¯Ù‚Øª\n",
    "        </li>\n",
    "        <li>\n",
    "            Macro-F1\n",
    "        </li>\n",
    "    </ul>\n",
    "    Ø¨Ø§ ØªÙˆØ¬Ù‡ Ø¨Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ØŒ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ø²ÛŒØ± Ø¬ÙˆØ§Ø¨ Ø¯Ù‡ÛŒØ¯:\n",
    "    <ul>\n",
    "        <li>Ù…Ø¯Ù„ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ú©Ø¯Ø§Ù… Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¨Ù‡ØªØ±ÛŒÙ† Ùˆ Ø¨Ø¯ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø¯Ø§Ø´ØªÙ‡â€ŒØ§Ø³ØªØŸ</li>\n",
    "        <li>Ù…Ø¯Ù„ Ú©Ø¯Ø§Ù… Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø±Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø¯ÙØ¹Ù‡ Ø¨Ø§ ÛŒÚ©Ø¯ÛŒÚ¯Ø± Ø§Ø´ØªØ¨Ø§Ù‡ Ú¯Ø±ÙØªÙ‡â€ŒØ§Ø³ØªØŸ</li>\n",
    "        <li>Ø³Ù‡ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù…ØªÙˆÙ†ÛŒ Ú©Ù‡ Ù…Ø¯Ù„ Ø¨Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ú†Ø³Ø¨ Ø²Ø¯Ù‡â€ŒØ§Ø³Øª Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒØ¯ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒ Ø¢Ù†Ù‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.</li>\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style='background:#fffbe6; font-family: Vazir; border:1px dashed #f0ad4e; padding:12px; border-radius:8px; color:#111'>\n",
    "âœï¸ <b>Ù¾Ø§Ø³Ø® ØªØ´Ø±ÛŒØ­ÛŒ:</b><br>\n",
    "\n",
    "<b>ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø®Ø¨Ø§Ø±:</b>\n",
    "\n",
    "<b>Û±. Ø¨Ù‡ØªØ±ÛŒÙ† Ùˆ Ø¨Ø¯ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯:</b>\n",
    "Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…ØªØ±ÛŒÚ© F1-Score Ùˆ Confusion Matrix:\n",
    "- <b>Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯</b>: Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ú©Ù„Ø§Ø³ Sports Ø¯Ø§Ø±Ø§ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø§Ø³Øª Ú†ÙˆÙ† Ø§Ø®Ø¨Ø§Ø± ÙˆØ±Ø²Ø´ÛŒ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø§Ø®ØªØµØ§ØµÛŒ Ùˆ Ù…ØªÙ…Ø§ÛŒØ²ØªØ±ÛŒ Ø¯Ø§Ø±Ù†Ø¯ (Ù†Ø§Ù… Ø¨Ø§Ø²ÛŒÚ©Ù†Ø§Ù†ØŒ ØªÛŒÙ…â€ŒÙ‡Ø§ØŒ Ø§Ù…ØªÛŒØ§Ø²Ù‡Ø§)\n",
    "- <b>Ø¨Ø¯ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯</b>: Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ World Ùˆ Business Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙâ€ŒØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯ Ú†ÙˆÙ† Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø¢Ù†Ù‡Ø§ Ú¯Ø§Ù‡ÛŒ Ø¨Ø§ Ù‡Ù… overlap Ø¯Ø§Ø±Ù†Ø¯ (Ù…Ø«Ù„Ø§Ù‹ Ø§Ø®Ø¨Ø§Ø± Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ø¬Ù‡Ø§Ù†ÛŒ)\n",
    "\n",
    "<b>Û². Ø§Ø´ØªØ¨Ø§Ù‡Ø§Øª Ø±Ø§ÛŒØ¬:</b>\n",
    "Ø¨ÛŒØ´ØªØ±ÛŒÙ† confusion Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨ÛŒÙ† Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø±Ø® Ù…ÛŒâ€ŒØ¯Ù‡Ø¯:\n",
    "- <b>World Ùˆ Business</b>: Ø§Ø®Ø¨Ø§Ø± Ø¬Ù‡Ø§Ù†ÛŒ Ø§Ù‚ØªØµØ§Ø¯ÛŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¯Ø± Ù‡Ø± Ø¯Ùˆ Ú©Ù„Ø§Ø³ Ù‚Ø±Ø§Ø± Ú¯ÛŒØ±Ø¯\n",
    "- <b>World Ùˆ Sci/Tech</b>: Ø§Ø®Ø¨Ø§Ø± ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ Ø¬Ù‡Ø§Ù†ÛŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù…Ø¨Ù‡Ù… Ø¨Ø§Ø´Ù†Ø¯\n",
    "- <b>Business Ùˆ Sci/Tech</b>: Ø´Ø±Ú©Øªâ€ŒÙ‡Ø§ÛŒ ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ Ø¯Ø± Ù‡Ø± Ø¯Ùˆ Ø¯Ø³ØªÙ‡ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†Ø¯\n",
    "\n",
    "<b>Û³. ØªØ­Ù„ÛŒÙ„ Ù…ØªÙˆÙ† Ø§Ø´ØªØ¨Ø§Ù‡:</b>\n",
    "Ø¨Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø´ØªØ¨Ø§Ù‡ Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯:\n",
    "- Ù…ØªÙˆÙ† Ú©ÙˆØªØ§Ù‡ Ú©Ù‡ context Ú©Ø§ÙÛŒ Ù†Ø¯Ø§Ø±Ù†Ø¯\n",
    "- Ø§Ø®Ø¨Ø§Ø±ÛŒ Ú©Ù‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡ Ø¯Ø§Ø±Ù†Ø¯ (Ù…Ø«Ù„Ø§Ù‹ Ø´Ø±Ú©Øª ØªÚ©Ù†ÙˆÙ„ÙˆÚ˜ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ø§Ø± Ø¬Ù‡Ø§Ù†ÛŒ)\n",
    "- Ø§Ø®Ø¨Ø§Ø± Ø¨Ø§ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø±Ø²ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø¯Ø± Ú†Ù†Ø¯ Ú©Ù„Ø§Ø³ Ù‚Ø±Ø§Ø± Ú¯ÛŒØ±Ù†Ø¯\n",
    "- Ú©Ù…Ø¨ÙˆØ¯ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø´Ø®ØµÙ‡ Ø¯Ø± Ù…ØªÙ†\n",
    "\n",
    "<b>Û´. Ø±Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯:</b>\n",
    "- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ù…Ø§Ù†Ù†Ø¯ LSTM ÛŒØ§ Transformer\n",
    "- Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ\n",
    "- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ensemble methods\n",
    "- Fine-tuning Ø±ÙˆÛŒ embedding Ù‡Ø§ÛŒ pre-trained\n",
    "- Ø§ÙØ²ÙˆØ¯Ù† features Ø¨ÛŒØ´ØªØ± Ù…Ø§Ù†Ù†Ø¯ TF-IDF Ø¯Ø± Ú©Ù†Ø§Ø± FastText\n",
    "\n",
    "<b>Ûµ. Ù†ØªÛŒØ¬Ù‡â€ŒÚ¯ÛŒØ±ÛŒ:</b>\n",
    "Ù…Ø¯Ù„ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù‚Ø§Ø¨Ù„ Ù‚Ø¨ÙˆÙ„ÛŒ Ø¯Ø± Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø§Ø±Ø¯ Ø§Ù…Ø§ Ø¯Ø± Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ù…Ø±Ø²ÛŒ Ùˆ overlap Ø¯Ø§Ø± Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ù†ÛŒØ§Ø² Ø¯Ø§Ø±Ø¯. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² FastText Ø¨Ù‡ Ù…Ø¯Ù„ Ú©Ù…Ú© Ú©Ø±Ø¯Ù‡ Ú©Ù‡ Ø­ØªÛŒ Ø¨Ø§ Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ù†ÛŒØ² Ø¨Ø±Ø®ÙˆØ±Ø¯ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model(loader, model, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "test_preds, test_labels_list = evaluate_model(test_loader, model, device)\n",
    "\n",
    "test_accuracy = accuracy_score(test_labels_list, test_preds)\n",
    "test_f1 = f1_score(test_labels_list, test_preds, average='macro')\n",
    "\n",
    "print(t('test_accuracy', test_accuracy))\n",
    "print(t('test_f1', test_f1))\n",
    "\n",
    "cm = confusion_matrix(test_labels_list, test_preds)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(1, 5), yticklabels=range(1, 5))\n",
    "plt.title(t('confusion_matrix'))\n",
    "plt.xlabel(t('predicted_label'))\n",
    "plt.ylabel(t('true_label'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history - Loss curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "ax1.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
    "ax1.plot(epochs, valid_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(epochs, train_accuracies, 'b-', label='Train Accuracy', linewidth=2)\n",
    "ax2.plot(epochs, valid_accuracies, 'r-', label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12)\n",
    "ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Training curves plotted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class performance metrics\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PER-CLASS PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Get per-class metrics\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    test_labels_list, test_preds, average=None\n",
    ")\n",
    "\n",
    "class_names_list = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Class': class_names_list,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(metrics_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Plot per-class metrics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "x = range(len(class_names_list))\n",
    "\n",
    "axes[0].bar(x, precision, color='skyblue', edgecolor='black')\n",
    "axes[0].set_xlabel('Class', fontsize=12)\n",
    "axes[0].set_ylabel('Precision', fontsize=12)\n",
    "axes[0].set_title('Precision per Class', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(class_names_list, rotation=45, ha='right')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[1].bar(x, recall, color='lightcoral', edgecolor='black')\n",
    "axes[1].set_xlabel('Class', fontsize=12)\n",
    "axes[1].set_ylabel('Recall', fontsize=12)\n",
    "axes[1].set_title('Recall per Class', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(class_names_list, rotation=45, ha='right')\n",
    "axes[1].set_ylim(0, 1)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "axes[2].bar(x, f1, color='lightgreen', edgecolor='black')\n",
    "axes[2].set_xlabel('Class', fontsize=12)\n",
    "axes[2].set_ylabel('F1-Score', fontsize=12)\n",
    "axes[2].set_title('F1-Score per Class', fontsize=14, fontweight='bold')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(class_names_list, rotation=45, ha='right')\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify best and worst performing classes\n",
    "best_f1_idx = f1.argmax()\n",
    "worst_f1_idx = f1.argmin()\n",
    "\n",
    "print(f\"ğŸ† Best performing class: {class_names_list[best_f1_idx]} (F1: {f1[best_f1_idx]:.4f})\")\n",
    "print(f\"âš ï¸  Worst performing class: {class_names_list[worst_f1_idx]} (F1: {f1[worst_f1_idx]:.4f})\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix Analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"CONFUSION MATRIX ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Find most confused class pairs\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print(\"Most confused class pairs (excluding diagonal):\")\n",
    "confused_pairs = []\n",
    "for i in range(len(class_names_list)):\n",
    "    for j in range(len(class_names_list)):\n",
    "        if i != j:\n",
    "            confused_pairs.append((i, j, cm[i, j], cm_normalized[i, j]))\n",
    "\n",
    "confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "for i, (true_idx, pred_idx, count, norm_val) in enumerate(confused_pairs[:5], 1):\n",
    "    print(f\"{i}. {class_names_list[true_idx]} â†’ {class_names_list[pred_idx]}: \"\n",
    "          f\"{count} samples ({norm_val*100:.1f}% of {class_names_list[true_idx]} class)\")\n",
    "print()\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Absolute confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=class_names_list, yticklabels=class_names_list,\n",
    "            ax=ax1, cbar_kws={'label': 'Count'})\n",
    "ax1.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax1.set_ylabel('True Label', fontsize=12)\n",
    "ax1.set_title('Confusion Matrix (Absolute)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Normalized confusion matrix\n",
    "sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "            xticklabels=class_names_list, yticklabels=class_names_list,\n",
    "            ax=ax2, cbar_kws={'label': 'Proportion'})\n",
    "ax2.set_xlabel('Predicted Label', fontsize=12)\n",
    "ax2.set_ylabel('True Label', fontsize=12)\n",
    "ax2.set_title('Confusion Matrix (Normalized)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze misclassified examples\n",
    "print(\"=\" * 80)\n",
    "print(\"MISCLASSIFIED EXAMPLES ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Find misclassified examples\n",
    "test_texts_list = test_data['text'].tolist()\n",
    "misclassified_indices = [i for i, (true, pred) in enumerate(zip(test_labels_list, test_preds)) if true != pred]\n",
    "\n",
    "print(f\"Total misclassified samples: {len(misclassified_indices)} out of {len(test_labels_list)}\")\n",
    "print(f\"Misclassification rate: {len(misclassified_indices)/len(test_labels_list)*100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# Show 5 example misclassifications\n",
    "print(\"Examples of misclassified texts:\")\n",
    "print(\"-\" * 80)\n",
    "for i, idx in enumerate(misclassified_indices[:5], 1):\n",
    "    true_label = test_labels_list[idx]\n",
    "    pred_label = test_preds[idx]\n",
    "    text = test_texts_list[idx]\n",
    "    \n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"True class: {class_names_list[true_label]}\")\n",
    "    print(f\"Predicted class: {class_names_list[pred_label]}\")\n",
    "    print(f\"Text: {text[:200]}...\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Misclassification distribution\n",
    "misclass_by_true = {}\n",
    "misclass_by_pred = {}\n",
    "\n",
    "for idx in misclassified_indices:\n",
    "    true_label = test_labels_list[idx]\n",
    "    pred_label = test_preds[idx]\n",
    "    \n",
    "    true_class = class_names_list[true_label]\n",
    "    pred_class = class_names_list[pred_label]\n",
    "    \n",
    "    misclass_by_true[true_class] = misclass_by_true.get(true_class, 0) + 1\n",
    "    misclass_by_pred[pred_class] = misclass_by_pred.get(pred_class, 0) + 1\n",
    "\n",
    "# Plot misclassification distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# By true class\n",
    "true_classes = list(misclass_by_true.keys())\n",
    "true_counts = list(misclass_by_true.values())\n",
    "ax1.bar(true_classes, true_counts, color='salmon', edgecolor='black')\n",
    "ax1.set_xlabel('True Class', fontsize=12)\n",
    "ax1.set_ylabel('Number of Misclassifications', fontsize=12)\n",
    "ax1.set_title('Misclassifications by True Class', fontsize=14, fontweight='bold')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# By predicted class\n",
    "pred_classes = list(misclass_by_pred.keys())\n",
    "pred_counts = list(misclass_by_pred.values())\n",
    "ax2.bar(pred_classes, pred_counts, color='lightblue', edgecolor='black')\n",
    "ax2.set_xlabel('Predicted Class', fontsize=12)\n",
    "ax2.set_ylabel('Number of Misclassifications', fontsize=12)\n",
    "ax2.set_title('Misclassifications by Predicted Class', fontsize=14, fontweight='bold')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model confidence analysis\n",
    "print(\"=\" * 80)\n",
    "print(\"MODEL CONFIDENCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Get prediction probabilities\n",
    "model.eval()\n",
    "all_probs = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "all_probs = np.array(all_probs)\n",
    "max_probs = np.max(all_probs, axis=1)\n",
    "pred_entropy = -np.sum(all_probs * np.log(all_probs + 1e-10), axis=1)\n",
    "\n",
    "# Separate correct and incorrect predictions\n",
    "correct_mask = np.array(test_labels_list) == np.array(test_preds)\n",
    "correct_probs = max_probs[correct_mask]\n",
    "incorrect_probs = max_probs[~correct_mask]\n",
    "\n",
    "print(f\"Average confidence for correct predictions: {correct_probs.mean():.4f}\")\n",
    "print(f\"Average confidence for incorrect predictions: {incorrect_probs.mean():.4f}\")\n",
    "print()\n",
    "\n",
    "# Plot confidence distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Confidence histogram\n",
    "axes[0, 0].hist(correct_probs, bins=30, alpha=0.7, color='green', label='Correct', edgecolor='black')\n",
    "axes[0, 0].hist(incorrect_probs, bins=30, alpha=0.7, color='red', label='Incorrect', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Prediction Confidence', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Distribution of Prediction Confidence', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=11)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confidence box plot\n",
    "axes[0, 1].boxplot([correct_probs, incorrect_probs], labels=['Correct', 'Incorrect'],\n",
    "                    patch_artist=True,\n",
    "                    boxprops=dict(facecolor='lightblue', edgecolor='black'),\n",
    "                    medianprops=dict(color='red', linewidth=2))\n",
    "axes[0, 1].set_ylabel('Prediction Confidence', fontsize=12)\n",
    "axes[0, 1].set_title('Confidence Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Entropy distribution\n",
    "axes[1, 0].hist(pred_entropy[correct_mask], bins=30, alpha=0.7, color='green', \n",
    "                label='Correct', edgecolor='black')\n",
    "axes[1, 0].hist(pred_entropy[~correct_mask], bins=30, alpha=0.7, color='red',\n",
    "                label='Incorrect', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Prediction Entropy', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1, 0].set_title('Distribution of Prediction Entropy', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=11)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Confidence vs accuracy scatter\n",
    "confidence_bins = np.linspace(0, 1, 11)\n",
    "bin_accuracies = []\n",
    "bin_centers = []\n",
    "for i in range(len(confidence_bins) - 1):\n",
    "    mask = (max_probs >= confidence_bins[i]) & (max_probs < confidence_bins[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        bin_acc = correct_mask[mask].mean()\n",
    "        bin_accuracies.append(bin_acc)\n",
    "        bin_centers.append((confidence_bins[i] + confidence_bins[i+1]) / 2)\n",
    "\n",
    "axes[1, 1].plot(bin_centers, bin_accuracies, 'o-', linewidth=2, markersize=8, color='blue')\n",
    "axes[1, 1].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Calibration')\n",
    "axes[1, 1].set_xlabel('Confidence', fontsize=12)\n",
    "axes[1, 1].set_ylabel('Accuracy', fontsize=12)\n",
    "axes[1, 1].set_title('Model Calibration', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].legend(fontsize=11)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].set_xlim(0, 1)\n",
    "axes[1, 1].set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… All visualizations completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir='rtl' style=\"line-height: 2.0; text-align: right; font-family: Vazir; font-size: 16px; margin-top: 20px; color: white; background-color:rgb(0, 40, 30); padding: 30px; border-radius: 8px;\">\n",
    "ğŸ¯ <b>Ø®Ø±ÙˆØ¬ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±:</b><br>\n",
    "<ul>\n",
    "    <li>\n",
    "    Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ú¯ÙØªÙ‡â€ŒØ´Ø¯Ù‡\n",
    "    </li>\n",
    "    <li>\n",
    "    ØªØ­Ù„ÛŒÙ„ Ù†ØªØ§ÛŒØ¬\n",
    "    </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_title"
   },
   "source": [
    "# <h1 style=\"text-align: right;\">**Ù†Ú©Ø§Øª Ù…Ù‡Ù… Ùˆ Ù‚ÙˆØ§Ù†ÛŒÙ† ØªØ­ÙˆÛŒÙ„**</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "policies_body"
   },
   "source": [
    "\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø§ ÙØ±Ù…Øª Ø²ÛŒØ± Ù†Ø§Ù…Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆØ¯: <code>NLP_CA{n}_{LASTNAME}_{STUDENTID}.ipynb</code></h4>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">Ù†Ø­ÙˆÙ‡ Ø§Ù†Ø¬Ø§Ù… ØªÙ…Ø±ÛŒÙ†:</h4>\n",
    "<ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "  <li>Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø¯ Ø¨Ø§ Ø¨Ø±Ú†Ø³Ø¨ <code>WRITE YOUR CODE HERE</code> Ø±Ø§ ØªÚ©Ù…ÛŒÙ„ Ú©Ù†ÛŒØ¯.</li>\n",
    "  <li>Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒØŒ Ù…ØªÙ† <code>{{Ù¾Ø§Ø³Ø®_Ø®ÙˆØ¯_Ø±Ø§_Ø§ÛŒÙ†Ø¬Ø§_Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯}}</code> Ø±Ø§ Ø¨Ø§ Ù¾Ø§Ø³Ø® Ø®ÙˆØ¯ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯.</li>\n",
    "</ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\"> <li>Ù…Ø§ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø´Ø®ØµÛŒ Ø§Ø² Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒØ§Ù† Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª ØªØµØ§Ø¯ÙÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ø®ÙˆØ§Ù‡ÛŒÙ… Ú©Ø±Ø¯. Ø§ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ù†Ø¯ Ú©Ù‡ Ú©Ø¯ÛŒ Ú©Ù‡ Ù†ÙˆØ´ØªÛŒØ¯ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ø§Ú¯Ø± Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ ØµØ­ÛŒØ­ Ø±Ø§ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø®ÙˆØ¯ Ø¨Ø¯ÙˆÙ† Ú©Ø¯ÛŒ Ú©Ù‡ ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø¢Ù† Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ú©Ù†Ø¯ ØªØ­ÙˆÛŒÙ„ Ø¯Ù‡ÛŒØ¯ØŒ Ø§ÛŒÙ† ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> <li>Ù…Ø§ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø±ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø³Ø±Ù‚Øª Ø¹Ù„Ù…ÛŒ Ø¯Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ©â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÙ„Ø¨ Ø§Ù†Ø¬Ø§Ù… Ø®ÙˆØ§Ù‡ÛŒÙ… Ø¯Ø§Ø¯. Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ú©Ø¯ Ø§Ø² Ø¯ÛŒÚ¯Ø±Ø§Ù† Ù†ÛŒØ² ÛŒÚ© Ù…ÙˆØ±Ø¯ Ø¬Ø¯ÛŒ Ø§Ø² Ø¹Ø¯Ù… ØµØ¯Ø§Ù‚Øª Ø¹Ù„Ù…ÛŒ Ù…Ø­Ø³ÙˆØ¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯.</li> </ul>\n",
    "<h4 dir=\"rtl\" style=\"font-family: Vazir; width: 85%;\">ØªÙˆØ¶ÛŒØ­Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ:</h4> <ul dir=\"rtl\" style=\"font-family: Vazir; width: 85%; font-size: 16px;\">\n",
    "<li>\n",
    "Ø®ÙˆØ§Ù†Ø§ÛŒÛŒ Ùˆ Ø¯Ù‚Øª Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø®ÙˆØ±Ø¯Ø§Ø± Ø§Ø³Øª. Ø¨Ù‡ ØªÙ…Ø±ÛŒÙ†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§ØºØ°ÛŒ ØªØ­ÙˆÛŒÙ„ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ ÛŒØ§ Ø¨Ù‡ ØµÙˆØ±Øª Ø¹Ú©Ø³ Ø¯Ø± Ø³Ø§ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´ÙˆÙ†Ø¯ØŒ ØªØ±ØªÛŒØ¨ Ø§Ø«Ø±ÛŒ Ø¯Ø§Ø¯Ù‡ Ù†Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.</li>\n",
    "<li>\n",
    " Ù‡Ù…Ù‡â€ŒÛŒ Ú©Ø¯Ù‡Ø§ÛŒ Ù¾ÛŒÙˆØ³Øª Ú¯Ø²Ø§Ø±Ø´ Ø¨Ø§ÛŒØ³ØªÛŒ Ù‚Ø§Ø¨Ù„ÛŒØª Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯. Ø¯Ø± ØµÙˆØ±ØªÛŒ Ú©Ù‡ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ Ù…Ø¬Ø¯Ø¯ Ø¢Ù†â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø®Ø§ØµÛŒ Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯ØŒ Ø¨Ø§ÛŒØ³ØªÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ù†ÛŒØ² Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´ Ø®ÙˆØ¯ Ø°Ú©Ø± Ú©Ù†ÛŒØ¯.  Ø¯Ù‚Øª Ú©Ù†ÛŒØ¯ Ú©Ù‡  ØªÙ…Ø§Ù…ÛŒ Ú©Ø¯Ù‡Ø§ Ø¨Ø§ÛŒØ¯ ØªÙˆØ³Ø· Ø´Ù…Ø§ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ú©Ø¯Ù‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ø¨Ø§Ø´Ø¯. Ø¨Ù‡ Ú©Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ Ø§Ø±Ø³Ø§Ù„ÛŒ Ù…Ø´Ø®Øµ Ù†Ø¨Ø§Ø´Ø¯ Ù†Ù…Ø±Ù‡â€ŒØ§ÛŒ ØªØ¹Ù„Ù‚ Ù†Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.\n",
    "</li>\n",
    "<li>ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ ØµÙˆØ±Øª ØªÚ©â€ŒÙ†ÙØ±Ù‡ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ Ùˆ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ Ø¨Ø§ÛŒØ¯ Ù†ØªÛŒØ¬Ù‡ ÙØ¹Ø§Ù„ÛŒØª ÙØ±Ø¯ Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ (Ù‡Ù…ÙÚ©Ø±ÛŒ Ùˆ Ø¨Ù‡ Ø§ØªÙØ§Ù‚ Ù‡Ù… Ù†ÙˆØ´ØªÙ† ØªÙ…Ø±ÛŒÙ† Ù†ÛŒØ² Ù…Ù…Ù†ÙˆØ¹ Ø§Ø³Øª). Ø¯Ø± ØµÙˆØ±Øª Ù…Ø´Ø§Ù‡Ø¯Ù‡\n",
    " ØªØ´Ø§Ø¨Ù‡ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø§ÙØ±Ø§Ø¯ Ù…Ø´Ø§Ø±Ú©Øªâ€ŒÚ©Ù†Ù†Ø¯Ù‡ØŒ Ù†Ù…Ø±Ù‡ ØªÙ…Ø±ÛŒÙ† ØµÙØ± Ùˆ Ø¨Ù‡ Ø§Ø³ØªØ§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.\n",
    " </li>\n",
    "\n",
    " <li>\n",
    "Ù„Ø·ÙØ§Ù‹ ØªÙ…Ø§Ù…ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§ <b>ÙÙˆÙ†Øª ÙˆØ²ÛŒØ± (Vazir)</b> Ùˆ Ø¨Ù‡â€ŒØµÙˆØ±Øª <b>Ø±Ø§Ø³Øªâ€ŒÚ†ÛŒÙ†</b> Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÙÙˆÙ†Øªâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯ ØªØ§ Ø¸Ø§Ù‡Ø± Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ù…Ø§ ÛŒÚ©â€ŒØ¯Ø³Øª Ùˆ Ø®ÙˆØ§Ù†Ø§ Ø¨Ø§Ø´Ø¯.  \n",
    "Ø¯Ø± Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ ØªØ´Ø±ÛŒØ­ÛŒØŒ Ø³Ø¹ÛŒ Ú©Ù†ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ Ú©Ø§Ù…Ù„ØŒ Ù…Ù†Ø³Ø¬Ù… Ùˆ Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª Ù†Ú¯Ø§Ø±Ø´ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.  \n",
    "Ù‡Ù…Ú†Ù†ÛŒÙ†ØŒ Ø¨Ù‡ Ú†ÛŒÙ†Ø´ ØªÙ…ÛŒØ² Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø±Ø³Øª Ú©Ø¯Ù‡Ø§ ØªÙˆØ¬Ù‡ Ú©Ù†ÛŒØ¯ ØªØ§ ØªÙ…Ø±ÛŒÙ† Ø´Ù…Ø§ Ø¨Ø§ ÙØ±Ù…Øª Ø®ÙˆØ§Ø³ØªÙ‡â€ŒØ´Ø¯Ù‡ Ùˆ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø§Ø±Ø§Ø¦Ù‡ Ø´ÙˆØ¯.\n",
    "</li>\n",
    " <li>Ø¨Ø±Ø§ÛŒ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø¨ÛŒØ´ØªØ± Ø¯Ø±Ø¨Ø§Ø±Ù‡â€ŒÛŒ ÙØ±Ù…Øª Markdown Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² <a href=\"https://github.com/tajaddini/Persian-Markdown/blob/master/learn-MD.md\">Ø§ÛŒÙ† Ù„ÛŒÙ†Ú©</a> Ù…Ø·Ø§Ù„Ø¹Ù‡ Ú©Ù†ÛŒØ¯.\n",
    " </li>\n",
    " </ul>\n",
    "    \n",
    "\n",
    " </div>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "section2_title",
    "section3_title",
    "section4_title",
    "eval_title",
    "policies_title"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
