{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0751e14b",
   "metadata": {},
   "source": [
    "# NLP CA6 – RAG Pipeline with LangChain, FAISS, TogetherAI, Tavily, and LangGraph\n",
    "\n",
    "This notebook implements a complete Retrieval-Augmented Generation (RAG) system for Persian/English documents, following the assignment specification.\n",
    "\n",
    "\n",
    "\n",
    "We will cover:\n",
    "\n",
    "1. Vector representations and FAISS vector store with caching\n",
    "\n",
    "2. Retriever(s): FAISS semantic, BM25 lexical, and an Ensemble retriever\n",
    "\n",
    "3. Router Chain using TogetherAI (Meta Llama 3 70B, temperature=0)\n",
    "\n",
    "4. Search Engine Chain using Tavily\n",
    "\n",
    "5. (Optional) Relevancy Check Chain\n",
    "\n",
    "6. Fallback Chain\n",
    "\n",
    "7. Generate-with-Context Chain\n",
    "\n",
    "8. Full agent graph with LangGraph\n",
    "\n",
    "\n",
    "\n",
    "All explanations are in English; code supports Persian content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d85431",
   "metadata": {},
   "source": [
    "## 0) Environment Setup\n",
    "\n",
    "We install and import required libraries. Set your API keys in environment variables:\n",
    "\n",
    "\n",
    "\n",
    "- `TOGETHER_API_KEY` for TogetherAI\n",
    "\n",
    "- `TAVILY_API_KEY` for Tavily\n",
    "\n",
    "\n",
    "\n",
    "We will prefer CPU-compatible packages to avoid GPU dependency unless available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b167271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install core dependencies (run once)\n",
    "\n",
    "%pip -q install -U langchain langchain-community langchain-huggingface langgraph pydantic==2.*\n",
    "\n",
    "%pip -q install -U faiss-cpu sentence-transformers\n",
    "\n",
    "%pip -q install -U langchain-together tavily-python\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "# Set API keys via environment variables (edit as needed)\n",
    "\n",
    "# os.environ[\"TOGETHER_API_KEY\"] = \"<your_together_api_key>\"\n",
    "\n",
    "# os.environ[\"TAVILY_API_KEY\"] = \"<your_tavily_api_key>\"\n",
    "\n",
    "\n",
    "\n",
    "BASE_DIR = Path(\"/Users/tahamajs/Documents/uni/NLP/nlp-assignments-spring-2023/NLP_UT/last/NLP-CA6\")\n",
    "\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "\n",
    "DOCS_DIR = DATA_DIR / \"docs\"\n",
    "\n",
    "CACHE_DIR = DATA_DIR / \"emb_cache\"\n",
    "\n",
    "VSTORE_DIR = DATA_DIR / \"faiss_store\"\n",
    "\n",
    "\n",
    "\n",
    "for d in [DATA_DIR, DOCS_DIR, CACHE_DIR, VSTORE_DIR]:\n",
    "\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "BASE_DIR, DATA_DIR, DOCS_DIR, CACHE_DIR, VSTORE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874ce92a",
   "metadata": {},
   "source": [
    "## 1) Data Loading and Chunking\n",
    "\n",
    "\n",
    "\n",
    "We load documents (e.g., PDF, text files) and chunk them into manageable pieces for embedding and retrieval.\n",
    "\n",
    "\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "- **Document Loaders**: LangChain provides loaders for PDFs, text files, etc.\n",
    "\n",
    "- **Text Splitting**: Break long documents into chunks with overlap to preserve context.\n",
    "\n",
    "- **Chunk Size**: Balance between context (larger chunks) and precision (smaller chunks).\n",
    "\n",
    "- **Overlap**: Ensures important information at chunk boundaries isn't lost.\n",
    "\n",
    "\n",
    "\n",
    "For this assignment, you should place your reference book (NLP textbook) chapters in the `docs/` folder as PDF or text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15efbb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader, TextLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "\n",
    "# Load documents from the docs directory\n",
    "\n",
    "# Adjust loader based on your file types (PDF, txt, etc.)\n",
    "\n",
    "try:\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "\n",
    "        str(DOCS_DIR),\n",
    "\n",
    "        glob=\"**/*.pdf\",\n",
    "\n",
    "        loader_cls=PyPDFLoader,\n",
    "\n",
    "        show_progress=True\n",
    "\n",
    "    )\n",
    "\n",
    "    raw_documents = loader.load()\n",
    "\n",
    "    print(f\"Loaded {len(raw_documents)} document pages/sections\")\n",
    "except Exception as e:\n",
    "\n",
    "    print(f\"Error loading PDFs: {e}\")\n",
    "\n",
    "    print(\"Trying text files...\")\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "\n",
    "        str(DOCS_DIR),\n",
    "\n",
    "        glob=\"**/*.txt\",\n",
    "\n",
    "        loader_cls=TextLoader,\n",
    "\n",
    "        show_progress=True\n",
    "\n",
    "    )\n",
    "\n",
    "    raw_documents = loader.load()\n",
    "\n",
    "    print(f\"Loaded {len(raw_documents)} text documents\")\n",
    "\n",
    "\n",
    "\n",
    "# Show sample\n",
    "\n",
    "if raw_documents:\n",
    "\n",
    "    print(f\"\\nSample document (first 300 chars):\\n{raw_documents[0].page_content[:300]}...\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"\\n⚠️ No documents found. Please add PDF or text files to the docs/ directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af821e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk documents into smaller pieces\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "\n",
    "    chunk_size=1000,        # characters per chunk\n",
    "\n",
    "    chunk_overlap=200,      # overlap to preserve context\n",
    "\n",
    "    length_function=len,\n",
    "\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "chunked_documents = text_splitter.split_documents(raw_documents)\n",
    "\n",
    "print(f\"\\nTotal chunks created: {len(chunked_documents)}\")\n",
    "\n",
    "print(f\"Sample chunk (first 200 chars):\\n{chunked_documents[0].page_content[:200]}...\")\n",
    "\n",
    "\n",
    "\n",
    "# Statistics\n",
    "\n",
    "chunk_lengths = [len(doc.page_content) for doc in chunked_documents]\n",
    "\n",
    "print(f\"\\nChunk Statistics:\")\n",
    "\n",
    "print(f\"  Average chunk length: {sum(chunk_lengths)/len(chunk_lengths):.0f} chars\")\n",
    "\n",
    "print(f\"  Min chunk length: {min(chunk_lengths)} chars\")\n",
    "\n",
    "print(f\"  Max chunk length: {max(chunk_lengths)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a50de57",
   "metadata": {},
   "source": [
    "## 2) Vector Representations and FAISS Vector Store\n",
    "\n",
    "\n",
    "\n",
    "### Part A: Embeddings with HuggingFace and FAISS\n",
    "\n",
    "\n",
    "\n",
    "We use **HuggingFaceEmbeddings** to create vector representations of text chunks, then store them in **FAISS** (Facebook AI Similarity Search) for efficient retrieval.\n",
    "\n",
    "\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "\n",
    "\n",
    "#### Why Embeddings?\n",
    "\n",
    "- Convert text into dense numerical vectors (e.g., 768-dimensional)\n",
    "\n",
    "- Semantic similarity: similar texts have similar vectors\n",
    "\n",
    "- Enable vector search (find semantically similar documents)\n",
    "\n",
    "\n",
    "\n",
    "#### FAISS:\n",
    "\n",
    "- Developed by Meta (Facebook AI)\n",
    "\n",
    "- Optimized for fast similarity search in high-dimensional spaces\n",
    "\n",
    "- Supports billions of vectors with efficient indexing\n",
    "\n",
    "- CPU and GPU implementations available\n",
    "\n",
    "\n",
    "\n",
    "#### Cache-Backed Embeddings:\n",
    "\n",
    "- **Problem**: Re-embedding documents on every run is slow and expensive\n",
    "\n",
    "- **Solution**: `CacheBackedEmbeddings` stores embeddings on disk\n",
    "\n",
    "- First run: compute and cache embeddings\n",
    "\n",
    "- Subsequent runs: load from cache (much faster)\n",
    "\n",
    "- Trade-off: disk I/O vs GPU computation time\n",
    "\n",
    "\n",
    "\n",
    "For GPU-heavy workloads with many documents, re-computing on GPU might be faster than disk reads, but for moderate datasets, caching is highly beneficial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6e17f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.storage import LocalFileStore\n",
    "\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the base embedder\n",
    "\n",
    "# Default model: sentence-transformers/all-MiniLM-L6-v2 (English, 384-dim)\n",
    "\n",
    "# For Persian/multilingual: consider \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "base_embedder = HuggingFaceEmbeddings(\n",
    "\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "\n",
    "    model_kwargs={'device': 'cpu'},  # Change to 'cuda' if GPU available\n",
    "\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Setup cache-backed embeddings\n",
    "\n",
    "cache_store = LocalFileStore(str(CACHE_DIR))\n",
    "\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "\n",
    "    base_embedder,\n",
    "\n",
    "    cache_store,\n",
    "\n",
    "    namespace=\"huggingface_embeddings\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Embedder initialized with caching enabled\")\n",
    "\n",
    "print(f\"Cache directory: {CACHE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa6fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or load FAISS vector store\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "faiss_index_path = VSTORE_DIR / \"faiss_index\"\n",
    "\n",
    "\n",
    "\n",
    "if faiss_index_path.exists():\n",
    "\n",
    "    print(\"Loading existing FAISS index...\")\n",
    "\n",
    "    vectorstore = FAISS.load_local(\n",
    "\n",
    "        str(VSTORE_DIR),\n",
    "\n",
    "        cached_embedder,\n",
    "\n",
    "        \"faiss_index\",\n",
    "\n",
    "        allow_dangerous_deserialization=True\n",
    "\n",
    "    )\n",
    "\n",
    "    print(f\"Loaded {vectorstore.index.ntotal} vectors from disk\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Creating new FAISS index (this may take a few minutes on first run)...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "\n",
    "    vectorstore = FAISS.from_documents(\n",
    "\n",
    "        chunked_documents,\n",
    "\n",
    "        cached_embedder\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "\n",
    "    print(f\"FAISS index created in {elapsed:.1f} seconds\")\n",
    "\n",
    "    print(f\"Total vectors: {vectorstore.index.ntotal}\")\n",
    "\n",
    "    \n",
    "\n",
    "    # Save to disk\n",
    "\n",
    "    vectorstore.save_local(str(VSTORE_DIR), \"faiss_index\")\n",
    "\n",
    "    print(f\"Index saved to {VSTORE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a906ebda",
   "metadata": {},
   "source": [
    "### Part B: Importance of Choosing the Right Embedder\n",
    "\n",
    "\n",
    "\n",
    "**Why does the choice of embedder matter?**\n",
    "\n",
    "\n",
    "\n",
    "#### 1. Language-Specific Training\n",
    "\n",
    "If we use an embedder trained **only on English** to embed **Persian** text:\n",
    "\n",
    "\n",
    "\n",
    "**Problems:**\n",
    "\n",
    "- **Out-of-vocabulary tokens**: Persian words may be split into meaningless subwords\n",
    "\n",
    "- **Poor semantic understanding**: Model hasn't learned Persian grammar, idioms, or word relationships\n",
    "\n",
    "- **Low-quality embeddings**: Similar Persian sentences may have dissimilar vectors\n",
    "\n",
    "- **Retrieval failures**: Relevant Persian documents won't be found\n",
    "\n",
    "\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```\n",
    "\n",
    "Query: \"سلام دنیا\" (Hello world)\n",
    "\n",
    "English-only model: May treat each Persian character as noise\n",
    "\n",
    "Result: Cannot find semantically similar Persian documents\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Domain-Specific Models\n",
    "\n",
    "- **General models** (e.g., all-MiniLM): Good for everyday language\n",
    "\n",
    "- **Domain models** (e.g., biomedical, legal): Better for specialized text\n",
    "\n",
    "- **Multilingual models** (e.g., paraphrase-multilingual): Support 50+ languages\n",
    "\n",
    "\n",
    "\n",
    "#### 3. For Persian NLP:\n",
    "\n",
    "**Recommended models:**\n",
    "\n",
    "- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` (50+ languages)\n",
    "\n",
    "- `sentence-transformers/LaBSE` (109 languages, high quality)\n",
    "\n",
    "- Persian-specific models from HuggingFace (search for \"persian\" or \"farsi\")\n",
    "\n",
    "\n",
    "\n",
    "#### 4. Trade-offs:\n",
    "\n",
    "- **Size vs Quality**: Larger models (768-dim) often perform better than smaller (384-dim)\n",
    "\n",
    "- **Speed vs Accuracy**: Faster models may sacrifice some quality\n",
    "\n",
    "- **Language coverage**: Multilingual models may be weaker per-language than monolingual\n",
    "\n",
    "\n",
    "\n",
    "**Bottom line**: Always choose an embedder that:\n",
    "\n",
    "1. Supports your target language(s)\n",
    "\n",
    "2. Matches your domain if possible\n",
    "\n",
    "3. Balances quality and computational resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da4dc2c",
   "metadata": {},
   "source": [
    "## 3) Retriever Implementation\n",
    "\n",
    "\n",
    "\n",
    "### Part A (Basic): FAISS Semantic Retriever\n",
    "\n",
    "\n",
    "\n",
    "A retriever takes a query and returns the most relevant documents from the vector store.\n",
    "\n",
    "\n",
    "\n",
    "We'll implement:\n",
    "\n",
    "1. **Semantic retriever** using FAISS (vector similarity)\n",
    "\n",
    "2. Test with 3 queries as specified in the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c17eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FAISS-based semantic retriever\n",
    "\n",
    "faiss_retriever = vectorstore.as_retriever(\n",
    "\n",
    "    search_type=\"similarity\",\n",
    "\n",
    "    search_kwargs={\"k\": 5}  # Return top 5 documents\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Test queries (as per assignment requirements)\n",
    "\n",
    "test_queries = [\n",
    "\n",
    "    \"What is natural language processing?\",  # NLP-related (in-domain)\n",
    "\n",
    "    \"Explain binary search trees\",            # Computer science (out-of-NLP-domain)\n",
    "\n",
    "    \"Who is the president of Bolivia?\"        # General knowledge (out-of-scope)\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"TESTING FAISS SEMANTIC RETRIEVER\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "\n",
    "    print(f\"Query {i}: {query}\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    \n",
    "\n",
    "    docs = faiss_retriever.invoke(query)\n",
    "\n",
    "    \n",
    "\n",
    "    print(f\"\\nRetrieved {len(docs)} documents:\\n\")\n",
    "\n",
    "    for j, doc in enumerate(docs, 1):\n",
    "\n",
    "        print(f\"[Doc {j}] (first 150 chars)\")\n",
    "\n",
    "        print(f\"{doc.page_content[:150]}...\")\n",
    "\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722b9e71",
   "metadata": {},
   "source": [
    "### Part B (Bonus): Hybrid Retriever with BM25 and Ensemble\n",
    "\n",
    "\n",
    "\n",
    "**Lexical vs Semantic Retrieval:**\n",
    "\n",
    "\n",
    "\n",
    "#### Lexical Retrieval (BM25):\n",
    "\n",
    "- **Based on**: Exact keyword matching\n",
    "\n",
    "- **Algorithm**: BM25 (Best Matching 25) - a probabilistic ranking function\n",
    "\n",
    "- **How it works**: \n",
    "\n",
    "  - Counts term frequency (TF) in documents\n",
    "\n",
    "  - Considers document length normalization\n",
    "\n",
    "  - Uses inverse document frequency (IDF) for term importance\n",
    "\n",
    "- **Strengths**:\n",
    "\n",
    "  - Fast and lightweight (no embeddings needed)\n",
    "\n",
    "  - Excellent for exact keyword matches\n",
    "\n",
    "  - Works well with technical terms, names, acronyms\n",
    "\n",
    "  - No training required\n",
    "\n",
    "- **Weaknesses**:\n",
    "\n",
    "  - No semantic understanding (\"car\" ≠ \"automobile\")\n",
    "\n",
    "  - Fails on paraphrasing or synonyms\n",
    "\n",
    "  - Sensitive to exact wording\n",
    "\n",
    "\n",
    "\n",
    "#### Semantic Retrieval (FAISS):\n",
    "\n",
    "- **Based on**: Vector similarity in embedding space\n",
    "\n",
    "- **How it works**:\n",
    "\n",
    "  - Convert query and documents to dense vectors\n",
    "\n",
    "  - Find nearest neighbors using cosine similarity or L2 distance\n",
    "\n",
    "- **Strengths**:\n",
    "\n",
    "  - Understands semantic meaning\n",
    "\n",
    "  - Handles synonyms, paraphrasing\n",
    "\n",
    "  - Cross-lingual retrieval possible\n",
    "\n",
    "  - Captures context and intent\n",
    "\n",
    "- **Weaknesses**:\n",
    "\n",
    "  - Computationally expensive (embeddings + indexing)\n",
    "\n",
    "  - May miss exact keyword matches\n",
    "\n",
    "  - Requires good quality embedder\n",
    "\n",
    "\n",
    "\n",
    "#### Ensemble Retriever:\n",
    "\n",
    "Combines both approaches:\n",
    "\n",
    "- **Weighted fusion**: `score = w1 * lexical_score + w2 * semantic_score`\n",
    "\n",
    "- **Best of both worlds**: Keyword precision + semantic understanding\n",
    "\n",
    "- **Use case**: When you want robustness across different query types\n",
    "\n",
    "\n",
    "\n",
    "**Importance by use case:**\n",
    "\n",
    "- **Technical documentation**: Lexical (exact API names, functions)\n",
    "\n",
    "- **Conversational QA**: Semantic (natural language understanding)\n",
    "\n",
    "- **Hybrid systems**: Ensemble (maximum coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d81ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "\n",
    "\n",
    "# Create BM25 lexical retriever\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(chunked_documents)\n",
    "\n",
    "bm25_retriever.k = 5  # Return top 5\n",
    "\n",
    "\n",
    "\n",
    "print(\"BM25 lexical retriever initialized\")\n",
    "\n",
    "print(f\"Total documents indexed: {len(chunked_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4196bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with weight combinations\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"EXPERIMENTING WITH ENSEMBLE WEIGHTS\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "# Test different weight configurations\n",
    "\n",
    "weight_configs = [\n",
    "\n",
    "    (1.0, 0.0, \"100% Lexical (BM25)\"),\n",
    "\n",
    "    (0.0, 1.0, \"100% Semantic (FAISS)\"),\n",
    "\n",
    "    (0.5, 0.5, \"50-50 Balanced\"),\n",
    "\n",
    "    (0.3, 0.7, \"30% Lexical, 70% Semantic (Recommended for English)\"),\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "test_query = \"What is natural language processing?\"\n",
    "\n",
    "print(f\"\\nTest Query: '{test_query}'\\n\")\n",
    "\n",
    "\n",
    "\n",
    "for bm25_weight, faiss_weight, description in weight_configs:\n",
    "\n",
    "    ensemble = EnsembleRetriever(\n",
    "\n",
    "        retrievers=[bm25_retriever, faiss_retriever],\n",
    "\n",
    "        weights=[bm25_weight, faiss_weight]\n",
    "\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "    docs = ensemble.invoke(test_query)\n",
    "\n",
    "    print(f\"\\n{description}:\")\n",
    "\n",
    "    print(f\"  Retrieved {len(docs)} documents\")\n",
    "\n",
    "    print(f\"  First doc preview: {docs[0].page_content[:100]}...\" if docs else \"  No docs\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "print(\"CONCLUSION: For strong English embedders, semantic weight 0.6-0.7 often works best.\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0558f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final ensemble retriever with chosen weights\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "\n",
    "    retrievers=[bm25_retriever, faiss_retriever],\n",
    "\n",
    "    weights=[0.3, 0.7]  # 30% lexical, 70% semantic\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"TESTING FINAL ENSEMBLE RETRIEVER\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "\n",
    "    print(f\"Query {i}: {query}\")\n",
    "\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    \n",
    "\n",
    "    docs = ensemble_retriever.invoke(query)\n",
    "\n",
    "    \n",
    "\n",
    "    print(f\"\\nRetrieved {len(docs)} documents:\\n\")\n",
    "\n",
    "    for j, doc in enumerate(docs, 1):\n",
    "\n",
    "        print(f\"[Doc {j}] (first 150 chars)\")\n",
    "\n",
    "        print(f\"{doc.page_content[:150]}...\")\n",
    "\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aee4cc",
   "metadata": {},
   "source": [
    "## 4) Chain Implementations\n",
    "\n",
    "\n",
    "\n",
    "### Understanding Chains in LangChain\n",
    "\n",
    "\n",
    "\n",
    "A **chain** is a sequence of components connected in a directed acyclic graph (DAG):\n",
    "\n",
    "```\n",
    "\n",
    "Input → Prompt Template → LLM → Output Parser → Result\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**Three main components:**\n",
    "\n",
    "1. **Prompt Template**: Format input with placeholders for variables\n",
    "\n",
    "2. **LLM**: Large language model that generates responses\n",
    "\n",
    "3. **Output Parser**: Validates and structures the LLM output\n",
    "\n",
    "\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "\n",
    "chain = prompt | llm | parser\n",
    "\n",
    "result = chain.invoke({\"query\": \"Hello\"})\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "The `|` operator connects components into a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203af76c",
   "metadata": {},
   "source": [
    "### 4.1) Router Chain (TogetherAI + Llama 3 70B)\n",
    "\n",
    "\n",
    "\n",
    "**Purpose**: Classify user queries into three categories:\n",
    "\n",
    "1. `VectorStore`: NLP-related questions (use local knowledge base)\n",
    "\n",
    "2. `SearchEngine`: Computer science but non-NLP (use web search)\n",
    "\n",
    "3. `None` (Fallback): Out-of-scope questions\n",
    "\n",
    "\n",
    "\n",
    "**Why Temperature=0?**\n",
    "\n",
    "- Temperature controls randomness in LLM outputs\n",
    "\n",
    "- **Temperature=0**: Deterministic, always picks most likely token\n",
    "\n",
    "- **Temperature>0**: Adds randomness, more creative but less predictable\n",
    "\n",
    "- For classification/routing, we want **consistent, deterministic** decisions\n",
    "\n",
    "- Higher temperature would cause unpredictable routing—bad for a decision system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9de8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_together import ChatTogether\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "\n",
    "# Initialize TogetherAI LLM\n",
    "\n",
    "router_llm = ChatTogether(\n",
    "\n",
    "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
    "\n",
    "    temperature=0.0,\n",
    "\n",
    "    max_tokens=100\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ TogetherAI LLM initialized (Llama 3 70B, temp=0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7eb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Pydantic output schema for router\n",
    "\n",
    "class RouterOutput(BaseModel):\n",
    "\n",
    "    \"\"\"Router decision for query classification.\"\"\"\n",
    "\n",
    "    tool: Literal[\"VectorStore\", \"SearchEngine\", \"None\"] = Field(\n",
    "\n",
    "        description=\"The tool to use: VectorStore for NLP topics, SearchEngine for general CS topics, None for out-of-scope\"\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "router_parser = PydanticOutputParser(pydantic_object=RouterOutput)\n",
    "\n",
    "\n",
    "\n",
    "# Router prompt\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "    (\"system\", \"\"\"You are a query classifier for an NLP chatbot.\n",
    "\n",
    "\n",
    "\n",
    "Classify the user's query into ONE of these categories:\n",
    "\n",
    "- VectorStore: Questions about Natural Language Processing (NLP), machine learning for text, transformers, etc.\n",
    "\n",
    "- SearchEngine: Questions about Computer Science topics OUTSIDE of NLP (e.g., algorithms, data structures, OS)\n",
    "\n",
    "- None: Questions completely outside the chatbot's domain (e.g., geography, politics, history)\n",
    "\n",
    "\n",
    "\n",
    "Respond ONLY with the tool name.\n",
    "\n",
    "\n",
    "\n",
    "{format_instructions}\"\"\"),\n",
    "\n",
    "    (\"human\", \"{query}\")\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Build router chain\n",
    "\n",
    "router_chain = router_prompt | router_llm | router_parser\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ Router chain assembled\")\n",
    "\n",
    "print(f\"Output format: {router_parser.get_format_instructions()[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95283354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test router chain\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"TESTING ROUTER CHAIN\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "test_routing_queries = [\n",
    "\n",
    "    \"What is tokenization in NLP?\",\n",
    "\n",
    "    \"Explain quicksort algorithm\",\n",
    "\n",
    "    \"What is the capital of France?\"\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for query in test_routing_queries:\n",
    "\n",
    "    result = router_chain.invoke({\n",
    "\n",
    "        \"query\": query,\n",
    "\n",
    "        \"format_instructions\": router_parser.get_format_instructions()\n",
    "\n",
    "    })\n",
    "\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "\n",
    "    print(f\"  → Routed to: {result.tool}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b9f8e",
   "metadata": {},
   "source": [
    "### 4.2) Search Engine Chain (Tavily)\n",
    "\n",
    "\n",
    "\n",
    "**Tavily** is an API service optimized for LLM-based applications:\n",
    "\n",
    "- Retrieves high-quality web search results\n",
    "\n",
    "- Returns content in LLM-friendly format\n",
    "\n",
    "- Free tier: 1000 searches/month\n",
    "\n",
    "\n",
    "\n",
    "Our chain will:\n",
    "\n",
    "1. Query Tavily API\n",
    "\n",
    "2. Parse results into LangChain `Document` objects\n",
    "\n",
    "3. Return top 5 results with content + URL metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deac849",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Tavily search tool\n",
    "\n",
    "tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "\n",
    "\n",
    "# Post-processor to convert Tavily results to LangChain Documents\n",
    "\n",
    "def tavily_to_documents(results):\n",
    "\n",
    "    \"\"\"Convert Tavily results to Document objects.\"\"\"\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for result in results:\n",
    "\n",
    "        doc = Document(\n",
    "\n",
    "            page_content=result.get(\"content\", \"\"),\n",
    "\n",
    "            metadata={\"url\": result.get(\"url\", \"\")}\n",
    "\n",
    "        )\n",
    "\n",
    "        documents.append(doc)\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "# Build search engine chain\n",
    "\n",
    "search_engine_chain = tavily_tool | RunnableLambda(tavily_to_documents)\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ Search Engine chain assembled\")\n",
    "\n",
    "print(\"  Input: query string\")\n",
    "\n",
    "print(\"  Output: List[Document] with page_content and url metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3600b66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test search engine chain\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"TESTING SEARCH ENGINE CHAIN\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "\n",
    "test_search_query = \"What are transformers in deep learning?\"\n",
    "\n",
    "print(f\"\\nQuery: {test_search_query}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "search_results = search_engine_chain.invoke(test_search_query)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Retrieved {len(search_results)} documents:\\n\")\n",
    "\n",
    "for i, doc in enumerate(search_results, 1):\n",
    "\n",
    "    print(f\"[Result {i}]\")\n",
    "\n",
    "    print(f\"  Content (first 150 chars): {doc.page_content[:150]}...\")\n",
    "\n",
    "    print(f\"  URL: {doc.metadata['url']}\")\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3672eb",
   "metadata": {},
   "source": [
    "### 4.3) Relevancy Check Chain (Bonus/Optional)\n",
    "\n",
    "\n",
    "\n",
    "**Purpose**: Filter retrieved documents by relevance to the query.\n",
    "\n",
    "\n",
    "\n",
    "**Why needed?**\n",
    "\n",
    "- Retrievers may return marginally relevant documents\n",
    "\n",
    "- Low-quality documents can confuse the LLM during generation\n",
    "\n",
    "- Better to filter out noise before final answer generation\n",
    "\n",
    "\n",
    "\n",
    "**Example scenario:**\n",
    "\n",
    "- Query: \"What is BERT?\"\n",
    "\n",
    "- Retrieved doc talks about \"Sesame Street's Bert character\"\n",
    "\n",
    "- Relevancy check marks it as `irrelevant`\n",
    "\n",
    "- Only NLP-related BERT docs proceed to generation\n",
    "\n",
    "\n",
    "\n",
    "This chain evaluates each document individually and returns filtered list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevancy check output schema\n",
    "\n",
    "class RelevancyOutput(BaseModel):\n",
    "\n",
    "    \"\"\"Relevancy classification result.\"\"\"\n",
    "\n",
    "    relevance: Literal[\"relevant\", \"irrelevant\"] = Field(\n",
    "\n",
    "        description=\"Whether the document is relevant to the query\"\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "relevancy_parser = PydanticOutputParser(pydantic_object=RelevancyOutput)\n",
    "\n",
    "\n",
    "\n",
    "# Relevancy prompt\n",
    "\n",
    "relevancy_prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "    (\"system\", \"\"\"You are a relevancy judge. Given a query and a document, determine if the document is relevant to answering the query.\n",
    "\n",
    "\n",
    "\n",
    "Respond with ONLY 'relevant' or 'irrelevant'.\n",
    "\n",
    "\n",
    "\n",
    "{format_instructions}\"\"\"),\n",
    "\n",
    "    (\"human\", \"\"\"Query: {query}\n",
    "\n",
    "\n",
    "\n",
    "Document: {document}\n",
    "\n",
    "\n",
    "\n",
    "Is this document relevant to the query?\"\"\")\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Relevancy check chain (single document)\n",
    "\n",
    "relevancy_check_chain = relevancy_prompt | router_llm | relevancy_parser\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ Relevancy check chain assembled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df57cce",
   "metadata": {},
   "source": [
    "### 4.4) Fallback Chain\n",
    "\n",
    "\n",
    "\n",
    "When the router determines a query is out-of-scope (`None`), this chain:\n",
    "\n",
    "1. Takes the query + chat history\n",
    "\n",
    "2. Politely informs the user the chatbot cannot help\n",
    "\n",
    "3. Explains the chatbot's domain (NLP topics)\n",
    "\n",
    "\n",
    "\n",
    "We can use higher temperature here (e.g., 0.3-0.7) for more natural/varied responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2946e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "from langchain.schema.messages import BaseMessage\n",
    "\n",
    "\n",
    "\n",
    "# Helper to convert chat history to string\n",
    "\n",
    "def format_chat_history(messages: list[BaseMessage]) -> str:\n",
    "\n",
    "    \"\"\"Convert LangChain messages to readable text.\"\"\"\n",
    "\n",
    "    if not messages:\n",
    "\n",
    "        return \"No previous conversation.\"\n",
    "\n",
    "    \n",
    "\n",
    "    formatted = []\n",
    "\n",
    "    for msg in messages:\n",
    "\n",
    "        role = \"User\" if msg.type == \"human\" else \"Assistant\"\n",
    "\n",
    "        formatted.append(f\"{role}: {msg.content}\")\n",
    "\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "\n",
    "\n",
    "# Fallback LLM (can use higher temperature)\n",
    "\n",
    "fallback_llm = ChatTogether(\n",
    "\n",
    "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
    "\n",
    "    temperature=0.5,\n",
    "\n",
    "    max_tokens=200\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Fallback prompt\n",
    "\n",
    "fallback_prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "    (\"system\", \"\"\"You are an NLP (Natural Language Processing) chatbot assistant.\n",
    "\n",
    "\n",
    "\n",
    "Your domain: Natural language processing, machine learning for text, transformers, language models, etc.\n",
    "\n",
    "\n",
    "\n",
    "When asked about topics outside your domain, politely explain you can only help with NLP-related questions.\n",
    "\n",
    "\n",
    "\n",
    "Chat History:\n",
    "\n",
    "{chat_history}\"\"\"),\n",
    "\n",
    "    (\"human\", \"{query}\")\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Fallback chain\n",
    "\n",
    "fallback_chain = fallback_prompt | fallback_llm | StrOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ Fallback chain assembled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8502e51",
   "metadata": {},
   "source": [
    "### 4.5) Generate-with-Context Chain\n",
    "\n",
    "\n",
    "\n",
    "The final generation chain that:\n",
    "\n",
    "1. Takes user query + retrieved relevant documents\n",
    "\n",
    "2. Uses LLM to synthesize an answer based on the context\n",
    "\n",
    "3. Returns a natural language response\n",
    "\n",
    "\n",
    "\n",
    "This is the core RAG (Retrieval-Augmented Generation) component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db96409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation LLM\n",
    "\n",
    "generate_llm = ChatTogether(\n",
    "\n",
    "    model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
    "\n",
    "    temperature=0.3,  # Some creativity but still focused\n",
    "\n",
    "    max_tokens=512\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Helper to format documents\n",
    "\n",
    "def format_documents(docs: list[Document]) -> str:\n",
    "\n",
    "    \"\"\"Format documents as numbered context.\"\"\"\n",
    "\n",
    "    if not docs:\n",
    "\n",
    "        return \"No relevant documents found.\"\n",
    "\n",
    "    \n",
    "\n",
    "    formatted = []\n",
    "\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "\n",
    "        formatted.append(f\"[Document {i}]\\n{doc.page_content}\")\n",
    "\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "\n",
    "\n",
    "# Generation prompt\n",
    "\n",
    "generate_prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "    (\"system\", \"\"\"You are a helpful NLP assistant. Answer the user's question using ONLY the provided documents as context.\n",
    "\n",
    "\n",
    "\n",
    "If the documents don't contain enough information to answer, say so honestly.\n",
    "\n",
    "\n",
    "\n",
    "Context Documents:\n",
    "\n",
    "{context}\"\"\"),\n",
    "\n",
    "    (\"human\", \"{query}\")\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Generate-with-context chain\n",
    "\n",
    "generate_with_context_chain = generate_prompt | generate_llm | StrOutputParser()\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ Generate-with-context chain assembled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a0d822",
   "metadata": {},
   "source": [
    "## 5) LangGraph Agent Assembly\n",
    "\n",
    "\n",
    "\n",
    "Now we connect all chains into a state graph using **LangGraph**.\n",
    "\n",
    "\n",
    "\n",
    "### Agent State\n",
    "\n",
    "The state tracks all data flowing through the graph:\n",
    "\n",
    "```python\n",
    "\n",
    "- query: str               # User's question\n",
    "\n",
    "- chat_history: list       # Conversation history\n",
    "\n",
    "- documents: list[Document] # Retrieved docs\n",
    "\n",
    "- generation: str          # Final answer\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Nodes (Functions):\n",
    "\n",
    "1. **router_node**: Classifies query → decides next tool\n",
    "\n",
    "2. **vector_store**: Retrieves from FAISS\n",
    "\n",
    "3. **search_engine**: Retrieves from Tavily web search\n",
    "\n",
    "4. **filter_docs** (optional): Filters irrelevant documents\n",
    "\n",
    "5. **fallback**: Handles out-of-scope queries\n",
    "\n",
    "6. **generate_with_context**: Generates final answer\n",
    "\n",
    "\n",
    "\n",
    "### Edges (Conditional Routing):\n",
    "\n",
    "- Router → VectorStore / SearchEngine / Fallback\n",
    "\n",
    "- VectorStore → FilterDocs → Generate (or direct to Generate if no filtering)\n",
    "\n",
    "- SearchEngine → FilterDocs → Generate (or direct)\n",
    "\n",
    "- Fallback → END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ba38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "\n",
    "\n",
    "# Define agent state\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "\n",
    "    \"\"\"State dictionary tracking data through the graph.\"\"\"\n",
    "\n",
    "    query: str\n",
    "\n",
    "    chat_history: list[BaseMessage]\n",
    "\n",
    "    generation: str\n",
    "\n",
    "    documents: list[Document]\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ AgentState defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a3502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define node functions\n",
    "\n",
    "\n",
    "\n",
    "def router_node(state: AgentState) -> AgentState:\n",
    "\n",
    "    \"\"\"Route query to appropriate tool.\"\"\"\n",
    "\n",
    "    result = router_chain.invoke({\n",
    "\n",
    "        \"query\": state[\"query\"],\n",
    "\n",
    "        \"format_instructions\": router_parser.get_format_instructions()\n",
    "\n",
    "    })\n",
    "\n",
    "    state[\"route\"] = result.tool\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def vector_store_node(state: AgentState) -> AgentState:\n",
    "\n",
    "    \"\"\"Retrieve documents from local vector store.\"\"\"\n",
    "\n",
    "    docs = ensemble_retriever.invoke(state[\"query\"])\n",
    "\n",
    "    state[\"documents\"] = docs\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def search_engine_node(state: AgentState) -> AgentState:\n",
    "\n",
    "    \"\"\"Retrieve documents from web search.\"\"\"\n",
    "\n",
    "    docs = search_engine_chain.invoke(state[\"query\"])\n",
    "\n",
    "    state[\"documents\"] = docs\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def filter_docs_node(state: AgentState) -> AgentState:\n",
    "\n",
    "    \"\"\"Filter documents by relevancy (optional bonus).\"\"\"\n",
    "\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    docs = state.get(\"documents\", [])\n",
    "\n",
    "    \n",
    "\n",
    "    filtered = []\n",
    "\n",
    "    for doc in docs:\n",
    "\n",
    "        try:\n",
    "\n",
    "            result = relevancy_check_chain.invoke({\n",
    "\n",
    "                \"query\": query,\n",
    "\n",
    "                \"document\": doc.page_content[:500],  # Limit length for efficiency\n",
    "\n",
    "                \"format_instructions\": relevancy_parser.get_format_instructions()\n",
    "\n",
    "            })\n",
    "\n",
    "            if result.relevance == \"relevant\":\n",
    "\n",
    "                filtered.append(doc)\n",
    "\n",
    "        except:\n",
    "\n",
    "            # If relevancy check fails, keep the document\n",
    "\n",
    "            filtered.append(doc)\n",
    "\n",
    "    \n",
    "\n",
    "    state[\"documents\"] = filtered\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def fallback_node(state: AgentState) -> AgentState:\n",
    "\n",
    "    \"\"\"Handle out-of-scope queries.\"\"\"\n",
    "\n",
    "    response = fallback_chain.invoke({\n",
    "\n",
    "        \"query\": state[\"query\"],\n",
    "\n",
    "        \"chat_history\": format_chat_history(state.get(\"chat_history\", []))\n",
    "\n",
    "    })\n",
    "\n",
    "    state[\"generation\"] = response\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "def generate_with_context_node(state: AgentState) -> AgentState:\n",
    "\n",
    "    \"\"\"Generate answer using retrieved documents.\"\"\"\n",
    "\n",
    "    docs = state.get(\"documents\", [])\n",
    "\n",
    "    response = generate_with_context_chain.invoke({\n",
    "\n",
    "        \"query\": state[\"query\"],\n",
    "\n",
    "        \"context\": format_documents(docs)\n",
    "\n",
    "    })\n",
    "\n",
    "    state[\"generation\"] = response\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ All node functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec8c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "\n",
    "\n",
    "# Add nodes\n",
    "\n",
    "workflow.add_node(\"router\", router_node)\n",
    "\n",
    "workflow.add_node(\"vector_store\", vector_store_node)\n",
    "\n",
    "workflow.add_node(\"search_engine\", search_engine_node)\n",
    "\n",
    "workflow.add_node(\"filter_docs\", filter_docs_node)  # Optional bonus node\n",
    "\n",
    "workflow.add_node(\"fallback\", fallback_node)\n",
    "\n",
    "workflow.add_node(\"generate\", generate_with_context_node)\n",
    "\n",
    "\n",
    "\n",
    "# Set entry point\n",
    "\n",
    "workflow.set_entry_point(\"router\")\n",
    "\n",
    "\n",
    "\n",
    "# Define conditional routing from router\n",
    "\n",
    "def route_query(state: AgentState) -> str:\n",
    "\n",
    "    \"\"\"Determine which node to call based on router decision.\"\"\"\n",
    "\n",
    "    route = state.get(\"route\", \"None\")\n",
    "\n",
    "    if route == \"VectorStore\":\n",
    "\n",
    "        return \"vector_store\"\n",
    "\n",
    "    elif route == \"SearchEngine\":\n",
    "\n",
    "        return \"search_engine\"\n",
    "\n",
    "    else:  # None or fallback\n",
    "\n",
    "        return \"fallback\"\n",
    "\n",
    "\n",
    "\n",
    "# Add conditional edges from router\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "\n",
    "    \"router\",\n",
    "\n",
    "    route_query,\n",
    "\n",
    "    {\n",
    "\n",
    "        \"vector_store\": \"vector_store\",\n",
    "\n",
    "        \"search_engine\": \"search_engine\",\n",
    "\n",
    "        \"fallback\": \"fallback\"\n",
    "\n",
    "    }\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# WITH bonus filter_docs node:\n",
    "\n",
    "workflow.add_edge(\"vector_store\", \"filter_docs\")\n",
    "\n",
    "workflow.add_edge(\"search_engine\", \"filter_docs\")\n",
    "\n",
    "workflow.add_edge(\"filter_docs\", \"generate\")\n",
    "\n",
    "\n",
    "\n",
    "# WITHOUT bonus (direct to generate) - comment out above 3 lines and uncomment these:\n",
    "\n",
    "# workflow.add_edge(\"vector_store\", \"generate\")\n",
    "\n",
    "# workflow.add_edge(\"search_engine\", \"generate\")\n",
    "\n",
    "\n",
    "\n",
    "# Fallback and generate both end the workflow\n",
    "\n",
    "workflow.add_edge(\"fallback\", END)\n",
    "\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "\n",
    "\n",
    "# Compile the graph\n",
    "\n",
    "app = workflow.compile()\n",
    "\n",
    "\n",
    "\n",
    "print(\"✅ LangGraph compiled successfully!\")\n",
    "\n",
    "print(\"\\nGraph structure:\")\n",
    "\n",
    "print(\"  router → [vector_store | search_engine | fallback]\")\n",
    "\n",
    "print(\"  vector_store → filter_docs → generate → END\")\n",
    "\n",
    "print(\"  search_engine → filter_docs → generate → END\")\n",
    "\n",
    "print(\"  fallback → END\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0427f225",
   "metadata": {},
   "source": [
    "## 6️⃣ Testing the RAG Pipeline\n",
    "\n",
    "\n",
    "\n",
    "Now let's test our complete RAG system with three types of queries:\n",
    "\n",
    "\n",
    "\n",
    "### Test Query Types\n",
    "\n",
    "\n",
    "\n",
    "1. **NLP-Related Query**: Should route to `VectorStore` and retrieve relevant documents from our corpus\n",
    "\n",
    "2. **Computer Science (Non-NLP) Query**: Should route to `SearchEngine` for web search results\n",
    "\n",
    "3. **Out-of-Scope Query**: Should route to `Fallback` for a general response\n",
    "\n",
    "\n",
    "\n",
    "### Execution Flow\n",
    "\n",
    "\n",
    "\n",
    "For each query, we'll:\n",
    "\n",
    "- Display the router's classification decision\n",
    "\n",
    "- Show retrieved/searched documents (if applicable)\n",
    "\n",
    "- Present the final generated answer\n",
    "\n",
    "- Analyze the routing correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a692a12",
   "metadata": {},
   "source": [
    "### Test 1: NLP-Related Query\n",
    "\n",
    "\n",
    "\n",
    "This query should be routed to **VectorStore** since it's related to natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: NLP-Related Query\n",
    "\n",
    "query_nlp = \"What are the main applications of natural language processing?\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"▶️ Running NLP query...\")\n",
    "\n",
    "print(f\"Query: {query_nlp}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "result_nlp = app.invoke({\"query\": query_nlp, \"chat_history\": []})\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n✅ Routing Decision: {result_nlp.get('route', 'N/A')}\")\n",
    "\n",
    "print(f\"\\n✅ Number of Retrieved Documents: {len(result_nlp.get('documents', []))}\")\n",
    "\n",
    "\n",
    "\n",
    "if result_nlp.get('documents'):\n",
    "\n",
    "    print(\"\\n✅ Retrieved Documents (top 3):\")\n",
    "\n",
    "    for i, doc in enumerate(result_nlp['documents'][:3], 1):\n",
    "\n",
    "        content_preview = doc.page_content[:200].replace('\\n', ' ')\n",
    "\n",
    "        print(f\"\\n  [{i}] {content_preview}...\")\n",
    "\n",
    "        if hasattr(doc, 'metadata') and doc.metadata:\n",
    "\n",
    "            print(f\"      Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n\\n✅ Generated Answer:\\n\")\n",
    "\n",
    "print(result_nlp.get('generation', 'No generation available'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e5c3b",
   "metadata": {},
   "source": [
    "### Test 2: Computer Science (Non-NLP) Query\n",
    "\n",
    "\n",
    "\n",
    "This query is about computer science but not NLP-specific, so it should route to **SearchEngine** for web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee97ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: CS but non-NLP Query\n",
    "\n",
    "query_cs = \"Explain how binary search trees work and their time complexity\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"▶️ Running CS (non-NLP) query...\")\n",
    "\n",
    "print(f\"Query: {query_cs}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "result_cs = app.invoke({\"query\": query_cs, \"chat_history\": []})\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n✅ Routing Decision: {result_cs.get('route', 'N/A')}\")\n",
    "\n",
    "print(f\"\\n✅ Number of Retrieved Documents: {len(result_cs.get('documents', []))}\")\n",
    "\n",
    "\n",
    "\n",
    "if result_cs.get('documents'):\n",
    "\n",
    "    print(\"\\n✅ Retrieved Documents (top 3):\")\n",
    "\n",
    "    for i, doc in enumerate(result_cs['documents'][:3], 1):\n",
    "\n",
    "        content_preview = doc.page_content[:200].replace('\\n', ' ')\n",
    "\n",
    "        print(f\"\\n  [{i}] {content_preview}...\")\n",
    "\n",
    "        if hasattr(doc, 'metadata') and doc.metadata:\n",
    "\n",
    "            source = doc.metadata.get('source', 'Unknown')\n",
    "\n",
    "            # For search results, show URL if available\n",
    "\n",
    "            if 'url' in doc.metadata:\n",
    "\n",
    "                print(f\"      URL: {doc.metadata['url']}\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                print(f\"      Source: {source}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n\\n✅ Generated Answer:\\n\")\n",
    "\n",
    "print(result_cs.get('generation', 'No generation available'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af48011e",
   "metadata": {},
   "source": [
    "### Test 3: Out-of-Scope Query\n",
    "\n",
    "\n",
    "\n",
    "This query is completely unrelated to computer science or NLP, so it should route to **Fallback** for a polite decline response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad83b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: Out-of-Scope Query\n",
    "\n",
    "query_oos = \"Who is the current president of Bolivia?\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"▶️ Running out-of-scope query...\")\n",
    "\n",
    "print(f\"Query: {query_oos}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "result_oos = app.invoke({\"query\": query_oos, \"chat_history\": []})\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n✅ Routing Decision: {result_oos.get('route', 'N/A')}\")\n",
    "\n",
    "print(f\"\\n✅ Number of Retrieved Documents: {len(result_oos.get('documents', []))}\")\n",
    "\n",
    "\n",
    "\n",
    "if result_oos.get('documents'):\n",
    "\n",
    "    print(\"\\n✅ Retrieved Documents:\")\n",
    "\n",
    "    for i, doc in enumerate(result_oos['documents'][:3], 1):\n",
    "\n",
    "        content_preview = doc.page_content[:200].replace('\\n', ' ')\n",
    "\n",
    "        print(f\"\\n  [{i}] {content_preview}...\")\n",
    "\n",
    "        if hasattr(doc, 'metadata') and doc.metadata:\n",
    "\n",
    "            print(f\"      Source: {doc.metadata.get('source', 'Unknown')}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n\\n✅ Generated Answer:\\n\")\n",
    "\n",
    "print(result_oos.get('generation', 'No generation available'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5875d0cc",
   "metadata": {},
   "source": [
    "## 7️⃣ Results Analysis and Evaluation\n",
    "\n",
    "\n",
    "\n",
    "### Routing Accuracy\n",
    "\n",
    "\n",
    "\n",
    "Let's analyze if the router correctly classified each query type:\n",
    "\n",
    "\n",
    "\n",
    "| Query Type | Expected Route | Actual Route | Correct? |\n",
    "\n",
    "|------------|----------------|--------------|----------|\n",
    "\n",
    "| NLP-Related | VectorStore | (see above) | ✅/❌ |\n",
    "\n",
    "| CS (Non-NLP) | SearchEngine | (see above) | ✅/❌ |\n",
    "\n",
    "| Out-of-Scope | Fallback | (see above) | ✅/❌ |\n",
    "\n",
    "\n",
    "\n",
    "### Pipeline Performance\n",
    "\n",
    "\n",
    "\n",
    "**Strengths:**\n",
    "\n",
    "- 📚 **Hybrid Retrieval**: Ensemble retriever combines semantic understanding (FAISS) with keyword matching (BM25)\n",
    "\n",
    "- 🧠 **Smart Routing**: Temperature=0 ensures deterministic classification by the router\n",
    "\n",
    "- ✅ **Relevancy Filtering**: Optional filter_docs node removes low-quality documents before generation\n",
    "\n",
    "- 🔍 **Web Search Fallback**: Tavily integration provides up-to-date information for non-corpus queries\n",
    "\n",
    "- 💾 **Efficient Caching**: Embeddings are cached to speed up repeated runs\n",
    "\n",
    "\n",
    "\n",
    "**Limitations:**\n",
    "\n",
    "- 🌐 **Language Bias**: English-optimized embedder may underperform on non-English corpora\n",
    "\n",
    "- 📊 **Corpus Dependency**: Vector store quality depends on document coverage and chunking strategy\n",
    "\n",
    "- 🔑 **API Requirements**: Requires TogetherAI and Tavily API keys\n",
    "\n",
    "- ⏱️ **Latency**: Multiple LLM calls (router, relevancy check, generation) add response time\n",
    "\n",
    "\n",
    "\n",
    "### Possible Improvements\n",
    "\n",
    "\n",
    "\n",
    "1. **Adaptive Retrieval**: Dynamically adjust ensemble weights based on query type\n",
    "\n",
    "2. **Multi-lingual Support**: Use language-agnostic or multilingual embedders (e.g., `paraphrase-multilingual-mpnet-base-v2`)\n",
    "\n",
    "3. **Re-ranking**: Add a re-ranker after retrieval to improve document ordering\n",
    "\n",
    "4. **Conversational Memory**: Implement chat history tracking for multi-turn conversations\n",
    "\n",
    "5. **Evaluation Metrics**: Add automated evaluation with RAGAS (Retrieval Augmented Generation Assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da78c9c1",
   "metadata": {},
   "source": [
    "## 8️⃣ Conclusion\n",
    "\n",
    "\n",
    "\n",
    "This notebook implemented a complete **Retrieval-Augmented Generation (RAG)** pipeline using:\n",
    "\n",
    "\n",
    "\n",
    "### Core Technologies\n",
    "\n",
    "- **LangChain**: Framework for building LLM applications with chains and agents\n",
    "\n",
    "- **LangGraph**: State machine for orchestrating multi-step agent workflows\n",
    "\n",
    "- **FAISS**: Efficient vector similarity search for semantic retrieval\n",
    "\n",
    "- **TogetherAI**: LLM provider (Llama 3 70B) for routing, filtering, and generation\n",
    "\n",
    "- **Tavily**: Web search API for real-time information retrieval\n",
    "\n",
    "- **HuggingFace**: Sentence transformers for document embeddings\n",
    "\n",
    "\n",
    "\n",
    "### Implementation Highlights\n",
    "\n",
    "1. **Document Processing**: PDF/text loading, recursive chunking (1000 chars, 200 overlap)\n",
    "\n",
    "2. **Embeddings**: Cached HuggingFace embeddings (all-MiniLM-L6-v2) with LocalFileStore\n",
    "\n",
    "3. **Hybrid Retrieval**: Ensemble combining BM25 (30%) and FAISS (70%)\n",
    "\n",
    "4. **Intelligent Routing**: Router chain classifies queries → VectorStore/SearchEngine/Fallback\n",
    "\n",
    "5. **Optional Filtering**: Relevancy check chain removes low-quality documents\n",
    "\n",
    "6. **Context Generation**: LLM generates answers grounded in retrieved documents\n",
    "\n",
    "7. **Stateful Workflow**: LangGraph manages state transitions and conditional routing\n",
    "\n",
    "\n",
    "\n",
    "### Assignment Completion\n",
    "\n",
    "- ✅ **Required**: Basic FAISS retriever, router chain, search engine, fallback, generate chain\n",
    "\n",
    "- ✅ **Bonus**: Hybrid retriever (BM25 + FAISS), relevancy check chain\n",
    "\n",
    "- ✅ **Testing**: Three query types (NLP, CS non-NLP, out-of-scope)\n",
    "\n",
    "- ✅ **Documentation**: Comprehensive English explanations for each component\n",
    "\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **RAG bridges LLMs and external knowledge**: Reduces hallucinations by grounding responses in documents\n",
    "\n",
    "- **Hybrid search is powerful**: Combining lexical and semantic retrieval improves recall\n",
    "\n",
    "- **Temperature matters**: Lower temperatures (0-0.3) for deterministic tasks, higher (0.5+) for creative generation\n",
    "\n",
    "- **Caching saves time**: Persistent embeddings cache avoids recomputation\n",
    "\n",
    "- **Modular design enables flexibility**: Each chain/node can be tested and improved independently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b389c4c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "## 📦 Appendix: Setup Instructions\n",
    "\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "1. Python 3.8+ installed\n",
    "\n",
    "2. API keys for TogetherAI and Tavily\n",
    "\n",
    "\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "\n",
    "pip install langchain langchain-community langchain-huggingface langgraph \\\n",
    "\n",
    "            faiss-cpu sentence-transformers langchain-together tavily-python \\\n",
    "\n",
    "            pypdf pydantic\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "Create a `.env` file or set environment variables:\n",
    "\n",
    "```bash\n",
    "\n",
    "export TOGETHER_API_KEY=\"your-together-api-key\"\n",
    "\n",
    "export TAVILY_API_KEY=\"your-tavily-api-key\"\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "1. Create directory structure:\n",
    "\n",
    "   ```\n",
    "\n",
    "   NLP-CA6/\n",
    "\n",
    "   ├── answer/\n",
    "\n",
    "   │   └── code.ipynb\n",
    "\n",
    "   ├── data/\n",
    "\n",
    "   │   └── documents/  # Place your PDFs/text files here\n",
    "\n",
    "   ├── cache/  # Auto-created for embeddings\n",
    "\n",
    "   └── vector_store/  # Auto-created for FAISS index\n",
    "\n",
    "   ```\n",
    "\n",
    "\n",
    "\n",
    "2. Add documents to `data/documents/` (PDFs or .txt files)\n",
    "\n",
    "\n",
    "\n",
    "### Running the Notebook\n",
    "\n",
    "1. Open `code.ipynb` in Jupyter Lab/Notebook\n",
    "\n",
    "2. Run cells sequentially from top to bottom\n",
    "\n",
    "3. First run will:\n",
    "\n",
    "   - Generate and cache embeddings (~2-5 min depending on corpus size)\n",
    "\n",
    "   - Create FAISS vector store\n",
    "\n",
    "   - Initialize BM25 index\n",
    "\n",
    "4. Subsequent runs will load from cache (much faster)\n",
    "\n",
    "\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "- **Import errors**: Ensure all packages are installed with correct versions\n",
    "\n",
    "- **API key errors**: Verify environment variables are set correctly\n",
    "\n",
    "- **Memory issues**: Reduce `chunk_size` or process fewer documents\n",
    "\n",
    "- **Slow embeddings**: First run is slow; subsequent runs use cache\n",
    "\n",
    "- **Empty retrievals**: Check document paths and ensure PDFs contain extractable text"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
