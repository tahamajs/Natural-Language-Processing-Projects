# CA1:

# Natural Language Processing – Assignment 1

Term: 4012 
 Sharif University of Technology

In this assignment, we work with user comments collected from several well-known poems published on the Ganjoor website. The dataset consists of real-world textual data in Persian. A series of Natural Language Processing (NLP) steps are applied to these comments, including text cleaning, tokenization, normalization, stopword removal, and possibly further analysis such as frequency distribution, sentiment analysis, or topic modeling.

Ganjoor Wabsite :

[https://ganjoor.net/](https://ganjoor.net/)

## Text Exploration and Comment Extraction from the Website:

In this exercise, the Scrapy framework has been used for web crawling.
Scrapy is a powerful Python library designed for extracting structured data from websites, including text, images, and specific page elements.

The following Python script extracts user comments related to selected poems by Omar Khayyam  Hafez, and Parvin E'tesami from the Ganjoor website.
The extracted comments exhibit a nested structure, meaning that each poem contains comments, and those comments may have reply threads (sub-comments). These are stored in a hierarchical or nested object format for further processing.

```
try:
    import scrapy 
except: 
    !pip3 install scrapy
try:
    import crochet
except:
    !pip3 install crochet
try:
    import jsonlines
except:
    !pip3 install jsonlines
```

## Importing Dependencies

```python
import scrapy
from scrapy.crawler import CrawlerRunner
import re
from crochet import setup, wait_for, TimeoutError
import jsonlines

setup()

```

This code sets up a web scraping environment using Scrapy and Crochet. It imports Scrapy and `CrawlerRunner` to define and run spiders programmatically, allowing data extraction from websites. The `re` module is used for regular expression operations, typically to clean or process the scraped text. The `crochet` library is used to integrate Scrapy’s asynchronous code with regular synchronous Python code—`setup()` initializes the Crochet environment, and `wait_for` is used to wait for asynchronous results. Lastly, `jsonlines` is imported to save the scraped data in JSON Lines format, which is efficient for storing structured text data line by line.

## Generating Page URLs for Selected Poets on the Ganjoor Website

In this exercise, user comments on the poems of six renowned Persian poets have been extracted from the Ganjoor website. The process involves generating the appropriate page URLs for each poet in order to access and collect their associated comment sections.

```python
MAJOR_POETS = {
    'khayyam': {
        'ganjoor_name': 'khayyam',
    },
    'hafez': {
        'ganjoor_name': 'hafez',
    },
    'moulavi': {
        'ganjoor_name': 'moulavi',
    },
    'ferdousi': {
        'ganjoor_name': 'ferdousi',
    },
    'nezami': {
        'ganjoor_name': 'nezami',
    },
    'saadi': {
        'ganjoor_name': 'saadi'
    }
}

GANJOOR_URL = 'https://ganjoor.net/'

def gen_start_urls():
    for poet in MAJOR_POETS.values():
        yield GANJOOR_URL + poet['ganjoor_name'] + '/'

list(gen_start_urls())
```

---

This Python snippet defines a dictionary `MAJOR_POETS`, where each key corresponds to a famous Persian poet, and each value contains their specific Ganjoor URL name. The function `gen_start_urls()` iterates through these poets and constructs the full URLs by combining a base URL (`https://ganjoor.net/`) with each poet’s page name. When called, `list(gen_start_urls())` returns a list of the main Ganjoor URLs for Khayyam, Hafez, Moulavi, Ferdousi, Nezami, and Saadi.

---

### Output of `list(gen_start_urls())`:

```python
[
    'https://ganjoor.net/khayyam/',
    'https://ganjoor.net/hafez/',
    'https://ganjoor.net/moulavi/',
    'https://ganjoor.net/ferdousi/',
    'https://ganjoor.net/nezami/',
    'https://ganjoor.net/saadi/'
]

```

outputs:

![Screenshot 2025-04-09 at 9.50.44 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_9.50.44_PM.png)

To limit the dataset size and the number of requests, the comments related to the first 100 poems of each poet are extracted.

```python
LIMIT_POEM_PAGE_PER_POET = 300

global poems_crawled_per_poet
poems_crawled_per_poet = {}
for poet in MAJOR_POETS:
    poems_crawled_per_poet[poet] = 0

```

The variable `LIMIT_POEM_PAGE_PER_POET` sets an upper limit (in this case, 300) on how many poem pages should be crawled for each poet. The dictionary `poems_crawled_per_poet` is used to keep track of how many poem pages have been processed for each poet during the crawling process. Initially, all values are set to 0, ensuring that the crawler can incrementally update the count as it navigates through each poet's poems while respecting the defined limit.

---

**Spider Class for Crawling the Ganjoor Website**

The extraction of comments from the Ganjoor website has been implemented using a custom Scrapy spider class. In the Scrapy framework, spiders are specialized classes designed to automate the crawling and data extraction process for specific tasks.

The crawling begins at the main pages of selected poets and follows links up to a depth of three levels. During this traversal, the spider identifies and filters links that match the URL pattern of individual poem pages. If such a URL is detected, it is passed to the `parse` method for further processing.

When a URL is confirmed to be a poem page, it becomes a target for comment extraction, which is handled by the `parse_poem_page` function. This function retrieves key details for each comment, including the comment ID, commenter name, comment content, the related poet, and the full URL of the poem page.

```python
class GanjoorCommentsSpider(scrapy.Spider):
    name = "GanjoorSubSpider"
    allowed_domains = ["ganjoor.net"]
    start_urls = list(gen_start_urls())

    custom_settings = {
        'CONCURRENT_REQUESTS': 200,
        'LOG_LEVEL': 'WARN',
        'FEED_EXPORT_ENCODING': 'utf-8',
        'FEEDS': {
            'ganjoor.jsonlines': {
                'format': 'jsonlines',
                'overwrite': True
            }
        }
    }

    def parse(self, response):

        right_path_pattern = r"/(khayyam|hafez|moulavi|ferdousi|nezami|saadi)/[a-z]+.*"
        poem_path_pattern = r"/(khayyam|hafez|moulavi|ferdousi|nezami|saadi)/[a-z]{2,20}/.*sh[0-9]{1,3}.?"

        is_poem_page = response.meta.get("is_poem_page", False)
        if is_poem_page:
            return

        depth = response.meta.get("depth", 1)

        links = response.css("a::attr(href)").getall()

        links = [link for link in links if re.match(right_path_pattern, link)]

        poem_links = [link for link in links if re.match(poem_path_pattern, link)]

        for poem_link in poem_links:
            response_meta = {
                "is_poem_page": True,
                "depth": depth + 1,
                'poem_no': str(poem_link).split('#')[-1],
                'poet': str(poem_link).split('/')[1],
                'page_hierarchy': response.css('#page-hierarchy > a::text').getall()
            }
            yield scrapy.Request(response.urljoin(poem_link), callback=self.parse_poem_page, meta=response_meta)

        if len(poem_links) == 0 and depth < 4:
            for link in links:
                yield scrapy.Request(response.urljoin(link), callback=self.parse, meta={"depth": depth + 1})

    def parse_poem_page(self, response):
        poet_name = response.meta.get('poet')
        if poems_crawled_per_poet[poet_name] > LIMIT_POEM_PAGE_PER_POET:
            return

        for comment_block in response.css('#comments-block > .ganjoor-comment'):
            comment_id = comment_block.attrib.get('id')
            comment_text = comment_block.css('blockquote > p::text').getall()
            comment_author = comment_block.css('.comment-author::text').get()
            date_raw = comment_block.css('.comment-author > small::text').get()
            poems_crawled_per_poet[poet_name] = poems_crawled_per_poet[poet_name] + 1

            yield {
                'comment_id': comment_id,
                'comment_author': comment_author,
                'comment_text': comment_text,
                'poet': poet_name,
                'poem_no': str(poems_crawled_per_poet[poet_name]),
                'poem_path': response.meta.get('poem_no'),
                'date_raw': date_raw
            }

```

---

This Scrapy spider, named `GanjoorCommentsSpider`, is designed to extract user comments from the poem pages of six famous Persian poets on the Ganjoor website. It begins crawling from the main page of each poet (e.g., `/hafez/`, `/saadi/`) and navigates through internal links to locate individual poem pages that contain user comments.

The spider uses two regular expression patterns to filter URLs: one to find general subpaths under each poet’s section, and another to specifically identify poem pages by looking for URLs that include structures like `/shXX`, which represent poem identifiers. The spider follows links recursively up to a depth of 3 to reach these pages.

When a poem page is found, the spider passes it to the `parse_poem_page()` method. This method ensures that no more than a predefined number of poems (e.g., 300) are processed per poet. On each poem page, it searches for comment blocks, and for every comment found, it extracts key information such as comment ID, text, author name, date, poet name, and poem identifier.

All extracted comment data is structured into dictionaries and saved in `.jsonlines` format. The use of `jsonlines` ensures each comment is stored as a single line of JSON, which is efficient for further analysis. UTF-8 encoding is used to properly support Persian characters in the output.

Additional custom settings like increasing `CONCURRENT_REQUESTS` and setting a minimal log level (`WARN`) are applied to make the crawl fast and clean. This setup makes the spider efficient, scalable, and well-suited for collecting large volumes of Persian textual data for natural language processing tasks.

---

### **Executing Crawling**

The `run_spider()` function initiates the crawling process. It is decorated with `@wait_for`, which allows synchronous waiting for the asynchronous Scrapy spider to complete. A timeout of 4 minutes is set to ensure the extraction process does not run indefinitely, making the operation more controlled and reliable.

```python
@wait_for(240)
def run_spider():
    crawler = CrawlerRunner()
    d = crawler.crawl(GanjoorCommentsSpider)
    return d

try:
    run_spider()
except TimeoutError:
    pass

```

---

The `run_spider()` function launches the `GanjoorCommentsSpider` using `CrawlerRunner` from the Scrapy framework. It is decorated with `@wait_for(240)` from the Crochet library, which blocks the main thread and waits up to 240 seconds (4 minutes) for the asynchronous crawling task to finish. If the operation exceeds the time limit, a `TimeoutError` is raised. The `try-except` block is used to catch and silently handle this timeout, ensuring the script doesn't crash if the spider takes too long.

```python
import json 

with jsonlines.open('ganjoor.jsonlines', mode='r') as reader:
    objs = reader.read(), reader.read()
    print(json.dumps(objs, indent=1, ensure_ascii=False))

```

outputs:

![Screenshot 2025-04-09 at 9.56.49 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_9.56.49_PM.png)

![Screenshot 2025-04-09 at 9.58.46 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_9.58.46_PM.png)

this is that based format :

```
{"comment_id": "comment-43263", "comment_author": " ", "comment_text": ["با درود خدمت اساتید", " \"آمیزاده نگه دار که مصحف ببرد\" به چه معناست؟ سپاسگزارم"], "poet": "saadi", "poem_no": "1", "poem_path": "/saadi/nasr/sh6", "date_raw": "در ‫۳ سال و ۲ ماه قبل، دوشنبه ۱۱ آذر ۱۳۹۸، ساعت ۱۱:۴۷"}
{"comment_id": "comment-46672", "comment_author": "مرزبان ", "comment_text": ["محمد جان یعنی ادمیزاد را بنگر که قران میدزدد - نگه دار مانند گوش دار  به معنی بنگر است  و مضمون بیت اینست که  با انکه دیو از قران خواندن میگریزد پاره ای از انسانها چون  از دستش اید حتی قران را میدزدد"], "poet": "saadi", "poem_no": "2", "poem_path": "/saadi/nasr/sh6", "date_raw": "در ‫۲ سال و ۱۱ ماه قبل، یکشنبه ۳ فروردین ۱۳۹۹، ساعت ۱۹:۵۳"}
{"comment_id": "comment-46673", "comment_author": "مرزبان ", "comment_text": ["خزما از وی باز نستانند  - درست ایت و نه باز ستانند همانگونه که میبینیم  در اخر هم تاکید کرده که  فرمود تا بها و  خرما از بقالان باز نستانند و زر پس دهند"], "poet": "saadi", "poem_no": "3", "poem_path": "/saadi/nasr/sh6", "date_raw": "در ‫۲ سال و ۱۱ ماه قبل، یکشنبه ۳ فروردین ۱۳۹۹، ساعت ۱۹:۵۵"}
{"comment_id": "comment-46674", "comment_author": "مرزبان ", "comment_text": ["شمس الدین تازیکو برای همین بذل و بخششها عاقبت برشکست شد و همه مال خود از دست داد"], "poet": "saadi", "poem_no": "4", "poem_path": "/saadi/nasr/sh6", "date_raw": "در ‫۲ سال و ۱۱ ماه قبل، یکشنبه ۳ فروردین ۱۳۹۹، ساعت ۱۹:۵۵"}
{"comment_id": "comment-46675", "comment_author": "مرزبان ", "comment_text": ["رحمت خدا بر علی بن  احمد بیستون نگارنده و گرد اورنده کلیات سعدی  که این تقاریر زیبا را برای ما به یادگار گذاشت  این نوشته همانگونه که دوستان میدانند از شیخ نیست  و بجز شعرهایی که در لابلای گفتار امده مابقی از  علی بن  احمد بیستون است  که با  نثر بسیار روان خود نوشته ای کاش که با این  نثر روان  خود نیز اثاری داشت  افسوس که قدر نثر روان خود ندانسته و ما را از نوشته های خود  محروم کرده است"], "poet": "saadi", "poem_no": "5", "poem_path": "/saadi/nasr/sh6", "date_raw": "در ‫۲ سال و ۱۱ ماه قبل، یکشنبه ۳ فروردین ۱۳۹۹، ساعت ۱۹:۵۹"}
{"comment_id": "comment-92333", "comment_author": " ", "comment_text": ["توجه به تذکر شادروان فروغی راجع به این دیباچه در پاورقی این بخش از تصحیحش از کلیات سعدی ضروری است:", "«این دیباچه بی هیچ تردید از شیخ سعدی نیست و چون در نسخه‌های قدیم و معتبر که ما در دست داریم نیست ناگزیر از روی نسخ چاپی و نسخه‌هایی که در حدود قرن دهم کتابت شده تصحیح کردیم»."], "poet": "saadi", "poem_no": "6", "poem_path": "/saadi/nasr/sh1", "date_raw": "در ‫۱ سال و ۱ ماه قبل، دوشنبه ۲۹ آذر ۱۴۰۰، ساعت ۱۹:۴۹"}
{"comment_id": "comment-96883", "comment_author": " ", "comment_text": ["بنام خدا", "عبارت «جل جلاله و عم نواله »  به صورتهای مختلف  در برخی خطب بزرگان دین ، دیده شده است. مثلا سید رضی به هنگام بیان برخی از خطبه های امیرالمومنین (ع) از این تعبیر استفاده کرده است . در آغاز خطبه 86 نهج البلاغه چنین گفته است:", "من خطبة له (علیه السلام) و فیها بیان صفات الحق جلّ جلاله،", " معنای خبری عبارت سعدی در آغاز دیباچه خود چنین است :  ", "«بزرگی و جلالت خداوند عظیم تر از تصور است و نعمتهای او عام است و همه را فرا می گیرد.»", "در واقع  این عبارت شامل دو جمله  فعلیه است.  «نوال» هم به معنای دهش و بخشش و عطاست.", "این نوع جملات فعلیه در واقع جمله دعائی هستند و مرکب از فعل و فاعل اند.  این نوع جمله دعایی را  معمولاً پس از ذکر نام خداوند می آورند. و در واقع ثنائی است که پس از بردن نام خدای تعالی ذکر میشود. بنابراین بهتر است به فارسی چنین ترجمه شود:", "« قدر و جلالت خداوند ، بزرگ باد.  و نعمتهای او  فرا گیر باد. »", " ", " "], "poet": "saadi", "poem_no": "7", "poem_path": "/saadi/nasr/sh1", "date_raw": "در ‫۱۰ ماه قبل، شنبه ۱۰ اردیبهشت ۱۴۰۱، ساعت ۲۲:۲۵"}
{"comment_id": "comment-58718", "comment_author": " ", "comment_text": ["درود. مصرع های دوم آخرین دو بیتی این صفحه جابجا نوشته شده؛یعنی باید اینچنین باشد:", "ای برتر از خیال و قیاس و گمان و وهم ", "وز هر چه گفته‌اند و شنیدیم و دیده‌ایم", "مجلس تمام گشت و به آخر رسید عمر", "ما همچنان در اول وصف تو مانده‌ایم", "سپاس."], "poet": "saadi", "poem_no": "8", "poem_path": "/saadi/nasr/sh4", "date_raw": "در ‫۲ سال و ۹ ماه قبل، سه‌شنبه ۶ خرداد ۱۳۹۹، ساعت ۱۵:۱۳"}
{"comment_id": "comment-27475", "comment_author": "حمید ", "comment_text": ["سلام", "خسته نباشید.", "فکر می کنم در بیت آخر \"به شیر\" صحیح تر باشد.", "---", " با تشکر، طبق پیشنهاد شما «بشیر» با «به شیر» جایگزین شد."], "poet": "saadi", "poem_no": "9", "poem_path": "/saadi/divan/ghazals/sh290/", "date_raw": "در ‫۱۴ سال و ۱۰ ماه قبل، دوشنبه ۱۳ خرداد ۱۳۸۷، ساعت ۲۰:۵۵"}

```

This dataset is a collection of user comments extracted from the Ganjoor website, formatted in JSON Lines where each line is an independent JSON object. Each object represents one comment and includes several descriptive fields. For example, the `comment_id` uniquely identifies the comment, while the `comment_author` provides the name of the individual who posted it (which may sometimes be empty). The `comment_text` is stored as an array of strings, allowing for multi-line or segmented text entries. Additionally, fields like `poet` and `poem_no` indicate the poet associated with the comment and a sequential identifier for the poem, respectively, and the `poem_path` shows the specific page path on the site where the comment was made. Finally, the `date_raw` field records the original posting date and time in a raw text format that includes both relative and absolute time information, primarily in Persian.

## Data Processing After Extraction:

### Installing Tools Used for Preprocessing and Data Processing

Two essential tools, Hazm and Dadatools, have been employed for data normalization and various processing tasks. Hazm, a library tailored for Persian language processing, offers functionalities such as tokenization, stemming, and normalization, ensuring that the text is properly cleaned and standardized. In parallel, Dadatools provides additional processing and analysis capabilities that further refine the dataset for subsequent analysis. Together, these tools streamline the preprocessing workflow, making the raw data more usable and reliable for further tasks.

```python
try:
    import hazm
except:
    !pip3 install hazm
try:
    import dadmatools
except:
    !pip3 install dadmatools
try:
    import pandas as pd
except:
    !pip3 install pandas
    import pandas as pd
try:
    import numpy as np
except:
    !pip3 install numpy
    import numpy as np

pd.set_option('max_colwidth', None)
```

### Reading the Extracted Dataset from the JSONLines File

    To begin processing, the dataset—previously extracted and saved in a `.jsonlines` format—is loaded using the <code>pandas</code> library. JSONLines is an efficient format where each line contains a separate JSON object, making it suitable for large-scale text data. The following code reads the file into a DataFrame and displays the first three entries:

![Screenshot 2025-04-09 at 10.05.59 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_10.05.59_PM.png)

now we need to Normalizing the Comments :

![Screenshot 2025-04-09 at 10.10.10 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_10.10.10_PM.png)

```python
from __future__ import unicode_literals
from hazm import Normalizer as HazmNormalizer

hazm_normalizer = HazmNormalizer(
    remove_extra_spaces=True,
    persian_style=True,
    persian_numbers=True,
    remove_diacritics=True,
    affix_spacing=True,
    punctuation_spacing=True
)
df["hazm_normalized_comment"] = df["comment_text"].apply(lambda comment: hazm_normalizer.normalize('\r\n'.join(comment))) 
```

In this step, the normalized comments obtained from the previous stage were segmented into individual sentences using Hazm's sentence tokenizer. Sentence segmentation is a crucial preprocessing step in natural language processing, as it allows for more granular analysis of textual data.

The result of this segmentation is reflected in a new column within the dataset, where each comment is split into a list of separate sentences. This transformation enables more precise downstream tasks such as sentiment analysis, syntactic parsing, or sentence-level classification.

```python
from hazm import sent_tokenize
import itertools
df["comment_as_sentences"] = df["hazm_normalized_comment"].apply(lambda comment: sent_tokenize(comment)) 

df[["hazm_normalized_comment", "comment_as_sentences"]][5:10]

```

output sample :

![Screenshot 2025-04-09 at 10.15.27 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_10.15.27_PM.png)

## Performing Analyses on the Comment Data:

```python
data_per_poet = df.groupby(by=['poet'])
data_per_poet.get_group('saadi').iloc()[:3]
```

![Screenshot 2025-04-09 at 10.16.16 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_10.16.16_PM.png)

### Grouping All Comments by Poet

In this part of the exercise, the goal is to analyze user comments in relation to each individual poet. Since we are interested in identifying differences and patterns in the comments across various poets, it is essential to first group the dataset accordingly.

To achieve this, we use the `groupby` function from the `pandas` library, which allows us to organize the data based on the unique name of each poet. This grouping facilitates poet-specific analysis, such as examining the tone, length, or themes of comments related to each poet's works.

```python
pages_per_poet = data_per_poet.count()['comment_id']
comments_per_poem = df.groupby(by=['poet', 'poem_path']).count()['comment_id']
dff = pd.DataFrame(comments_per_poem)
dff['comments_count'] = dff['comment_id']
dff = dff.drop('comment_id', axis=1)

mean_page_comment_poet = dff.groupby(by=['poet'], sort=True).mean('comments_count').sort_values(by='comments_count')

mean_page_comment_poet
```

![Screenshot 2025-04-09 at 10.18.02 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_10.18.02_PM.png)

The values in the table above have been plotted in the figure below using the pandas library. The average number of comments per poet’s page can serve as a proxy for the poet’s popularity and the controversial nature of their poetry.

```
mean_page_comment_poet.plot(kind='bar', ylabel='Mean Num of Comments Per Page', legend=False)
```

![Screenshot 2025-04-09 at 10.19.15 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_10.19.15_PM.png)

## now Frequency Analysis :

## **Tokenization of Texts:**

The first step in frequency analysis is tokenizing the text. Each of the Ganjoor comments has been split into individual words using the Hazm tokenizer. The input to the tokenizer is the normalized data obtained from the previous preprocessing steps.

```python
from hazm import word_tokenize
df["tokenized_comment"] = df["comment_as_sentences"].apply(lambda sentences: word_tokenize(''.join(sentences))) 

df[["tokenized_comment"]][5:10]
```

![Screenshot 2025-04-09 at 10.21.54 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_10.21.54_PM.png)

Removing Stopwords

```python
stop_words = hazm.stopwords_list()
' , '.join(stop_words[:50])
```

![Screenshot 2025-04-09 at 10.45.04 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_10.45.04_PM.png)

```python
from hazm import word_tokenize
df["tokenized_comment_stripped"] = df["tokenized_comment"].apply(lambda words: [w for w in words if w not in stop_words]) 

df[["tokenized_comment", "tokenized_comment_stripped"]].iloc()[:3]

```

![Screenshot 2025-04-09 at 10.45.18 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_10.45.18_PM.png)

The code snippet below uses the HAZM stopword list to remove stopwords from all comments. As shown in the output, the filtered tokens are not ideal—low-value characters still remain in the word list for each comment. Therefore, the next code snippet adds additional symbols to the stopword list to ensure only meaningful words are extracted. In the output, the effect of tokenization after adding the new stopwords is visible. (The right column contains the updated tokens.)

```python
modified_stop_words = hazm.stopwords_list() + [',', '.', ':', '«', '»', '(', ')', '،', '-', '?', '؟', '-']

df["stripped2"] = df["tokenized_comment_stripped"].apply(lambda words: [w for w in words if w not in modified_stop_words]) 

df[["tokenized_comment_stripped", "stripped2"]].iloc()[:3]
```

![Screenshot 2025-04-09 at 10.46.00 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_10.46.00_PM.png)

## Word Frequency Analysis

Following normalization and the removal of low-value stopwords, we proceed with word frequency analysis on the comments associated with each poet's poem pages. This step helps uncover the most commonly used meaningful words within the user-generated content.

In the code snippet provided below, the comments are first grouped by poet, and then tokenized word lists are analyzed using the FreqDist class from the nltk .library. This class efficiently computes word frequencies. For each poet, the top 25 most frequently occurring words are extracted, providing insight into recurring themes, vocabulary, or linguistic patterns.

The results are presented in a structured DataFrame that lists the most frequent words for each poet, serving as a useful tool for comparative analysis and thematic exploration

```python

from nltk import FreqDist
data_per_poet = df.groupby(by=['poet'])["stripped2"]

tokenized_comments_per_poet = data_per_poet.apply(lambda x: np.array(x, dtype=object))
freq_dist = {}
for poet in data_per_poet:
    wlist_poet = list(np.concatenate(tokenized_comments_per_poet[poet[0]]).flatten())
    freq_dist[poet[0]] = FreqDist(wlist_poet).most_common(25)

freq_dist
fdf = pd.DataFrame(freq_dist)
fdf

```

![Screenshot 2025-04-09 at 10.46.31 PM.png](CA1%201d09381f908c80e6ad61cc5588ed0f43/Screenshot_2025-04-09_at_10.46.31_PM.png)

### Comparison of References to Different Poets in the Comment Sections of Each Poet’s Page:

In this part of the exercise, by analyzing the text of comments on poets' pages, it is calculated how many times on average each poet is referenced on the pages of other poets. This value is computed for each pair of poets. The output data from this section can indicate which poets are more frequently compared with each other in user discussions on the website. Furthermore, poets whose names are frequently referenced reciprocally on each other’s pages likely share similarities in their era, poetic style, or poem content.